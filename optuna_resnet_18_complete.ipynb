{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jychen630/dl-fall22-mini-project/blob/main/Dennis_Mini_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM4tmSSp0cPb",
        "outputId": "6812ad09-c68f-4944-9e47-dbc822206c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SsNWB9qNNKka"
      },
      "outputs": [],
      "source": [
        "#!pip install -e git+https://github.com/marcoancona/TorchPruner.git#egg=torchpruner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GXI-w576XzT_"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "def set_up_ssl():\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "set_up_ssl()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wTLc4fVcQ857"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import argparse\n",
        "import humanize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jQeGvfSCRM4i"
      },
      "outputs": [],
      "source": [
        "term_width = 5\n",
        "TOTAL_BAR_LENGTH = 7\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wTCFIHn0XzUL"
      },
      "outputs": [],
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # pruning model parameters in the first convolution layer\n",
        "\n",
        "        # prune.random_unstructured(self.conv1, name='weight', amount=0.6)\n",
        "        # prune.remove(self.conv1, 'weight')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        # pruning model parameters in the second convolution layer\n",
        "\n",
        "        # prune.random_unstructured(self.conv2, name='weight', amount=0.6)\n",
        "        # prune.remove(self.conv2, 'weight')\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "\n",
        "        # prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
        "        # prune.remove(self.conv1, 'weight')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        \n",
        "        # prune.random_unstructured(self.conv2, name='weight', amount=0.5)\n",
        "        # prune.remove(self.conv2, 'weight')\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        \n",
        "        # prune.random_unstructured(self.conv3, name='weight', amount=0.5)\n",
        "        # prune.remove(self.conv3, 'weight')\n",
        "\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        \n",
        "        # prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
        "        # prune.remove(self.conv1, 'weight')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        # removing the 4th layer to reduce the size of the network\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        # prune.random_unstructured(self.linear, name='weight', amount=0.5)\n",
        "        # prune.remove(self.linear, 'weight')\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        # removing the 4th layer to reduce the size of the network\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "#seems not used\n",
        "# def test():\n",
        "#     net = ResNet18()\n",
        "#     y = net(torch.randn(1, 3, 32, 32))\n",
        "#     print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "f9cddfc3bc9d458e83054c3175b478ed",
            "b49d88b0fea6496592f35fddf0826c24",
            "fe6a906e76f3494baf5e4a109daa4e9c",
            "a9e02dc71a1f4283ac6086f30f354490",
            "374936fc9fb54190a370b476131bf5c6",
            "0d3ee5109c5c4fc0b47db02deca1f2f5",
            "79c500cbc94a4fcbbed6e968a01c760a",
            "ff5988d1a5cb4ac8a0fa6b69bc2cb428",
            "160869b4947e4dc6b74fe1856d6a3853",
            "9d407f071df2445e866ad8ed86f32afc",
            "13a0c8f358234c4e8bcb1ccf4e40eda1"
          ]
        },
        "id": "N_K9-VkFRsiL",
        "outputId": "f1de1c1e-62bc-4665-81fe-66ba4dceb0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9cddfc3bc9d458e83054c3175b478ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "The length of a train set is  45000\n",
            "The length of a validation set is  5000\n",
            "The length of a test set is  10000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "LOCAL_M1 = False\n",
        "\n",
        "if LOCAL_M1:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
        "else:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    #transforms.RandomErasing()\n",
        "  \n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# constructing validation set\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "torch.manual_seed(43)\n",
        "val_size = 5000\n",
        "train_size = len(trainset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)\n",
        "print(\"The length of a train set is \", len(train_ds))\n",
        "print(\"The length of a validation set is \", len(val_ds))\n",
        "print(\"The length of a test set is \", len(testset))\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_ds, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_ds, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "net = ResNet18() # 11.2 params\n",
        "#net = ResNet50() # 23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbzjCm0gOElC",
        "outputId": "5332130d-89a1-41ed-f572-d6f75d1660fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "layers[0]:  Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layers[1]:  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layers[2]:  Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential()\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential()\n",
            "  )\n",
            ")\n",
            "layers[3]:  Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential()\n",
            "  )\n",
            ")\n",
            "layers[4]:  Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential()\n",
            "  )\n",
            ")\n",
            "layers[5]:  Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential()\n",
            "  )\n",
            ")\n",
            "layers[6]:  Linear(in_features=512, out_features=10, bias=True)\n"
          ]
        }
      ],
      "source": [
        "layers = list(net.children())\n",
        "\n",
        "print(len(layers))\n",
        "\n",
        "print(\"layers[0]: \", layers[0])\n",
        "print(\"layers[1]: \", layers[1])\n",
        "print(\"layers[2]: \", layers[2])\n",
        "print(\"layers[3]: \", layers[3])\n",
        "print(\"layers[4]: \", layers[4])\n",
        "print(\"layers[5]: \", layers[5])\n",
        "print(\"layers[6]: \", layers[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7K43Gm-VfhJ",
        "outputId": "e52b833d-0606-43e7-9b53-3eeafdd7dfd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Torch-Pruning'...\n",
            "remote: Enumerating objects: 1833, done.\u001b[K\n",
            "remote: Counting objects: 100% (688/688), done.\u001b[K\n",
            "remote: Compressing objects: 100% (314/314), done.\u001b[K\n",
            "remote: Total 1833 (delta 418), reused 609 (delta 358), pack-reused 1145\u001b[K\n",
            "Receiving objects: 100% (1833/1833), 3.91 MiB | 9.39 MiB/s, done.\n",
            "Resolving deltas: 100% (1139/1139), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_pruning\n",
            "  Downloading torch_pruning-0.2.8-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch_pruning) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_pruning) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch_pruning) (4.1.1)\n",
            "Installing collected packages: torch-pruning\n",
            "Successfully installed torch-pruning-0.2.8\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/VainF/Torch-Pruning.git # recommended\n",
        "!pip3 install torch_pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2M1xDV4-VaKG"
      },
      "outputs": [],
      "source": [
        "import torch_pruning as tp\n",
        "\n",
        "def prune_model(model):\n",
        "    model.cpu()\n",
        "    DG = tp.DependencyGraph().build_dependency( model, torch.randn(1, 3, 32, 32) )\n",
        "    def prune_conv(conv, amount=0.2):\n",
        "        strategy = tp.strategy.L1Strategy()\n",
        "        pruning_index = strategy(conv.weight, amount=amount)\n",
        "        plan = DG.get_pruning_plan(conv, tp.prune_conv_out_channel, pruning_index)\n",
        "        plan.exec()\n",
        "    \n",
        "    block_prune_probs = [0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3]\n",
        "    blk_id = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance( m, BasicBlock):\n",
        "            prune_conv( m.conv1, block_prune_probs[blk_id] )\n",
        "            prune_conv( m.conv2, block_prune_probs[blk_id] )\n",
        "            blk_id+=1\n",
        "    return model   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kgQPV3H4ZTxA"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def print_params(model):\n",
        "  print(\"Number of parameters \", humanize.intword(count_parameters(model)))\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCkbs0btThnx",
        "outputId": "35468c13-ef6e-4576-9303-fee326e8ffbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of parameters before pruning is \n",
            "Number of parameters  11.2 million\n"
          ]
        }
      ],
      "source": [
        "print(\"The number of parameters before pruning is \")\n",
        "print_params(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duOPOJDMV2Vn",
        "outputId": "de142ea6-e741-4eb8-9455-bf8002e763e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(44, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(53, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(44, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(53, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(44, 83, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(83, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(44, 54, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(54, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(83, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(54, 164, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(164, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(164, 106, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(54, 106, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(106, 164, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(164, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(164, 106, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(106, 252, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(252, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(106, 124, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(124, 252, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(252, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=124, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "prune_model(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAZTZsHwTyOK",
        "outputId": "e7f7e5e7-69e9-4306-f896-9b37f9320651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of parameters after pruning is \n",
            "Number of parameters  1.9 million\n"
          ]
        }
      ],
      "source": [
        "print(\"The number of parameters after pruning is \")\n",
        "print_params(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ou8nTFiEPAqT"
      },
      "outputs": [],
      "source": [
        "# from torchpruner import (Pruner, ShapleyAttributionMetric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "htKbN2cYXzUh"
      },
      "outputs": [],
      "source": [
        "def get_pruned_parameters_countget_pruned_parameters_count(pruned_model):\n",
        "    params = 0\n",
        "    for param in pruned_model.parameters():\n",
        "        if param is not None:\n",
        "            params += torch.nonzero(param).size(0)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTqr1gYtVHgE",
        "outputId": "52f0f8db-80c4-4c58-9ed1-7f4fd63a064a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 44, 32, 32]           1,188\n",
            "       BatchNorm2d-2           [-1, 44, 32, 32]              88\n",
            "            Conv2d-3           [-1, 53, 32, 32]          20,988\n",
            "       BatchNorm2d-4           [-1, 53, 32, 32]             106\n",
            "            Conv2d-5           [-1, 44, 32, 32]          20,988\n",
            "       BatchNorm2d-6           [-1, 44, 32, 32]              88\n",
            "        BasicBlock-7           [-1, 44, 32, 32]               0\n",
            "            Conv2d-8           [-1, 53, 32, 32]          20,988\n",
            "       BatchNorm2d-9           [-1, 53, 32, 32]             106\n",
            "           Conv2d-10           [-1, 44, 32, 32]          20,988\n",
            "      BatchNorm2d-11           [-1, 44, 32, 32]              88\n",
            "       BasicBlock-12           [-1, 44, 32, 32]               0\n",
            "           Conv2d-13           [-1, 83, 16, 16]          32,868\n",
            "      BatchNorm2d-14           [-1, 83, 16, 16]             166\n",
            "           Conv2d-15           [-1, 54, 16, 16]          40,338\n",
            "      BatchNorm2d-16           [-1, 54, 16, 16]             108\n",
            "           Conv2d-17           [-1, 54, 16, 16]           2,376\n",
            "      BatchNorm2d-18           [-1, 54, 16, 16]             108\n",
            "       BasicBlock-19           [-1, 54, 16, 16]               0\n",
            "           Conv2d-20           [-1, 83, 16, 16]          40,338\n",
            "      BatchNorm2d-21           [-1, 83, 16, 16]             166\n",
            "           Conv2d-22           [-1, 54, 16, 16]          40,338\n",
            "      BatchNorm2d-23           [-1, 54, 16, 16]             108\n",
            "       BasicBlock-24           [-1, 54, 16, 16]               0\n",
            "           Conv2d-25            [-1, 164, 8, 8]          79,704\n",
            "      BatchNorm2d-26            [-1, 164, 8, 8]             328\n",
            "           Conv2d-27            [-1, 106, 8, 8]         156,456\n",
            "      BatchNorm2d-28            [-1, 106, 8, 8]             212\n",
            "           Conv2d-29            [-1, 106, 8, 8]           5,724\n",
            "      BatchNorm2d-30            [-1, 106, 8, 8]             212\n",
            "       BasicBlock-31            [-1, 106, 8, 8]               0\n",
            "           Conv2d-32            [-1, 164, 8, 8]         156,456\n",
            "      BatchNorm2d-33            [-1, 164, 8, 8]             328\n",
            "           Conv2d-34            [-1, 106, 8, 8]         156,456\n",
            "      BatchNorm2d-35            [-1, 106, 8, 8]             212\n",
            "       BasicBlock-36            [-1, 106, 8, 8]               0\n",
            "           Conv2d-37            [-1, 252, 4, 4]         240,408\n",
            "      BatchNorm2d-38            [-1, 252, 4, 4]             504\n",
            "           Conv2d-39            [-1, 124, 4, 4]         281,232\n",
            "      BatchNorm2d-40            [-1, 124, 4, 4]             248\n",
            "           Conv2d-41            [-1, 124, 4, 4]          13,144\n",
            "      BatchNorm2d-42            [-1, 124, 4, 4]             248\n",
            "       BasicBlock-43            [-1, 124, 4, 4]               0\n",
            "           Conv2d-44            [-1, 252, 4, 4]         281,232\n",
            "      BatchNorm2d-45            [-1, 252, 4, 4]             504\n",
            "           Conv2d-46            [-1, 124, 4, 4]         281,232\n",
            "      BatchNorm2d-47            [-1, 124, 4, 4]             248\n",
            "       BasicBlock-48            [-1, 124, 4, 4]               0\n",
            "           Linear-49                   [-1, 10]           1,250\n",
            "================================================================\n",
            "Total params: 1,898,868\n",
            "Trainable params: 1,898,868\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.88\n",
            "Params size (MB): 7.24\n",
            "Estimated Total Size (MB): 14.13\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "def print_model_summary(model):\n",
        "  print(summary(model.to(device), (3, 32, 32)))\n",
        "\n",
        "print_model_summary(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tejFfYvQJxR5",
        "outputId": "e0ef5bf9-b181-486a-eb53-ce7d5108df52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight torch.Size([44, 3, 3, 3])\n",
            "bn1.weight torch.Size([44])\n",
            "bn1.bias torch.Size([44])\n",
            "layer1.0.conv1.weight torch.Size([53, 44, 3, 3])\n",
            "layer1.0.bn1.weight torch.Size([53])\n",
            "layer1.0.bn1.bias torch.Size([53])\n",
            "layer1.0.conv2.weight torch.Size([44, 53, 3, 3])\n",
            "layer1.0.bn2.weight torch.Size([44])\n",
            "layer1.0.bn2.bias torch.Size([44])\n",
            "layer1.1.conv1.weight torch.Size([53, 44, 3, 3])\n",
            "layer1.1.bn1.weight torch.Size([53])\n",
            "layer1.1.bn1.bias torch.Size([53])\n",
            "layer1.1.conv2.weight torch.Size([44, 53, 3, 3])\n",
            "layer1.1.bn2.weight torch.Size([44])\n",
            "layer1.1.bn2.bias torch.Size([44])\n",
            "layer2.0.conv1.weight torch.Size([83, 44, 3, 3])\n",
            "layer2.0.bn1.weight torch.Size([83])\n",
            "layer2.0.bn1.bias torch.Size([83])\n",
            "layer2.0.conv2.weight torch.Size([54, 83, 3, 3])\n",
            "layer2.0.bn2.weight torch.Size([54])\n",
            "layer2.0.bn2.bias torch.Size([54])\n",
            "layer2.0.shortcut.0.weight torch.Size([54, 44, 1, 1])\n",
            "layer2.0.shortcut.1.weight torch.Size([54])\n",
            "layer2.0.shortcut.1.bias torch.Size([54])\n",
            "layer2.1.conv1.weight torch.Size([83, 54, 3, 3])\n",
            "layer2.1.bn1.weight torch.Size([83])\n",
            "layer2.1.bn1.bias torch.Size([83])\n",
            "layer2.1.conv2.weight torch.Size([54, 83, 3, 3])\n",
            "layer2.1.bn2.weight torch.Size([54])\n",
            "layer2.1.bn2.bias torch.Size([54])\n",
            "layer3.0.conv1.weight torch.Size([164, 54, 3, 3])\n",
            "layer3.0.bn1.weight torch.Size([164])\n",
            "layer3.0.bn1.bias torch.Size([164])\n",
            "layer3.0.conv2.weight torch.Size([106, 164, 3, 3])\n",
            "layer3.0.bn2.weight torch.Size([106])\n",
            "layer3.0.bn2.bias torch.Size([106])\n",
            "layer3.0.shortcut.0.weight torch.Size([106, 54, 1, 1])\n",
            "layer3.0.shortcut.1.weight torch.Size([106])\n",
            "layer3.0.shortcut.1.bias torch.Size([106])\n",
            "layer3.1.conv1.weight torch.Size([164, 106, 3, 3])\n",
            "layer3.1.bn1.weight torch.Size([164])\n",
            "layer3.1.bn1.bias torch.Size([164])\n",
            "layer3.1.conv2.weight torch.Size([106, 164, 3, 3])\n",
            "layer3.1.bn2.weight torch.Size([106])\n",
            "layer3.1.bn2.bias torch.Size([106])\n",
            "layer4.0.conv1.weight torch.Size([252, 106, 3, 3])\n",
            "layer4.0.bn1.weight torch.Size([252])\n",
            "layer4.0.bn1.bias torch.Size([252])\n",
            "layer4.0.conv2.weight torch.Size([124, 252, 3, 3])\n",
            "layer4.0.bn2.weight torch.Size([124])\n",
            "layer4.0.bn2.bias torch.Size([124])\n",
            "layer4.0.shortcut.0.weight torch.Size([124, 106, 1, 1])\n",
            "layer4.0.shortcut.1.weight torch.Size([124])\n",
            "layer4.0.shortcut.1.bias torch.Size([124])\n",
            "layer4.1.conv1.weight torch.Size([252, 124, 3, 3])\n",
            "layer4.1.bn1.weight torch.Size([252])\n",
            "layer4.1.bn1.bias torch.Size([252])\n",
            "layer4.1.conv2.weight torch.Size([124, 252, 3, 3])\n",
            "layer4.1.bn2.weight torch.Size([124])\n",
            "layer4.1.bn2.bias torch.Size([124])\n",
            "linear.weight torch.Size([10, 124])\n",
            "linear.bias torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "def print_model_layers(model):\n",
        "  for name, param in model.named_parameters():\n",
        "    print(name, param.size())\n",
        "\n",
        "print_model_layers(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iod4A6RJVzBb"
      },
      "outputs": [],
      "source": [
        "def freeze_layer(layer_name):\n",
        "  for param_name, param in net.named_parameters():\n",
        "    if layer_name in param_name:\n",
        "      param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "D4CjYR_cLnDx"
      },
      "outputs": [],
      "source": [
        "# freeze_layer(\"layer3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDYLyuksFbiB",
        "outputId": "7a0453f9-790e-4ccc-dc1c-6ee02a22e677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 44, 32, 32]           1,188\n",
            "       BatchNorm2d-2           [-1, 44, 32, 32]              88\n",
            "            Conv2d-3           [-1, 53, 32, 32]          20,988\n",
            "       BatchNorm2d-4           [-1, 53, 32, 32]             106\n",
            "            Conv2d-5           [-1, 44, 32, 32]          20,988\n",
            "       BatchNorm2d-6           [-1, 44, 32, 32]              88\n",
            "        BasicBlock-7           [-1, 44, 32, 32]               0\n",
            "            Conv2d-8           [-1, 53, 32, 32]          20,988\n",
            "       BatchNorm2d-9           [-1, 53, 32, 32]             106\n",
            "           Conv2d-10           [-1, 44, 32, 32]          20,988\n",
            "      BatchNorm2d-11           [-1, 44, 32, 32]              88\n",
            "       BasicBlock-12           [-1, 44, 32, 32]               0\n",
            "           Conv2d-13           [-1, 83, 16, 16]          32,868\n",
            "      BatchNorm2d-14           [-1, 83, 16, 16]             166\n",
            "           Conv2d-15           [-1, 54, 16, 16]          40,338\n",
            "      BatchNorm2d-16           [-1, 54, 16, 16]             108\n",
            "           Conv2d-17           [-1, 54, 16, 16]           2,376\n",
            "      BatchNorm2d-18           [-1, 54, 16, 16]             108\n",
            "       BasicBlock-19           [-1, 54, 16, 16]               0\n",
            "           Conv2d-20           [-1, 83, 16, 16]          40,338\n",
            "      BatchNorm2d-21           [-1, 83, 16, 16]             166\n",
            "           Conv2d-22           [-1, 54, 16, 16]          40,338\n",
            "      BatchNorm2d-23           [-1, 54, 16, 16]             108\n",
            "       BasicBlock-24           [-1, 54, 16, 16]               0\n",
            "           Conv2d-25            [-1, 164, 8, 8]          79,704\n",
            "      BatchNorm2d-26            [-1, 164, 8, 8]             328\n",
            "           Conv2d-27            [-1, 106, 8, 8]         156,456\n",
            "      BatchNorm2d-28            [-1, 106, 8, 8]             212\n",
            "           Conv2d-29            [-1, 106, 8, 8]           5,724\n",
            "      BatchNorm2d-30            [-1, 106, 8, 8]             212\n",
            "       BasicBlock-31            [-1, 106, 8, 8]               0\n",
            "           Conv2d-32            [-1, 164, 8, 8]         156,456\n",
            "      BatchNorm2d-33            [-1, 164, 8, 8]             328\n",
            "           Conv2d-34            [-1, 106, 8, 8]         156,456\n",
            "      BatchNorm2d-35            [-1, 106, 8, 8]             212\n",
            "       BasicBlock-36            [-1, 106, 8, 8]               0\n",
            "           Conv2d-37            [-1, 252, 4, 4]         240,408\n",
            "      BatchNorm2d-38            [-1, 252, 4, 4]             504\n",
            "           Conv2d-39            [-1, 124, 4, 4]         281,232\n",
            "      BatchNorm2d-40            [-1, 124, 4, 4]             248\n",
            "           Conv2d-41            [-1, 124, 4, 4]          13,144\n",
            "      BatchNorm2d-42            [-1, 124, 4, 4]             248\n",
            "       BasicBlock-43            [-1, 124, 4, 4]               0\n",
            "           Conv2d-44            [-1, 252, 4, 4]         281,232\n",
            "      BatchNorm2d-45            [-1, 252, 4, 4]             504\n",
            "           Conv2d-46            [-1, 124, 4, 4]         281,232\n",
            "      BatchNorm2d-47            [-1, 124, 4, 4]             248\n",
            "       BasicBlock-48            [-1, 124, 4, 4]               0\n",
            "           Linear-49                   [-1, 10]           1,250\n",
            "================================================================\n",
            "Total params: 1,898,868\n",
            "Trainable params: 1,898,868\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.88\n",
            "Params size (MB): 7.24\n",
            "Estimated Total Size (MB): 14.13\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print_model_summary(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IseBUFvnXzUj",
        "outputId": "43da23ee-e00f-4d4a-a1d0-7eaa278fcd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of parameters before the pruning is \n",
            "Number of parameters  1.9 million\n"
          ]
        }
      ],
      "source": [
        "print(\"The total number of parameters before the pruning is \")\n",
        "print_params(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "a3kWtBzVWg3Y"
      },
      "outputs": [],
      "source": [
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "lr = 0.1\n",
        "lr = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=5e-4)\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
        "                       momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "save_loss = {'train':[], 'test':[]}\n",
        "save_acc = {'train':[], 'test':[]}\n",
        "\n",
        "train_acc_array, train_loss_array = [], [] # for plotting\n",
        "val_acc_array, val_loss_array = [], [] # for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CIzJObnOWz2d"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_acc = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        train_acc=100.*correct/total\n",
        "        progress_bar(batch_idx, len(trainloader), 'Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    train_acc_array.append(train_acc) # for plottting\n",
        "    train_loss_array.append(train_loss) # for plottting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0CHlPBF6uIhT"
      },
      "outputs": [],
      "source": [
        "def evaluate(epoch): # validation\n",
        "   \n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    valid_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "          \n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(val_loader), 'Valid Loss: %.3f | Valid Acc: %.3f%% (%d/%d)'\n",
        "                         % (valid_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    valid_acc = 100.*correct/total\n",
        "    if valid_acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net_state_dict': net.state_dict(),\n",
        "            'acc': valid_acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = valid_acc\n",
        "    val_acc_array.append(valid_acc) # for plottting\n",
        "    val_loss_array.append(valid_loss) # for plottting\n",
        "    return valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "G5i2dqetBgXp"
      },
      "outputs": [],
      "source": [
        "# Load the best model parameters (measured in terms of validation loss) and evaluate the loss/accuracy on the test set.\n",
        "def test(): \n",
        "   \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "    net.load_state_dict(checkpoint['net_state_dict'])\n",
        "    best_epoch = checkpoint['epoch']\n",
        "    best_acc = checkpoint['acc']\n",
        "    net.eval()\n",
        "    print(f'Best validation acc: {best_acc:.3f}% at Epoch {best_epoch}')\n",
        "    with torch.no_grad():\n",
        "          \n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs) \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(testloader), 'Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyuhb-GrXzUo",
        "outputId": "ccbf2e25-30ab-436d-cade-48bd9295b4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device  cuda\n"
          ]
        }
      ],
      "source": [
        "print(\"Using device \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79HcWh5aXzUq",
        "outputId": "b23136a8-001e-4d87-f413-718c34343418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters  1.9 million\n"
          ]
        }
      ],
      "source": [
        "print_params(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJcMkrBzW7o7",
        "outputId": "31d33665-d65f-40ec-c16c-09c225256cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [======>]  Step: 486ms | Tot: 26s900ms | Train Loss: 1.524 | Train Acc: 44.911% (20210/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 76ms | Tot: 1s947ms | Valid Loss: 1.401 | Valid Acc: 51.120% (2556/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            " [======>]  Step: 49ms | Tot: 27s3ms | Train Loss: 1.152 | Train Acc: 58.929% (26518/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 7ms | Tot: 1s914ms | Valid Loss: 1.236 | Valid Acc: 56.220% (2811/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            " [======>]  Step: 47ms | Tot: 27s758ms | Train Loss: 0.971 | Train Acc: 65.816% (29617/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 1s973ms | Valid Loss: 1.044 | Valid Acc: 64.580% (3229/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            " [======>]  Step: 47ms | Tot: 28s297ms | Train Loss: 0.859 | Train Acc: 69.996% (31498/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 1s963ms | Valid Loss: 0.878 | Valid Acc: 69.760% (3488/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 4\n",
            " [======>]  Step: 46ms | Tot: 27s756ms | Train Loss: 0.786 | Train Acc: 72.624% (32681/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 1s950ms | Valid Loss: 0.895 | Valid Acc: 68.820% (3441/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 5\n",
            " [======>]  Step: 50ms | Tot: 27s903ms | Train Loss: 0.727 | Train Acc: 74.669% (33601/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 1s930ms | Valid Loss: 0.795 | Valid Acc: 72.820% (3641/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 6\n",
            " [======>]  Step: 51ms | Tot: 27s940ms | Train Loss: 0.686 | Train Acc: 76.213% (34296/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 1s914ms | Valid Loss: 0.743 | Valid Acc: 75.040% (3752/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 7\n",
            " [======>]  Step: 48ms | Tot: 27s902ms | Train Loss: 0.655 | Train Acc: 77.236% (34756/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 7ms | Tot: 1s959ms | Valid Loss: 0.795 | Valid Acc: 72.840% (3642/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 8\n",
            " [======>]  Step: 48ms | Tot: 27s842ms | Train Loss: 0.619 | Train Acc: 78.549% (35347/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 7ms | Tot: 1s942ms | Valid Loss: 0.727 | Valid Acc: 75.120% (3756/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 9\n",
            " [======>]  Step: 51ms | Tot: 28s32ms | Train Loss: 0.590 | Train Acc: 79.571% (35807/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s249ms | Valid Loss: 0.713 | Valid Acc: 75.260% (3763/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 10\n",
            " [======>]  Step: 48ms | Tot: 27s930ms | Train Loss: 0.565 | Train Acc: 80.238% (36107/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 1s956ms | Valid Loss: 0.708 | Valid Acc: 76.500% (3825/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 11\n",
            " [======>]  Step: 51ms | Tot: 27s931ms | Train Loss: 0.555 | Train Acc: 80.504% (36227/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 1s950ms | Valid Loss: 0.604 | Valid Acc: 78.380% (3919/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 12\n",
            " [======>]  Step: 47ms | Tot: 27s829ms | Train Loss: 0.528 | Train Acc: 81.700% (36765/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 1s969ms | Valid Loss: 0.589 | Valid Acc: 79.620% (3981/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 13\n",
            " [======>]  Step: 48ms | Tot: 27s955ms | Train Loss: 0.517 | Train Acc: 81.931% (36869/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 1s935ms | Valid Loss: 0.561 | Valid Acc: 80.580% (4029/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 14\n",
            " [======>]  Step: 47ms | Tot: 27s944ms | Train Loss: 0.495 | Train Acc: 82.762% (37243/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 1s999ms | Valid Loss: 0.628 | Valid Acc: 79.140% (3957/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 15\n",
            " [======>]  Step: 48ms | Tot: 28s94ms | Train Loss: 0.483 | Train Acc: 83.296% (37483/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s109ms | Valid Loss: 0.592 | Valid Acc: 79.400% (3970/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 16\n",
            " [======>]  Step: 49ms | Tot: 27s849ms | Train Loss: 0.466 | Train Acc: 83.758% (37691/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 1s986ms | Valid Loss: 0.605 | Valid Acc: 79.620% (3981/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 17\n",
            " [======>]  Step: 48ms | Tot: 27s979ms | Train Loss: 0.454 | Train Acc: 84.204% (37892/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 1s930ms | Valid Loss: 0.590 | Valid Acc: 80.460% (4023/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 18\n",
            " [======>]  Step: 49ms | Tot: 27s950ms | Train Loss: 0.457 | Train Acc: 84.136% (37861/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 1s928ms | Valid Loss: 0.533 | Valid Acc: 81.440% (4072/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 19\n",
            " [======>]  Step: 48ms | Tot: 27s864ms | Train Loss: 0.433 | Train Acc: 84.924% (38216/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 1s924ms | Valid Loss: 0.549 | Valid Acc: 80.820% (4041/5000)\b\b\b\b 40/40 \n",
            "---------------------------------------- Testing Model... ----------------------------------------\n",
            "Best validation acc: 81.440% at Epoch 18\n",
            " [======>]  Step: 19ms | Tot: 2s306ms | Test Loss: 0.360 | Test Acc: 87.600% (8760/10000)\b\b\b\b 100/100 \n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + NUM_EPOCHS):\n",
        "    train(epoch)\n",
        "    evaluate(epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "print('---------------------------------------- Testing Model... ----------------------------------------')\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F9l3iSKXzUs",
        "outputId": "ce43f615-dfa7-4844-e22b-74e128597ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters  4.5 million\n"
          ]
        }
      ],
      "source": [
        "print_params(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRtpQiLSWkIT",
        "outputId": "12bd0a98-308f-4e6e-8303-fe0628a74de3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f4ff8a74ad0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z_TwWP4Xkca",
        "outputId": "0761cc04-4bf4-4fb6-dedf-8190b30539e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[44.91111111111111,\n",
              " 58.92888888888889,\n",
              " 65.81555555555556,\n",
              " 69.99555555555555,\n",
              " 72.62444444444445,\n",
              " 74.66888888888889,\n",
              " 76.21333333333334,\n",
              " 77.23555555555555,\n",
              " 78.54888888888888,\n",
              " 79.57111111111111,\n",
              " 80.23777777777778,\n",
              " 80.50444444444445,\n",
              " 81.7,\n",
              " 81.93111111111111,\n",
              " 82.76222222222222,\n",
              " 83.29555555555555,\n",
              " 83.75777777777778,\n",
              " 84.20444444444445,\n",
              " 84.13555555555556,\n",
              " 84.92444444444445]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_acc_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "JHKohhBDJRIP",
        "outputId": "ee8099c9-f661-449d-abeb-fb112f7ca069"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x432 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "fig = plt.figure(figsize=(16,6))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(np.arange(1,NUM_EPOCHS+1),train_acc_array)\n",
        "plt.plot(np.arange(1,NUM_EPOCHS+1),val_acc_array)\n",
        "plt.title(\"Accuray\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Train Acc\",\"Valid Acc\"],loc = \"upper right\")\n",
        "\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(np.arange(1,NUM_EPOCHS+1,dtype=int),train_loss_array)\n",
        "plt.plot(np.arange(1,NUM_EPOCHS+1,dtype=int),val_loss_array)\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(['train loss', 'valid loss'], loc=\"upper right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX1dmdIIuPOo"
      },
      "outputs": [],
      "source": [
        "# can you create a subset of params exclusing pruned weights ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUPyXSy3yD6G"
      },
      "source": [
        "<h2>Automatic Hyperparameter Search using Optuna</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aefZAQVvyCgU",
        "outputId": "0f5c6f2c-3302-483e-f340-3435e052f8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.43)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=2617ae5bf5e5f9a30d5c74a30b3b5cac665fe7770bd714b755fc746da344879d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.11.0 pyperclip-1.8.2 stevedore-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "j2MShrCMyLjz"
      },
      "outputs": [],
      "source": [
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MAKNVr4AyOPa"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "\n",
        "  # generate a model\n",
        "  model = ResNet18().to(device)\n",
        "\n",
        "  prune_model(model)\n",
        "\n",
        "  # trying different optimizers: SGD, Adam, Adadelta, Adagrad\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"Adadelta\",\"Adagrad\"])\n",
        "  momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
        "  lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "  optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "  curr_batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=64)\n",
        "\n",
        "  # defining a loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "    train_ds, batch_size=curr_batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "    val_ds, batch_size=curr_batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=curr_batch_size, shuffle=False, num_workers=2, drop_last=True)  \n",
        "  \n",
        "  NUM_EPOCHS = 35\n",
        "\n",
        "  for epoch in range(start_epoch, start_epoch + NUM_EPOCHS):\n",
        "      train(epoch)\n",
        "      accuracy = evaluate(epoch)\n",
        "      scheduler.step()\n",
        "      # print(\"Accuracy \", accuracy)\n",
        "      # print(\"epoch \", epoch)\n",
        "      trial.report(accuracy, epoch)\n",
        "\n",
        "      if trial.should_prune():\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: link database to the Optuna study"
      ],
      "metadata": {
        "id": "3WVQFDtkmJDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7S4oOdt1Rx4",
        "outputId": "e8026ecf-aa96-44e4-e36f-689c944d3fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-11-19 00:51:53,634]\u001b[0m A new study created in memory with name: resNet-18\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [======>]  Step: 342ms | Tot: 23s474ms | Train Loss: 1.782 | Train Acc: 34.271% (15422/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 68ms | Tot: 2s675ms | Valid Loss: 1.563 | Valid Acc: 42.960% (2148/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [======>]  Step: 39ms | Tot: 24s757ms | Train Loss: 1.295 | Train Acc: 53.484% (24068/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s203ms | Valid Loss: 1.522 | Valid Acc: 49.360% (2468/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            " [======>]  Step: 39ms | Tot: 23s771ms | Train Loss: 1.068 | Train Acc: 62.231% (28004/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s250ms | Valid Loss: 1.161 | Valid Acc: 58.680% (2934/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            " [======>]  Step: 36ms | Tot: 23s893ms | Train Loss: 0.928 | Train Acc: 67.396% (30328/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s224ms | Valid Loss: 1.116 | Valid Acc: 62.020% (3101/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 4\n",
            " [======>]  Step: 37ms | Tot: 23s187ms | Train Loss: 0.846 | Train Acc: 70.336% (31651/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s506ms | Valid Loss: 0.961 | Valid Acc: 66.140% (3307/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 5\n",
            " [======>]  Step: 39ms | Tot: 23s383ms | Train Loss: 0.781 | Train Acc: 72.727% (32727/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s263ms | Valid Loss: 0.834 | Valid Acc: 70.060% (3503/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 6\n",
            " [======>]  Step: 36ms | Tot: 23s727ms | Train Loss: 0.732 | Train Acc: 74.507% (33528/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s195ms | Valid Loss: 0.907 | Valid Acc: 68.520% (3426/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 7\n",
            " [======>]  Step: 38ms | Tot: 24s436ms | Train Loss: 0.695 | Train Acc: 75.620% (34029/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s225ms | Valid Loss: 0.785 | Valid Acc: 72.320% (3616/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 8\n",
            " [======>]  Step: 37ms | Tot: 23s484ms | Train Loss: 0.660 | Train Acc: 76.909% (34609/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s192ms | Valid Loss: 0.686 | Valid Acc: 76.820% (3841/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 9\n",
            " [======>]  Step: 37ms | Tot: 23s839ms | Train Loss: 0.633 | Train Acc: 78.020% (35109/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s261ms | Valid Loss: 0.725 | Valid Acc: 75.060% (3753/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 10\n",
            " [======>]  Step: 39ms | Tot: 23s799ms | Train Loss: 0.605 | Train Acc: 78.838% (35477/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s202ms | Valid Loss: 0.741 | Valid Acc: 74.180% (3709/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 11\n",
            " [======>]  Step: 39ms | Tot: 23s204ms | Train Loss: 0.587 | Train Acc: 79.658% (35846/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 2s590ms | Valid Loss: 0.678 | Valid Acc: 77.060% (3853/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 12\n",
            " [======>]  Step: 38ms | Tot: 23s26ms | Train Loss: 0.565 | Train Acc: 80.438% (36197/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s242ms | Valid Loss: 0.728 | Valid Acc: 75.240% (3762/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 13\n",
            " [======>]  Step: 38ms | Tot: 24s624ms | Train Loss: 0.549 | Train Acc: 80.900% (36405/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s254ms | Valid Loss: 0.659 | Valid Acc: 77.400% (3870/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 14\n",
            " [======>]  Step: 39ms | Tot: 23s810ms | Train Loss: 0.541 | Train Acc: 81.120% (36504/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 2s230ms | Valid Loss: 0.787 | Valid Acc: 73.380% (3669/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 15\n",
            " [======>]  Step: 38ms | Tot: 23s272ms | Train Loss: 0.513 | Train Acc: 82.271% (37022/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 16ms | Tot: 2s249ms | Valid Loss: 0.638 | Valid Acc: 77.480% (3874/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 16\n",
            " [======>]  Step: 39ms | Tot: 23s842ms | Train Loss: 0.504 | Train Acc: 82.580% (37161/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s214ms | Valid Loss: 0.643 | Valid Acc: 77.540% (3877/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 17\n",
            " [======>]  Step: 39ms | Tot: 23s937ms | Train Loss: 0.493 | Train Acc: 82.791% (37256/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s233ms | Valid Loss: 0.630 | Valid Acc: 78.600% (3930/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 18\n",
            " [======>]  Step: 39ms | Tot: 23s956ms | Train Loss: 0.483 | Train Acc: 83.209% (37444/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s240ms | Valid Loss: 0.634 | Valid Acc: 78.640% (3932/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 19\n",
            " [======>]  Step: 37ms | Tot: 23s941ms | Train Loss: 0.470 | Train Acc: 83.564% (37604/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s250ms | Valid Loss: 0.584 | Valid Acc: 80.900% (4045/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 20\n",
            " [======>]  Step: 38ms | Tot: 23s943ms | Train Loss: 0.463 | Train Acc: 83.920% (37764/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s239ms | Valid Loss: 0.621 | Valid Acc: 78.560% (3928/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 21\n",
            " [======>]  Step: 38ms | Tot: 24s117ms | Train Loss: 0.450 | Train Acc: 84.322% (37945/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s247ms | Valid Loss: 0.591 | Valid Acc: 80.100% (4005/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 22\n",
            " [======>]  Step: 37ms | Tot: 23s733ms | Train Loss: 0.444 | Train Acc: 84.733% (38130/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s477ms | Valid Loss: 0.517 | Valid Acc: 82.020% (4101/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 23\n",
            " [======>]  Step: 37ms | Tot: 23s419ms | Train Loss: 0.427 | Train Acc: 85.247% (38361/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s231ms | Valid Loss: 0.574 | Valid Acc: 81.240% (4062/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 24\n",
            " [======>]  Step: 40ms | Tot: 24s157ms | Train Loss: 0.430 | Train Acc: 84.980% (38241/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 14ms | Tot: 2s700ms | Valid Loss: 0.529 | Valid Acc: 81.600% (4080/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 25\n",
            " [======>]  Step: 39ms | Tot: 24s240ms | Train Loss: 0.414 | Train Acc: 85.593% (38517/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s268ms | Valid Loss: 0.550 | Valid Acc: 80.940% (4047/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 26\n",
            " [======>]  Step: 36ms | Tot: 23s249ms | Train Loss: 0.412 | Train Acc: 85.796% (38608/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s630ms | Valid Loss: 0.532 | Valid Acc: 81.080% (4054/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 27\n",
            " [======>]  Step: 35ms | Tot: 23s488ms | Train Loss: 0.405 | Train Acc: 85.960% (38682/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s251ms | Valid Loss: 0.534 | Valid Acc: 81.680% (4084/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 28\n",
            " [======>]  Step: 35ms | Tot: 24s241ms | Train Loss: 0.404 | Train Acc: 86.100% (38745/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s278ms | Valid Loss: 0.533 | Valid Acc: 82.060% (4103/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 29\n",
            " [======>]  Step: 39ms | Tot: 23s934ms | Train Loss: 0.398 | Train Acc: 86.296% (38833/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s282ms | Valid Loss: 0.510 | Valid Acc: 82.220% (4111/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 30\n",
            " [======>]  Step: 36ms | Tot: 24s81ms | Train Loss: 0.385 | Train Acc: 86.489% (38920/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s252ms | Valid Loss: 0.562 | Valid Acc: 81.080% (4054/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 31\n",
            " [======>]  Step: 37ms | Tot: 24s356ms | Train Loss: 0.379 | Train Acc: 86.798% (39059/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s246ms | Valid Loss: 0.518 | Valid Acc: 81.920% (4096/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 32\n",
            " [======>]  Step: 38ms | Tot: 24s305ms | Train Loss: 0.380 | Train Acc: 86.744% (39035/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s282ms | Valid Loss: 0.524 | Valid Acc: 82.060% (4103/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 33\n",
            " [======>]  Step: 41ms | Tot: 24s239ms | Train Loss: 0.376 | Train Acc: 86.847% (39081/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s306ms | Valid Loss: 0.522 | Valid Acc: 82.380% (4119/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 34\n",
            " [======>]  Step: 38ms | Tot: 23s343ms | Train Loss: 0.367 | Train Acc: 87.182% (39232/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s261ms | Valid Loss: 0.514 | Valid Acc: 82.800% (4140/5000)\b\b\b\b 40/40 \n",
            "Saving..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-11-19 01:07:31,956]\u001b[0m Trial 0 finished with value: 82.8 and parameters: {'optimizer': 'SGD', 'momentum': 0.07654158383381116, 'lr': 4.4993780693007834e-05, 'batch_size': 64}. Best is trial 0 with value: 82.8.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [======>]  Step: 38ms | Tot: 24s140ms | Train Loss: 0.368 | Train Acc: 87.204% (39242/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s288ms | Valid Loss: 0.500 | Valid Acc: 82.800% (4140/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 1\n",
            " [======>]  Step: 37ms | Tot: 25s46ms | Train Loss: 0.359 | Train Acc: 87.589% (39415/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s278ms | Valid Loss: 0.531 | Valid Acc: 82.260% (4113/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 2\n",
            " [======>]  Step: 41ms | Tot: 24s345ms | Train Loss: 0.352 | Train Acc: 87.864% (39539/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s488ms | Valid Loss: 0.455 | Valid Acc: 83.960% (4198/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            " [======>]  Step: 38ms | Tot: 23s458ms | Train Loss: 0.351 | Train Acc: 87.784% (39503/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s222ms | Valid Loss: 0.527 | Valid Acc: 82.020% (4101/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 4\n",
            " [======>]  Step: 38ms | Tot: 24s405ms | Train Loss: 0.347 | Train Acc: 87.980% (39591/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s278ms | Valid Loss: 0.544 | Valid Acc: 82.700% (4135/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 5\n",
            " [======>]  Step: 37ms | Tot: 24s176ms | Train Loss: 0.344 | Train Acc: 88.038% (39617/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s293ms | Valid Loss: 0.458 | Valid Acc: 83.980% (4199/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 6\n",
            " [======>]  Step: 38ms | Tot: 23s510ms | Train Loss: 0.341 | Train Acc: 88.140% (39663/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s704ms | Valid Loss: 0.527 | Valid Acc: 82.700% (4135/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 7\n",
            " [======>]  Step: 36ms | Tot: 24s130ms | Train Loss: 0.330 | Train Acc: 88.511% (39830/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s242ms | Valid Loss: 0.485 | Valid Acc: 83.100% (4155/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 8\n",
            " [======>]  Step: 39ms | Tot: 24s60ms | Train Loss: 0.338 | Train Acc: 88.349% (39757/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s204ms | Valid Loss: 0.476 | Valid Acc: 83.460% (4173/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 9\n",
            " [======>]  Step: 40ms | Tot: 23s830ms | Train Loss: 0.330 | Train Acc: 88.391% (39776/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s224ms | Valid Loss: 0.478 | Valid Acc: 83.960% (4198/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 10\n",
            " [======>]  Step: 38ms | Tot: 23s699ms | Train Loss: 0.333 | Train Acc: 88.456% (39805/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s330ms | Valid Loss: 0.465 | Valid Acc: 84.660% (4233/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 11\n",
            " [======>]  Step: 35ms | Tot: 24s331ms | Train Loss: 0.324 | Train Acc: 88.647% (39891/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s251ms | Valid Loss: 0.500 | Valid Acc: 83.500% (4175/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 12\n",
            " [======>]  Step: 37ms | Tot: 23s802ms | Train Loss: 0.326 | Train Acc: 88.678% (39905/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s510ms | Valid Loss: 0.500 | Valid Acc: 83.840% (4192/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 13\n",
            " [======>]  Step: 41ms | Tot: 24s450ms | Train Loss: 0.323 | Train Acc: 88.787% (39954/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s383ms | Valid Loss: 0.462 | Valid Acc: 84.800% (4240/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 14\n",
            " [======>]  Step: 38ms | Tot: 23s527ms | Train Loss: 0.321 | Train Acc: 88.887% (39999/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s248ms | Valid Loss: 0.496 | Valid Acc: 83.760% (4188/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 15\n",
            " [======>]  Step: 37ms | Tot: 24s131ms | Train Loss: 0.311 | Train Acc: 89.131% (40109/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s309ms | Valid Loss: 0.471 | Valid Acc: 83.760% (4188/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 16\n",
            " [======>]  Step: 38ms | Tot: 24s169ms | Train Loss: 0.313 | Train Acc: 89.218% (40148/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s227ms | Valid Loss: 0.500 | Valid Acc: 83.740% (4187/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 17\n",
            " [======>]  Step: 40ms | Tot: 23s824ms | Train Loss: 0.310 | Train Acc: 89.207% (40143/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s770ms | Valid Loss: 0.476 | Valid Acc: 84.380% (4219/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 18\n",
            " [======>]  Step: 37ms | Tot: 24s72ms | Train Loss: 0.309 | Train Acc: 89.207% (40143/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s273ms | Valid Loss: 0.480 | Valid Acc: 83.440% (4172/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 19\n",
            " [======>]  Step: 34ms | Tot: 24s186ms | Train Loss: 0.310 | Train Acc: 89.136% (40111/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s288ms | Valid Loss: 0.455 | Valid Acc: 84.680% (4234/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 20\n",
            " [======>]  Step: 38ms | Tot: 24s63ms | Train Loss: 0.310 | Train Acc: 89.331% (40199/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s259ms | Valid Loss: 0.497 | Valid Acc: 83.020% (4151/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 21\n",
            " [======>]  Step: 42ms | Tot: 23s340ms | Train Loss: 0.304 | Train Acc: 89.427% (40242/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s791ms | Valid Loss: 0.448 | Valid Acc: 84.340% (4217/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 22\n",
            " [======>]  Step: 38ms | Tot: 23s823ms | Train Loss: 0.298 | Train Acc: 89.656% (40345/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s298ms | Valid Loss: 0.468 | Valid Acc: 83.920% (4196/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 23\n",
            " [======>]  Step: 37ms | Tot: 24s329ms | Train Loss: 0.298 | Train Acc: 89.647% (40341/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s290ms | Valid Loss: 0.488 | Valid Acc: 83.820% (4191/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 24\n",
            " [======>]  Step: 40ms | Tot: 24s891ms | Train Loss: 0.301 | Train Acc: 89.620% (40329/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s253ms | Valid Loss: 0.478 | Valid Acc: 84.040% (4202/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 25\n",
            " [======>]  Step: 37ms | Tot: 23s753ms | Train Loss: 0.299 | Train Acc: 89.773% (40398/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s360ms | Valid Loss: 0.508 | Valid Acc: 82.840% (4142/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 26\n",
            " [======>]  Step: 38ms | Tot: 24s167ms | Train Loss: 0.290 | Train Acc: 89.973% (40488/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s263ms | Valid Loss: 0.466 | Valid Acc: 84.280% (4214/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 27\n",
            " [======>]  Step: 36ms | Tot: 24s532ms | Train Loss: 0.292 | Train Acc: 89.904% (40457/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s314ms | Valid Loss: 0.513 | Valid Acc: 82.880% (4144/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 28\n",
            " [======>]  Step: 37ms | Tot: 24s286ms | Train Loss: 0.286 | Train Acc: 90.173% (40578/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s280ms | Valid Loss: 0.426 | Valid Acc: 85.360% (4268/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 29\n",
            " [======>]  Step: 35ms | Tot: 23s616ms | Train Loss: 0.285 | Train Acc: 90.104% (40547/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s424ms | Valid Loss: 0.509 | Valid Acc: 84.100% (4205/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 30\n",
            " [======>]  Step: 38ms | Tot: 24s707ms | Train Loss: 0.284 | Train Acc: 89.922% (40465/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s280ms | Valid Loss: 0.428 | Valid Acc: 85.620% (4281/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 31\n",
            " [======>]  Step: 39ms | Tot: 24s353ms | Train Loss: 0.284 | Train Acc: 90.267% (40620/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s259ms | Valid Loss: 0.468 | Valid Acc: 84.480% (4224/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 32\n",
            " [======>]  Step: 38ms | Tot: 24s365ms | Train Loss: 0.291 | Train Acc: 90.007% (40503/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s249ms | Valid Loss: 0.440 | Valid Acc: 84.920% (4246/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 33\n",
            " [======>]  Step: 40ms | Tot: 23s532ms | Train Loss: 0.281 | Train Acc: 90.244% (40610/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 14ms | Tot: 2s287ms | Valid Loss: 0.497 | Valid Acc: 83.480% (4174/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 34\n",
            " [======>]  Step: 41ms | Tot: 23s873ms | Train Loss: 0.281 | Train Acc: 90.256% (40615/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s314ms | Valid Loss: 0.458 | Valid Acc: 85.480% (4274/5000)\b\b\b\b 40/40 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-11-19 01:23:18,927]\u001b[0m Trial 1 finished with value: 85.48 and parameters: {'optimizer': 'SGD', 'momentum': 0.6672748982980846, 'lr': 0.00010817088395568206, 'batch_size': 192}. Best is trial 1 with value: 85.48.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [======>]  Step: 40ms | Tot: 24s863ms | Train Loss: 0.281 | Train Acc: 90.318% (40643/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s412ms | Valid Loss: 0.467 | Valid Acc: 84.900% (4245/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 1\n",
            " [======>]  Step: 39ms | Tot: 24s72ms | Train Loss: 0.282 | Train Acc: 90.042% (40519/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 15ms | Tot: 2s231ms | Valid Loss: 0.439 | Valid Acc: 85.380% (4269/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 2\n",
            " [======>]  Step: 41ms | Tot: 23s812ms | Train Loss: 0.282 | Train Acc: 90.351% (40658/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s238ms | Valid Loss: 0.433 | Valid Acc: 85.440% (4272/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 3\n",
            " [======>]  Step: 37ms | Tot: 24s177ms | Train Loss: 0.278 | Train Acc: 90.409% (40684/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s279ms | Valid Loss: 0.451 | Valid Acc: 84.820% (4241/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 4\n",
            " [======>]  Step: 37ms | Tot: 23s982ms | Train Loss: 0.281 | Train Acc: 90.282% (40627/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s243ms | Valid Loss: 0.427 | Valid Acc: 85.320% (4266/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 5\n",
            " [======>]  Step: 42ms | Tot: 23s872ms | Train Loss: 0.279 | Train Acc: 90.300% (40635/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s459ms | Valid Loss: 0.454 | Valid Acc: 85.240% (4262/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 6\n",
            " [======>]  Step: 39ms | Tot: 24s42ms | Train Loss: 0.279 | Train Acc: 90.389% (40675/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 14ms | Tot: 2s237ms | Valid Loss: 0.465 | Valid Acc: 85.660% (4283/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 7\n",
            " [======>]  Step: 38ms | Tot: 24s622ms | Train Loss: 0.279 | Train Acc: 90.189% (40585/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s197ms | Valid Loss: 0.478 | Valid Acc: 83.920% (4196/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 8\n",
            " [======>]  Step: 38ms | Tot: 24s207ms | Train Loss: 0.282 | Train Acc: 90.242% (40609/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s289ms | Valid Loss: 0.427 | Valid Acc: 85.640% (4282/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 9\n",
            " [======>]  Step: 37ms | Tot: 23s847ms | Train Loss: 0.274 | Train Acc: 90.398% (40679/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s783ms | Valid Loss: 0.487 | Valid Acc: 84.180% (4209/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 10\n",
            " [======>]  Step: 40ms | Tot: 23s714ms | Train Loss: 0.271 | Train Acc: 90.716% (40822/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s313ms | Valid Loss: 0.481 | Valid Acc: 83.760% (4188/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 11\n",
            " [======>]  Step: 37ms | Tot: 24s418ms | Train Loss: 0.274 | Train Acc: 90.460% (40707/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s254ms | Valid Loss: 0.447 | Valid Acc: 85.200% (4260/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 12\n",
            " [======>]  Step: 37ms | Tot: 24s930ms | Train Loss: 0.271 | Train Acc: 90.567% (40755/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 2s257ms | Valid Loss: 0.428 | Valid Acc: 85.100% (4255/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 13\n",
            " [======>]  Step: 37ms | Tot: 23s951ms | Train Loss: 0.265 | Train Acc: 90.989% (40945/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s679ms | Valid Loss: 0.451 | Valid Acc: 84.700% (4235/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 14\n",
            " [======>]  Step: 40ms | Tot: 23s817ms | Train Loss: 0.268 | Train Acc: 90.684% (40808/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s242ms | Valid Loss: 0.447 | Valid Acc: 84.940% (4247/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 15\n",
            " [======>]  Step: 36ms | Tot: 24s234ms | Train Loss: 0.275 | Train Acc: 90.500% (40725/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s288ms | Valid Loss: 0.492 | Valid Acc: 83.860% (4193/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 16\n",
            " [======>]  Step: 39ms | Tot: 24s92ms | Train Loss: 0.268 | Train Acc: 90.713% (40821/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s299ms | Valid Loss: 0.455 | Valid Acc: 85.280% (4264/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 17\n",
            " [======>]  Step: 40ms | Tot: 23s481ms | Train Loss: 0.269 | Train Acc: 90.933% (40920/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s239ms | Valid Loss: 0.458 | Valid Acc: 85.220% (4261/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 18\n",
            " [======>]  Step: 38ms | Tot: 25s93ms | Train Loss: 0.268 | Train Acc: 90.687% (40809/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s287ms | Valid Loss: 0.438 | Valid Acc: 85.000% (4250/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 19\n",
            " [======>]  Step: 37ms | Tot: 24s61ms | Train Loss: 0.263 | Train Acc: 90.887% (40899/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s326ms | Valid Loss: 0.477 | Valid Acc: 84.260% (4213/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 20\n",
            " [======>]  Step: 38ms | Tot: 23s912ms | Train Loss: 0.265 | Train Acc: 90.913% (40911/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s644ms | Valid Loss: 0.468 | Valid Acc: 85.480% (4274/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 21\n",
            " [======>]  Step: 38ms | Tot: 23s561ms | Train Loss: 0.271 | Train Acc: 90.504% (40727/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s228ms | Valid Loss: 0.444 | Valid Acc: 84.920% (4246/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 22\n",
            " [======>]  Step: 38ms | Tot: 24s109ms | Train Loss: 0.263 | Train Acc: 90.969% (40936/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s249ms | Valid Loss: 0.469 | Valid Acc: 84.680% (4234/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 23\n",
            " [======>]  Step: 36ms | Tot: 24s737ms | Train Loss: 0.261 | Train Acc: 90.984% (40943/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 15ms | Tot: 2s249ms | Valid Loss: 0.451 | Valid Acc: 85.140% (4257/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 24\n",
            " [======>]  Step: 41ms | Tot: 23s392ms | Train Loss: 0.269 | Train Acc: 90.658% (40796/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 14ms | Tot: 2s862ms | Valid Loss: 0.439 | Valid Acc: 85.640% (4282/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 25\n",
            " [======>]  Step: 36ms | Tot: 23s832ms | Train Loss: 0.261 | Train Acc: 90.902% (40906/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s277ms | Valid Loss: 0.467 | Valid Acc: 85.160% (4258/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 26\n",
            " [======>]  Step: 38ms | Tot: 24s215ms | Train Loss: 0.257 | Train Acc: 91.213% (41046/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s291ms | Valid Loss: 0.480 | Valid Acc: 84.960% (4248/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 27\n",
            " [======>]  Step: 39ms | Tot: 24s668ms | Train Loss: 0.261 | Train Acc: 90.958% (40931/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s278ms | Valid Loss: 0.435 | Valid Acc: 86.060% (4303/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 28\n",
            " [======>]  Step: 35ms | Tot: 23s560ms | Train Loss: 0.262 | Train Acc: 90.904% (40907/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s315ms | Valid Loss: 0.486 | Valid Acc: 83.900% (4195/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 29\n",
            " [======>]  Step: 39ms | Tot: 24s725ms | Train Loss: 0.262 | Train Acc: 91.062% (40978/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 15ms | Tot: 2s310ms | Valid Loss: 0.483 | Valid Acc: 84.360% (4218/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 30\n",
            " [======>]  Step: 37ms | Tot: 24s387ms | Train Loss: 0.263 | Train Acc: 90.804% (40862/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s266ms | Valid Loss: 0.492 | Valid Acc: 84.740% (4237/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 31\n",
            " [======>]  Step: 35ms | Tot: 24s224ms | Train Loss: 0.261 | Train Acc: 90.982% (40942/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s297ms | Valid Loss: 0.406 | Valid Acc: 86.220% (4311/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 32\n",
            " [======>]  Step: 37ms | Tot: 23s693ms | Train Loss: 0.256 | Train Acc: 91.198% (41039/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 14ms | Tot: 2s350ms | Valid Loss: 0.432 | Valid Acc: 85.480% (4274/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 33\n",
            " [======>]  Step: 37ms | Tot: 23s976ms | Train Loss: 0.259 | Train Acc: 90.991% (40946/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s302ms | Valid Loss: 0.433 | Valid Acc: 85.260% (4263/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 34\n",
            " [======>]  Step: 36ms | Tot: 24s362ms | Train Loss: 0.252 | Train Acc: 91.231% (41054/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s280ms | Valid Loss: 0.426 | Valid Acc: 85.420% (4271/5000)\b\b\b\b 40/40 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-11-19 01:39:07,557]\u001b[0m Trial 2 finished with value: 85.42 and parameters: {'optimizer': 'Adadelta', 'momentum': 0.9766171861231084, 'lr': 0.08722500442910044, 'batch_size': 64}. Best is trial 1 with value: 85.48.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [======>]  Step: 36ms | Tot: 24s603ms | Train Loss: 0.258 | Train Acc: 91.036% (40966/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s249ms | Valid Loss: 0.404 | Valid Acc: 86.320% (4316/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            " [======>]  Step: 39ms | Tot: 23s504ms | Train Loss: 0.257 | Train Acc: 91.140% (41013/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s216ms | Valid Loss: 0.556 | Valid Acc: 83.520% (4176/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 2\n",
            " [======>]  Step: 38ms | Tot: 24s438ms | Train Loss: 0.260 | Train Acc: 90.978% (40940/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s283ms | Valid Loss: 0.434 | Valid Acc: 85.540% (4277/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 3\n",
            " [======>]  Step: 40ms | Tot: 24s528ms | Train Loss: 0.257 | Train Acc: 91.013% (40956/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s295ms | Valid Loss: 0.426 | Valid Acc: 85.960% (4298/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 4\n",
            " [======>]  Step: 39ms | Tot: 24s55ms | Train Loss: 0.254 | Train Acc: 91.202% (41041/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s457ms | Valid Loss: 0.455 | Valid Acc: 85.760% (4288/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 5\n",
            " [======>]  Step: 39ms | Tot: 24s515ms | Train Loss: 0.258 | Train Acc: 91.116% (41002/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s220ms | Valid Loss: 0.446 | Valid Acc: 84.900% (4245/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 6\n",
            " [======>]  Step: 39ms | Tot: 24s592ms | Train Loss: 0.252 | Train Acc: 91.480% (41166/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s260ms | Valid Loss: 0.456 | Valid Acc: 85.100% (4255/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 7\n",
            " [======>]  Step: 38ms | Tot: 24s397ms | Train Loss: 0.254 | Train Acc: 91.122% (41005/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s316ms | Valid Loss: 0.505 | Valid Acc: 83.620% (4181/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 8\n",
            " [======>]  Step: 36ms | Tot: 23s617ms | Train Loss: 0.248 | Train Acc: 91.478% (41165/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s718ms | Valid Loss: 0.419 | Valid Acc: 85.800% (4290/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 9\n",
            " [======>]  Step: 36ms | Tot: 23s330ms | Train Loss: 0.259 | Train Acc: 91.104% (40997/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s235ms | Valid Loss: 0.451 | Valid Acc: 85.260% (4263/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 10\n",
            " [======>]  Step: 38ms | Tot: 24s578ms | Train Loss: 0.258 | Train Acc: 90.989% (40945/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s273ms | Valid Loss: 0.483 | Valid Acc: 84.460% (4223/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 11\n",
            " [======>]  Step: 39ms | Tot: 24s819ms | Train Loss: 0.249 | Train Acc: 91.340% (41103/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s226ms | Valid Loss: 0.452 | Valid Acc: 85.000% (4250/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 12\n",
            " [======>]  Step: 41ms | Tot: 23s512ms | Train Loss: 0.252 | Train Acc: 91.258% (41066/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s541ms | Valid Loss: 0.457 | Valid Acc: 85.920% (4296/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 13\n",
            " [======>]  Step: 40ms | Tot: 23s867ms | Train Loss: 0.256 | Train Acc: 91.140% (41013/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s227ms | Valid Loss: 0.467 | Valid Acc: 85.640% (4282/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 14\n",
            " [======>]  Step: 39ms | Tot: 24s336ms | Train Loss: 0.253 | Train Acc: 91.211% (41045/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s296ms | Valid Loss: 0.494 | Valid Acc: 84.220% (4211/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 15\n",
            " [======>]  Step: 41ms | Tot: 24s1ms | Train Loss: 0.246 | Train Acc: 91.531% (41189/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s457ms | Valid Loss: 0.501 | Valid Acc: 84.600% (4230/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 16\n",
            " [======>]  Step: 37ms | Tot: 23s810ms | Train Loss: 0.247 | Train Acc: 91.416% (41137/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s270ms | Valid Loss: 0.449 | Valid Acc: 85.840% (4292/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 17\n",
            " [======>]  Step: 38ms | Tot: 25s294ms | Train Loss: 0.252 | Train Acc: 91.316% (41092/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s318ms | Valid Loss: 0.452 | Valid Acc: 85.700% (4285/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 18\n",
            " [======>]  Step: 36ms | Tot: 24s336ms | Train Loss: 0.251 | Train Acc: 91.324% (41096/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s255ms | Valid Loss: 0.466 | Valid Acc: 85.480% (4274/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 19\n",
            " [======>]  Step: 40ms | Tot: 24s175ms | Train Loss: 0.250 | Train Acc: 91.453% (41154/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s619ms | Valid Loss: 0.391 | Valid Acc: 86.680% (4334/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 20\n",
            " [======>]  Step: 39ms | Tot: 23s978ms | Train Loss: 0.246 | Train Acc: 91.589% (41215/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s278ms | Valid Loss: 0.433 | Valid Acc: 85.420% (4271/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 21\n",
            " [======>]  Step: 37ms | Tot: 24s355ms | Train Loss: 0.244 | Train Acc: 91.684% (41258/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s287ms | Valid Loss: 0.483 | Valid Acc: 84.640% (4232/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 22\n",
            " [======>]  Step: 40ms | Tot: 24s973ms | Train Loss: 0.252 | Train Acc: 91.329% (41098/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s331ms | Valid Loss: 0.497 | Valid Acc: 84.120% (4206/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 23\n",
            " [======>]  Step: 41ms | Tot: 23s618ms | Train Loss: 0.245 | Train Acc: 91.378% (41120/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s724ms | Valid Loss: 0.412 | Valid Acc: 86.680% (4334/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 24\n",
            " [======>]  Step: 38ms | Tot: 23s583ms | Train Loss: 0.242 | Train Acc: 91.624% (41231/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 8ms | Tot: 2s258ms | Valid Loss: 0.434 | Valid Acc: 85.960% (4298/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 25\n",
            " [======>]  Step: 37ms | Tot: 24s240ms | Train Loss: 0.255 | Train Acc: 91.069% (40981/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s308ms | Valid Loss: 0.449 | Valid Acc: 86.060% (4303/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 26\n",
            " [======>]  Step: 36ms | Tot: 24s463ms | Train Loss: 0.249 | Train Acc: 91.442% (41149/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s261ms | Valid Loss: 0.421 | Valid Acc: 86.120% (4306/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 27\n",
            " [======>]  Step: 37ms | Tot: 23s624ms | Train Loss: 0.252 | Train Acc: 91.360% (41112/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s634ms | Valid Loss: 0.450 | Valid Acc: 85.820% (4291/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 28\n",
            " [======>]  Step: 37ms | Tot: 24s315ms | Train Loss: 0.248 | Train Acc: 91.473% (41163/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s260ms | Valid Loss: 0.462 | Valid Acc: 85.460% (4273/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 29\n",
            " [======>]  Step: 41ms | Tot: 24s154ms | Train Loss: 0.251 | Train Acc: 91.362% (41113/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s308ms | Valid Loss: 0.432 | Valid Acc: 85.260% (4263/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 30\n",
            " [======>]  Step: 40ms | Tot: 24s423ms | Train Loss: 0.246 | Train Acc: 91.549% (41197/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s257ms | Valid Loss: 0.465 | Valid Acc: 85.260% (4263/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 31\n",
            " [======>]  Step: 38ms | Tot: 23s479ms | Train Loss: 0.249 | Train Acc: 91.398% (41129/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s216ms | Valid Loss: 0.463 | Valid Acc: 84.860% (4243/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 32\n",
            " [======>]  Step: 41ms | Tot: 24s411ms | Train Loss: 0.243 | Train Acc: 91.518% (41183/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s282ms | Valid Loss: 0.442 | Valid Acc: 85.640% (4282/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 33\n",
            " [======>]  Step: 40ms | Tot: 24s277ms | Train Loss: 0.245 | Train Acc: 91.560% (41202/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 9ms | Tot: 2s226ms | Valid Loss: 0.411 | Valid Acc: 86.840% (4342/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 34\n",
            " [======>]  Step: 41ms | Tot: 24s900ms | Train Loss: 0.242 | Train Acc: 91.504% (41177/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s732ms | Valid Loss: 0.483 | Valid Acc: 84.820% (4241/5000)\b\b\b\b 40/40 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-11-19 01:54:59,025]\u001b[0m Trial 3 finished with value: 84.82 and parameters: {'optimizer': 'Adagrad', 'momentum': 0.6502301545622698, 'lr': 0.0005198707758856726, 'batch_size': 128}. Best is trial 1 with value: 85.48.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [======>]  Step: 38ms | Tot: 26s511ms | Train Loss: 0.251 | Train Acc: 91.307% (41088/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 15ms | Tot: 2s543ms | Valid Loss: 0.449 | Valid Acc: 85.320% (4266/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 1\n",
            " [======>]  Step: 40ms | Tot: 24s98ms | Train Loss: 0.241 | Train Acc: 91.698% (41264/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s271ms | Valid Loss: 0.484 | Valid Acc: 84.580% (4229/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 2\n",
            " [======>]  Step: 38ms | Tot: 24s256ms | Train Loss: 0.241 | Train Acc: 91.591% (41216/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s270ms | Valid Loss: 0.419 | Valid Acc: 86.220% (4311/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 3\n",
            " [======>]  Step: 35ms | Tot: 24s195ms | Train Loss: 0.245 | Train Acc: 91.478% (41165/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s220ms | Valid Loss: 0.404 | Valid Acc: 86.840% (4342/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 4\n",
            " [======>]  Step: 40ms | Tot: 24s195ms | Train Loss: 0.237 | Train Acc: 91.820% (41319/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s332ms | Valid Loss: 0.441 | Valid Acc: 85.260% (4263/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 5\n",
            " [======>]  Step: 39ms | Tot: 24s94ms | Train Loss: 0.249 | Train Acc: 91.393% (41127/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s329ms | Valid Loss: 0.446 | Valid Acc: 85.240% (4262/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 6\n",
            " [======>]  Step: 39ms | Tot: 24s401ms | Train Loss: 0.238 | Train Acc: 91.858% (41336/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 10ms | Tot: 2s233ms | Valid Loss: 0.452 | Valid Acc: 85.840% (4292/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 7\n",
            " [======>]  Step: 37ms | Tot: 24s308ms | Train Loss: 0.244 | Train Acc: 91.553% (41199/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s341ms | Valid Loss: 0.453 | Valid Acc: 84.720% (4236/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 8\n",
            " [======>]  Step: 38ms | Tot: 23s853ms | Train Loss: 0.243 | Train Acc: 91.613% (41226/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s315ms | Valid Loss: 0.458 | Valid Acc: 85.140% (4257/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 9\n",
            " [======>]  Step: 37ms | Tot: 24s355ms | Train Loss: 0.239 | Train Acc: 91.698% (41264/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s244ms | Valid Loss: 0.416 | Valid Acc: 86.500% (4325/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 10\n",
            " [======>]  Step: 36ms | Tot: 25s151ms | Train Loss: 0.238 | Train Acc: 91.693% (41262/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s280ms | Valid Loss: 0.493 | Valid Acc: 84.960% (4248/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 11\n",
            " [======>]  Step: 37ms | Tot: 24s421ms | Train Loss: 0.241 | Train Acc: 91.764% (41294/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s328ms | Valid Loss: 0.443 | Valid Acc: 85.580% (4279/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 12\n",
            " [======>]  Step: 38ms | Tot: 23s824ms | Train Loss: 0.237 | Train Acc: 91.751% (41288/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 12ms | Tot: 2s242ms | Valid Loss: 0.479 | Valid Acc: 84.540% (4227/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 13\n",
            " [======>]  Step: 36ms | Tot: 24s844ms | Train Loss: 0.245 | Train Acc: 91.567% (41205/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s306ms | Valid Loss: 0.446 | Valid Acc: 85.640% (4282/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 14\n",
            " [======>]  Step: 38ms | Tot: 24s274ms | Train Loss: 0.231 | Train Acc: 91.962% (41383/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s308ms | Valid Loss: 0.446 | Valid Acc: 85.560% (4278/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 15\n",
            " [======>]  Step: 40ms | Tot: 24s425ms | Train Loss: 0.240 | Train Acc: 91.789% (41305/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 14ms | Tot: 2s713ms | Valid Loss: 0.491 | Valid Acc: 84.180% (4209/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 16\n",
            " [======>]  Step: 38ms | Tot: 24s154ms | Train Loss: 0.241 | Train Acc: 91.736% (41281/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s299ms | Valid Loss: 0.444 | Valid Acc: 85.180% (4259/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 17\n",
            " [======>]  Step: 38ms | Tot: 24s675ms | Train Loss: 0.234 | Train Acc: 91.958% (41381/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 13ms | Tot: 2s329ms | Valid Loss: 0.416 | Valid Acc: 86.140% (4307/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 18\n",
            " [======>]  Step: 38ms | Tot: 24s560ms | Train Loss: 0.236 | Train Acc: 91.962% (41383/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s361ms | Valid Loss: 0.408 | Valid Acc: 86.900% (4345/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 19\n",
            " [======>]  Step: 35ms | Tot: 24s724ms | Train Loss: 0.236 | Train Acc: 91.753% (41289/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s318ms | Valid Loss: 0.407 | Valid Acc: 87.460% (4373/5000)\b\b\b\b 40/40 \n",
            "Saving..\n",
            "\n",
            "Epoch: 20\n",
            " [======>]  Step: 38ms | Tot: 23s993ms | Train Loss: 0.241 | Train Acc: 91.629% (41233/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 15ms | Tot: 2s358ms | Valid Loss: 0.486 | Valid Acc: 85.700% (4285/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 21\n",
            " [======>]  Step: 39ms | Tot: 25s159ms | Train Loss: 0.243 | Train Acc: 91.558% (41201/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s302ms | Valid Loss: 0.419 | Valid Acc: 86.140% (4307/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 22\n",
            " [======>]  Step: 36ms | Tot: 24s212ms | Train Loss: 0.232 | Train Acc: 91.951% (41378/45000)\b\b\b\b 352/352 \n",
            " [======>]  Step: 11ms | Tot: 2s323ms | Valid Loss: 0.435 | Valid Acc: 85.680% (4284/5000)\b\b\b\b 40/40 \n",
            "\n",
            "Epoch: 23\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), study_name=\"resNet-18\")\n",
        "\n",
        "study.optimize(objective, n_trials = 30)\n",
        "\n",
        "trial = study.best_trial\n",
        "\n",
        "print('Accuracy: {}'.format(trial.value))\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RphsqTa1uv_"
      },
      "outputs": [],
      "source": [
        "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete','duration','number'], axis=1)\n",
        "df.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMVKiy5_1lQz"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttCK-I3f1olz"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_contour(study, params=['batch_size', 'lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2J9SpQJ1sy5"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3tpr4KK2RVI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9cddfc3bc9d458e83054c3175b478ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b49d88b0fea6496592f35fddf0826c24",
              "IPY_MODEL_fe6a906e76f3494baf5e4a109daa4e9c",
              "IPY_MODEL_a9e02dc71a1f4283ac6086f30f354490"
            ],
            "layout": "IPY_MODEL_374936fc9fb54190a370b476131bf5c6"
          }
        },
        "b49d88b0fea6496592f35fddf0826c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d3ee5109c5c4fc0b47db02deca1f2f5",
            "placeholder": "​",
            "style": "IPY_MODEL_79c500cbc94a4fcbbed6e968a01c760a",
            "value": "100%"
          }
        },
        "fe6a906e76f3494baf5e4a109daa4e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff5988d1a5cb4ac8a0fa6b69bc2cb428",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_160869b4947e4dc6b74fe1856d6a3853",
            "value": 170498071
          }
        },
        "a9e02dc71a1f4283ac6086f30f354490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d407f071df2445e866ad8ed86f32afc",
            "placeholder": "​",
            "style": "IPY_MODEL_13a0c8f358234c4e8bcb1ccf4e40eda1",
            "value": " 170498071/170498071 [00:02&lt;00:00, 73101614.46it/s]"
          }
        },
        "374936fc9fb54190a370b476131bf5c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d3ee5109c5c4fc0b47db02deca1f2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79c500cbc94a4fcbbed6e968a01c760a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff5988d1a5cb4ac8a0fa6b69bc2cb428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160869b4947e4dc6b74fe1856d6a3853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d407f071df2445e866ad8ed86f32afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13a0c8f358234c4e8bcb1ccf4e40eda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}