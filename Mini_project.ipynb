{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "import ssl\n",
    "def set_up_ssl():\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "set_up_ssl()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        # pruning model parameters in the first convolution layer\n",
    "        prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv1, 'weight')\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        # pruning model parameters in the second convolution layer\n",
    "        prune.random_unstructured(self.conv2, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv2, 'weight')\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv1, 'weight')\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        prune.random_unstructured(self.conv2, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv2, 'weight')\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        prune.random_unstructured(self.conv3, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv3, 'weight')\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv1, 'weight')\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        prune.random_unstructured(self.linear, name='weight', amount=0.5)\n",
    "        prune.remove(self.linear, 'weight')\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n"
   ],
   "metadata": {
    "id": "wTLc4fVcQ857"
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "term_width = 5\n",
    "TOTAL_BAR_LENGTH = 7\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ],
   "metadata": {
    "id": "jQeGvfSCRM4i"
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "LOCAL_M1 = False\n",
    "\n",
    "if LOCAL_M1:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "else:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "net = ResNet18() # 11.2 params\n",
    "#net = ResNet50() # 23.5"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_K9-VkFRsiL",
    "outputId": "6f2e07bf-e937-4776-9a0e-4ee20f298bd5"
   },
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "import humanize\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of parameters\", humanize.intword(count_parameters(net)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgQPV3H4ZTxA",
    "outputId": "4d24d10a-e99b-4eba-a5f4-887dc2840e3c"
   },
   "execution_count": 104,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_59934/3903068146.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtorchsummary\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msummary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhumanize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcount_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequires_grad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torchsummary'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_total_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The total number of parameters is \", get_total_params(net))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_pruned_parameters_countget_pruned_parameters_count(pruned_model):\n",
    "    params = 0\n",
    "    for param in pruned_model.parameters():\n",
    "        if param is not None:\n",
    "            params += torch.nonzero(param).size(0)\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning some of the model parameters\n",
      "The total number of parameters before the pruning is  11173962\n",
      "The total number of parameters after the pruning is  5673002\n"
     ]
    }
   ],
   "source": [
    "print(\"Pruning some of the model parameters\")\n",
    "\n",
    "print(\"The total number of parameters before the pruning is \",\n",
    "      get_total_params(net))\n",
    "\n",
    "print(\"The total number of parameters after the pruning is \",\n",
    "      get_pruned_parameters_countget_pruned_parameters_count(net))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "lr = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "save_loss = {'train':[], 'test':[]}\n",
    "save_acc = {'train':[], 'test':[]}"
   ],
   "metadata": {
    "id": "a3kWtBzVWg3Y"
   },
   "execution_count": 106,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # print(f\"Epoch:{epoch} -- Phase:Train -- Loss:{save_loss[phase][-1]:.2f} -- Acc:{save_acc[phase][-1]*100:.2f}\")\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
   ],
   "metadata": {
    "id": "CIzJObnOWz2d"
   },
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n"
   ],
   "metadata": {
    "id": "t-33JkfpW1Cu"
   },
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device  mps\n"
     ]
    }
   ],
   "source": [
    "print(\"Using device \", device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5673002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_59934/2399023997.py:5: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  params += torch.nonzero(param).size(0)\n"
     ]
    }
   ],
   "source": [
    "total_params = get_pruned_parameters_countget_pruned_parameters_count(net)\n",
    "print(total_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJcMkrBzW7o7",
    "outputId": "c5a2ca2e-2506-4f05-b561-136556a44ebd",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [======>]  Step: 238ms | Tot: 55s225ms | Loss: inf | Acc: 26.544% (13272/50 391/391 \n",
      " [======>]  Step: 24ms | Tot: 2s190ms | Loss: 1.722 | Acc: 35.121% (3477/9 99/100 \r..]  Step: 22ms | Tot: 760ms | Loss: 1.725 | Acc: 35.429% (1240/3 35/ [==>....]  Step: 23ms | Tot: 807ms | Loss: 1.723 | Acc: 35.270% (1305/3 37/ [==>....]  Step: 21ms | Tot: 850ms | Loss: 1.723 | Acc: 35.256% (1375/3 39/ [==>....]  Step: 22ms | Tot: 895ms | Loss: 1.723 | Acc: 35.244% (1445/4 41/ [==>....]  Step: 22ms | Tot: 940ms | Loss: 1.722 | Acc: 35.442% (1524/4 43/ [===>...]  Step: 22ms | Tot: 985ms | Loss: 1.719 | Acc: 35.600% (1602/4 45/ [===>...]  Step: 22ms | Tot: 1s29ms | Loss: 1.718 | Acc: 35.468% (1667/4 47/ [===>...]  Step: 22ms | Tot: 1s75ms | Loss: 1.718 | Acc: 35.429% (1736/4 49/ [===>...]  Step: 23ms | Tot: 1s120ms | Loss: 1.716 | Acc: 35.549% (1813/5 51/ [===>...]  Step: 22ms | Tot: 1s165ms | Loss: 1.716 | Acc: 35.491% (1881/5 53/ [===>...]  Step: 21ms | Tot: 1s209ms | Loss: 1.723 | Acc: 35.255% (1939/5 55/ [===>...]  Step: 21ms | Tot: 1s254ms | Loss: 1.724 | Acc: 35.298% (2012/5 57/ [====>..]  Step: 22ms | Tot: 1s299ms | Loss: 1.720 | Acc: 35.305% (2083/5 59/ [====>..]  Step: 22ms | Tot: 1s343ms | Loss: 1.720 | Acc: 35.279% (2152/6 61/ 62/ [====>..]  Step: 22ms | Tot: 1s410ms | Loss: 1.720 | Acc: 35.328% (2261/6 64/ [====>..]  Step: 21ms | Tot: 1s453ms | Loss: 1.721 | Acc: 35.242% (2326/6 66/ [====>..]  Step: 22ms | Tot: 1s498ms | Loss: 1.720 | Acc: 35.147% (2390/6 68/ [====>..]  Step: 22ms | Tot: 1s543ms | Loss: 1.721 | Acc: 35.057% (2454/7 70/ [====>..]  Step: 22ms | Tot: 1s587ms | Loss: 1.720 | Acc: 35.125% (2529/7 72/ [=====>.]  Step: 21ms | Tot: 1s631ms | Loss: 1.719 | Acc: 35.230% (2607/7 74/ [=====>.]  Step: 23ms | Tot: 1s677ms | Loss: 1.719 | Acc: 35.184% (2674/7 76/ [=====>.]  Step: 21ms | Tot: 1s720ms | Loss: 1.717 | Acc: 35.244% (2749/7 78/ [=====>.]  Step: 21ms | Tot: 1s764ms | Loss: 1.720 | Acc: 35.237% (2819/8 80/ [=====>.]  Step: 22ms | Tot: 1s809ms | Loss: 1.719 | Acc: 35.171% (2884/8 82/ [=====>.]  Step: 21ms | Tot: 1s853ms | Loss: 1.720 | Acc: 35.250% (2961/8 84/ [=====>.]  Step: 21ms | Tot: 1s897ms | Loss: 1.721 | Acc: 35.151% (3023/8 86/ [======>]  Step: 21ms | Tot: 1s941ms | Loss: 1.722 | Acc: 35.091% (3088/8 88/ [======>]  Step: 21ms | Tot: 1s985ms | Loss: 1.722 | Acc: 35.089% (3158/9 90/ [======>]  Step: 22ms | Tot: 2s29ms | Loss: 1.721 | Acc: 35.087% (3228/9 92/ [======>]  Step: 22ms | Tot: 2s73ms | Loss: 1.720 | Acc: 35.202% (3309/9 94/ [======>]  Step: 24ms | Tot: 2s119ms | Loss: 1.720 | Acc: 35.167% (3376/9 96/ [======>]  Step: 23ms | Tot: 2s165ms | Loss: 1.722 | Acc: 35.122% (3442/9 98/ [======>]  Step: 22ms | Tot: 2s212ms | Loss: 1.721 | Acc: 35.140% (3514/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [======>]  Step: 153ms | Tot: 56s327ms | Loss: 1.602 | Acc: 40.200% (20100/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s158ms | Loss: 1.402 | Acc: 48.470% (4847/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [======>]  Step: 195ms | Tot: 55s903ms | Loss: 1.345 | Acc: 50.910% (25455/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s211ms | Loss: 1.237 | Acc: 55.030% (5503/10 100/100 ]  Step: 22ms | Tot: 1s13ms | Loss: 1.228 | Acc: 54.957% (2528/4 46/ [===>...]  Step: 21ms | Tot: 1s57ms | Loss: 1.223 | Acc: 55.167% (2648/4 [===>...]  Step: 22ms | Tot: 1s126ms | Loss: 1.215 | Acc: 55.490% (2830/5 51/ [===>...]  Step: 22ms | Tot: 1s171ms | Loss: 1.214 | Acc: 55.491% (2941/5 53/ [===>...]  Step: 22ms | Tot: 1s217ms | Loss: 1.216 | Acc: 55.382% (3046/5 55/ [===>...]  Step: 22ms | Tot: 1s263ms | Loss: 1.218 | Acc: 55.439% (3160/5 57/ [====>..]  Step: 22ms | Tot: 1s308ms | Loss: 1.218 | Acc: 55.458% (3272/5 59/ [====>..]  Step: 22ms | Tot: 1s352ms | Loss: 1.219 | Acc: 55.426% (3381/6 61/ [====>..]  Step: 22ms | Tot: 1s397ms | Loss: 1.219 | Acc: 55.349% (3487/6 63/ [====>..]  Step: 23ms | Tot: 1s442ms | Loss: 1.224 | Acc: 55.231% (3590/6 65/ [====>..]  Step: 22ms | Tot: 1s488ms | Loss: 1.227 | Acc: 55.104% (3692/6 67/ [====>..]  Step: 21ms | Tot: 1s532ms | Loss: 1.231 | Acc: 55.029% (3797/6 69/ [====>..]  Step: 22ms | Tot: 1s577ms | Loss: 1.232 | Acc: 55.028% (3907/7 71/ [=====>.]  Step: 21ms | Tot: 1s620ms | Loss: 1.228 | Acc: 55.205% (4030/7 73/ [=====>.]  Step: 21ms | Tot: 1s664ms | Loss: 1.226 | Acc: 55.160% (4137/7 75/ [=====>.]  Step: 21ms | Tot: 1s709ms | Loss: 1.228 | Acc: 55.143% (4246/7 77/ [=====>.]  Step: 21ms | Tot: 1s752ms | Loss: 1.228 | Acc: 55.114% (4354/7 79/ [=====>.]  Step: 22ms | Tot: 1s796ms | Loss: 1.228 | Acc: 55.099% (4463/8 81/ [=====>.]  Step: 22ms | Tot: 1s840ms | Loss: 1.231 | Acc: 54.916% (4558/8 83/ [=====>.]  Step: 22ms | Tot: 1s885ms | Loss: 1.233 | Acc: 54.929% (4669/8 85/ [======>]  Step: 21ms | Tot: 1s928ms | Loss: 1.235 | Acc: 54.885% (4775/8 87/ [======>]  Step: 21ms | Tot: 1s971ms | Loss: 1.235 | Acc: 54.933% (4889/8 89/ [======>]  Step: 21ms | Tot: 2s14ms | Loss: 1.236 | Acc: 54.890% (4995/9 91/ [======>]  Step: 21ms | Tot: 2s58ms | Loss: 1.234 | Acc: 55.043% (5119/9 93/ [======>]  Step: 22ms | Tot: 2s102ms | Loss: 1.236 | Acc: 55.063% (5231/9 95/ [======>]  Step: 21ms | Tot: 2s146ms | Loss: 1.235 | Acc: 55.113% (5346/9 97/ [======>]  Step: 21ms | Tot: 2s189ms | Loss: 1.236 | Acc: 55.030% (5448/9 99/\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [======>]  Step: 176ms | Tot: 54s542ms | Loss: 1.135 | Acc: 59.206% (29603/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s240ms | Loss: 1.064 | Acc: 62.380% (6238/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [======>]  Step: 261ms | Tot: 55s438ms | Loss: 0.936 | Acc: 66.854% (33427/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s253ms | Loss: 0.928 | Acc: 67.490% (6749/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [======>]  Step: 337ms | Tot: 1m2s | Loss: 0.794 | Acc: 72.200% (36100/50 391/391 \n",
      " [======>]  Step: 24ms | Tot: 2s518ms | Loss: 0.777 | Acc: 73.690% (7369/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [======>]  Step: 185ms | Tot: 59s925ms | Loss: 0.681 | Acc: 76.286% (38143/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s220ms | Loss: 0.804 | Acc: 72.333% (7161/9 99/100 \r..]  Step: 22ms | Tot: 1s15ms | Loss: 0.792 | Acc: 73.043% (3360/4 46/ [===>...]  Step: 23ms | Tot: 1s61ms | Loss: 0.790 | Acc: 73.104% (3509/4 48/ [===>...]  Step: 23ms | Tot: 1s107ms | Loss: 0.791 | Acc: 73.180% (3659/5 50/ [===>...]  Step: 21ms | Tot: 1s152ms | Loss: 0.790 | Acc: 73.250% (3809/5 52/ [===>...]  Step: 22ms | Tot: 1s198ms | Loss: 0.790 | Acc: 73.278% (3957/5 54/ [===>...]  Step: 22ms | Tot: 1s242ms | Loss: 0.789 | Acc: 73.179% (4098/5 56/ [===>...]  Step: 21ms | Tot: 1s285ms | Loss: 0.790 | Acc: 73.121% (4241/5 58/ [====>..]  Step: 23ms | Tot: 1s332ms | Loss: 0.793 | Acc: 73.083% (4385/6 60/ [====>..]  Step: 21ms | Tot: 1s377ms | Loss: 0.794 | Acc: 72.887% (4519/6 62/ [====>..]  Step: 22ms | Tot: 1s422ms | Loss: 0.791 | Acc: 72.984% (4671/6 64/ [====>..]  Step: 23ms | Tot: 1s468ms | Loss: 0.794 | Acc: 72.803% (4805/6 66/ [====>..]  Step: 22ms | Tot: 1s513ms | Loss: 0.797 | Acc: 72.691% (4943/6 68/ [====>..]  Step: 23ms | Tot: 1s559ms | Loss: 0.799 | Acc: 72.543% (5078/7 70/ [====>..]  Step: 22ms | Tot: 1s604ms | Loss: 0.800 | Acc: 72.458% (5217/7 72/ [=====>.]  Step: 22ms | Tot: 1s648ms | Loss: 0.797 | Acc: 72.608% (5373/7 74/ [=====>.]  Step: 22ms | Tot: 1s694ms | Loss: 0.797 | Acc: 72.553% (5514/7 76/ [=====>.]  Step: 22ms | Tot: 1s740ms | Loss: 0.801 | Acc: 72.372% (5645/7 78/ [=====>.]  Step: 21ms | Tot: 1s785ms | Loss: 0.802 | Acc: 72.338% (5787/8 80/ [=====>.]  Step: 24ms | Tot: 1s831ms | Loss: 0.801 | Acc: 72.366% (5934/8 82/ [=====>.]  Step: 25ms | Tot: 1s880ms | Loss: 0.801 | Acc: 72.393% (6081/8 84/ [=====>.]  Step: 23ms | Tot: 1s927ms | Loss: 0.803 | Acc: 72.349% (6222/8 86/ [======>]  Step: 22ms | Tot: 1s973ms | Loss: 0.804 | Acc: 72.295% (6362/8 88/ [======>]  Step: 22ms | Tot: 2s19ms | Loss: 0.804 | Acc: 72.289% (6506/9 90/ [======>]  Step: 22ms | Tot: 2s64ms | Loss: 0.801 | Acc: 72.380% (6659/9 92/ [======>]  Step: 22ms | Tot: 2s109ms | Loss: 0.803 | Acc: 72.415% (6807/9 94/ [======>]  Step: 22ms | Tot: 2s153ms | Loss: 0.802 | Acc: 72.448% (6955/9 96/ [======>]  Step: 22ms | Tot: 2s198ms | Loss: 0.803 | Acc: 72.439% (7099/9 98/ [======>]  Step: 22ms | Tot: 2s242ms | Loss: 0.804 | Acc: 72.330% (7233/10 100/100 \n",
      "\n",
      "Epoch: 7\n",
      " [======>]  Step: 218ms | Tot: 51s410ms | Loss: 0.621 | Acc: 78.610% (39305/50 391/391 \n",
      " [======>]  Step: 23ms | Tot: 2s359ms | Loss: 0.581 | Acc: 79.870% (7987/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [======>]  Step: 228ms | Tot: 53s471ms | Loss: 0.576 | Acc: 79.982% (39991/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s226ms | Loss: 0.607 | Acc: 79.172% (7838/9 99/100 \r=>.]  Step: 22ms | Tot: 1s705ms | Loss: 0.604 | Acc: 79.368% (6032/7 76/ [=====>.]  Step: 22ms | Tot: 1s749ms | Loss: 0.604 | Acc: 79.333% (6188/7 78/ [=====>.]  Step: 22ms | Tot: 1s794ms | Loss: 0.607 | Acc: 79.200% (6336/8 80/ [=====>.]  Step: 23ms | Tot: 1s840ms | Loss: 0.604 | Acc: 79.256% (6499/8 82/ [=====>.]  Step: 22ms | Tot: 1s885ms | Loss: 0.604 | Acc: 79.274% (6659/8 84/ [=====>.]  Step: 23ms | Tot: 1s931ms | Loss: 0.607 | Acc: 79.256% (6816/8 86/ [======>]  Step: 22ms | Tot: 1s976ms | Loss: 0.608 | Acc: 79.170% (6967/8 88/ [======>]  Step: 21ms | Tot: 2s20ms | Loss: 0.607 | Acc: 79.233% (7131/9 90/ [======>]  Step: 22ms | Tot: 2s65ms | Loss: 0.605 | Acc: 79.337% (7299/9 92/ [======>]  Step: 22ms | Tot: 2s110ms | Loss: 0.607 | Acc: 79.223% (7447/9 94/ [======>]  Step: 24ms | Tot: 2s158ms | Loss: 0.607 | Acc: 79.260% (7609/9 96/ [======>]  Step: 23ms | Tot: 2s204ms | Loss: 0.607 | Acc: 79.224% (7764/9 98/ [======>]  Step: 22ms | Tot: 2s248ms | Loss: 0.607 | Acc: 79.170% (7917/10 100/100 \n",
      "\n",
      "Epoch: 9\n",
      " [======>]  Step: 136ms | Tot: 55s635ms | Loss: 0.545 | Acc: 81.374% (40687/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s134ms | Loss: 0.741 | Acc: 75.242% (7449/9 99/100 \r===>..]  Step: 22ms | Tot: 1s550ms | Loss: 0.741 | Acc: 75.222% (5416/7 72/ [=====>.]  Step: 21ms | Tot: 1s594ms | Loss: 0.736 | Acc: 75.365% (5577/7 74/ [=====>.]  Step: 21ms | Tot: 1s637ms | Loss: 0.739 | Acc: 75.276% (5721/7 76/ [=====>.]  Step: 21ms | Tot: 1s680ms | Loss: 0.740 | Acc: 75.282% (5872/7 78/ [=====>.]  Step: 21ms | Tot: 1s723ms | Loss: 0.739 | Acc: 75.338% (6027/8 80/ [=====>.]  Step: 21ms | Tot: 1s767ms | Loss: 0.737 | Acc: 75.427% (6185/8 82/ [=====>.]  Step: 21ms | Tot: 1s810ms | Loss: 0.737 | Acc: 75.357% (6330/8 84/ [=====>.]  Step: 21ms | Tot: 1s853ms | Loss: 0.738 | Acc: 75.314% (6477/8 86/ [======>]  Step: 21ms | Tot: 1s896ms | Loss: 0.739 | Acc: 75.273% (6624/8 88/ [======>]  Step: 21ms | Tot: 1s939ms | Loss: 0.739 | Acc: 75.289% (6776/9 90/ [======>]  Step: 21ms | Tot: 1s983ms | Loss: 0.738 | Acc: 75.326% (6930/9 92/ [======>]  Step: 21ms | Tot: 2s26ms | Loss: 0.741 | Acc: 75.202% (7069/9 94/ [======>]  Step: 21ms | Tot: 2s70ms | Loss: 0.740 | Acc: 75.271% (7226/9 96/ [======>]  Step: 21ms | Tot: 2s113ms | Loss: 0.741 | Acc: 75.255% (7375/9 98/ [======>]  Step: 21ms | Tot: 2s156ms | Loss: 0.741 | Acc: 75.190% (7519/10 100/100 \n",
      "\n",
      "Epoch: 10\n",
      " [======>]  Step: 137ms | Tot: 53s626ms | Loss: 0.516 | Acc: 82.188% (41094/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s225ms | Loss: 0.684 | Acc: 77.590% (7759/10 100/100   Step: 22ms | Tot: 879ms | Loss: 0.682 | Acc: 77.585% (3181/4 41/ [==>....]  Step: 22ms | Tot: 924ms | Loss: 0.676 | Acc: 77.791% (3345/4 43/ [===>...]  Step: 22ms | Tot: 969ms | Loss: 0.681 | Acc: 77.667% (3495/4 45/ [===>...]  Step: 22ms | Tot: 1s15ms | Loss: 0.678 | Acc: 77.723% (3653/4 47/ [===>...]  Step: 22ms | Tot: 1s59ms | Loss: 0.678 | Acc: 77.714% (3808/4 49/ [===>...]  Step: 21ms | Tot: 1s103ms | Loss: 0.678 | Acc: 77.686% (3962/5 51/ [===>...]  Step: 25ms | Tot: 1s151ms | Loss: 0.678 | Acc: 77.623% (4114/5 53/ [===>...]  Step: 22ms | Tot: 1s195ms | Loss: 0.679 | Acc: 77.491% (4262/5 55/ [===>...]  Step: 21ms | Tot: 1s238ms | Loss: 0.680 | Acc: 77.474% (4416/5 57/ [====>..]  Step: 21ms | Tot: 1s282ms | Loss: 0.683 | Acc: 77.373% (4565/5 59/ [====>..]  Step: 22ms | Tot: 1s327ms | Loss: 0.680 | Acc: 77.443% (4724/6 61/ [====>..]  Step: 23ms | Tot: 1s373ms | Loss: 0.678 | Acc: 77.476% (4881/6 63/ [====>..]  Step: 23ms | Tot: 1s421ms | Loss: 0.679 | Acc: 77.446% (5034/6 65/ [====>..]  Step: 22ms | Tot: 1s469ms | Loss: 0.675 | Acc: 77.537% (5195/6 67/ [====>..]  Step: 22ms | Tot: 1s515ms | Loss: 0.679 | Acc: 77.420% (5342/6 69/ [====>..]  Step: 22ms | Tot: 1s559ms | Loss: 0.680 | Acc: 77.423% (5497/7 71/ [=====>.]  Step: 24ms | Tot: 1s606ms | Loss: 0.679 | Acc: 77.562% (5662/7 73/ [=====>.]  Step: 26ms | Tot: 1s655ms | Loss: 0.677 | Acc: 77.627% (5822/7 75/ [=====>.]  Step: 23ms | Tot: 1s701ms | Loss: 0.678 | Acc: 77.610% (5976/7 77/ [=====>.]  Step: 22ms | Tot: 1s746ms | Loss: 0.678 | Acc: 77.532% (6125/7 79/ [=====>.]  Step: 23ms | Tot: 1s792ms | Loss: 0.678 | Acc: 77.519% (6279/8 81/ [=====>.]  Step: 22ms | Tot: 1s838ms | Loss: 0.679 | Acc: 77.518% (6434/8 83/ [=====>.]  Step: 22ms | Tot: 1s884ms | Loss: 0.681 | Acc: 77.471% (6585/8 85/ [======>]  Step: 23ms | Tot: 1s931ms | Loss: 0.684 | Acc: 77.368% (6731/8 87/ [======>]  Step: 22ms | Tot: 1s976ms | Loss: 0.685 | Acc: 77.416% (6890/8 89/ [======>]  Step: 23ms | Tot: 2s22ms | Loss: 0.686 | Acc: 77.429% (7046/9 91/ [======>]  Step: 22ms | Tot: 2s71ms | Loss: 0.683 | Acc: 77.581% (7215/9 93/ [======>]  Step: 22ms | Tot: 2s116ms | Loss: 0.682 | Acc: 77.632% (7375/9 95/ [======>]  Step: 22ms | Tot: 2s159ms | Loss: 0.680 | Acc: 77.680% (7535/9 97/ [======>]  Step: 21ms | Tot: 2s203ms | Loss: 0.684 | Acc: 77.576% (7680/9 99/\n",
      "\n",
      "Epoch: 11\n",
      " [======>]  Step: 172ms | Tot: 55s796ms | Loss: 0.498 | Acc: 83.080% (41540/50 391/391 \r  Step: 200ms | Tot: 19s736ms | Loss: 0.504 | Acc: 82.679% (15451/18 146/ [==>....]  Step: 153ms | Tot: 20s47ms | Loss: 0.503 | Acc: 82.728% (15672/18 148/ [==>....]  Step: 211ms | Tot: 20s405ms | Loss: 0.503 | Acc: 82.745% (15887/19 150/ [==>....]  Step: 141ms | Tot: 20s704ms | Loss: 0.504 | Acc: 82.720% (16094/19 152/ [==>....]  Step: 145ms | Tot: 21s5ms | Loss: 0.505 | Acc: 82.706% (16303/19 154/ [==>....]  Step: 144ms | Tot: 21s290ms | Loss: 0.505 | Acc: 82.717% (16517/19 156/ [==>....]  Step: 138ms | Tot: 21s602ms | Loss: 0.505 | Acc: 82.719% (16729/20 158/ [==>....]  Step: 144ms | Tot: 21s897ms | Loss: 0.505 | Acc: 82.705% (16938/20 160/ [==>....]  Step: 140ms | Tot: 22s205ms | Loss: 0.505 | Acc: 82.731% (17155/20 162/ [==>....]  Step: 138ms | Tot: 22s502ms | Loss: 0.504 | Acc: 82.741% (17369/20 164/ [==>....]  Step: 138ms | Tot: 22s775ms | Loss: 0.503 | Acc: 82.780% (17589/21 166/ [==>....]  Step: 141ms | Tot: 23s73ms | Loss: 0.503 | Acc: 82.771% (17799/21 168/ [===>...]  Step: 136ms | Tot: 23s351ms | Loss: 0.502 | Acc: 82.790% (18015/21 170/ [===>...]  Step: 136ms | Tot: 23s624ms | Loss: 0.503 | Acc: 82.758% (18220/22 172/ [===>...]  Step: 145ms | Tot: 23s910ms | Loss: 0.503 | Acc: 82.732% (18426/22 174/ [===>...]  Step: 171ms | Tot: 24s235ms | Loss: 0.502 | Acc: 82.737% (18639/22 176/ [===>...]  Step: 171ms | Tot: 24s554ms | Loss: 0.502 | Acc: 82.764% (18857/22 178/ [===>...]  Step: 153ms | Tot: 24s847ms | Loss: 0.501 | Acc: 82.799% (19077/23 180/ [===>...]  Step: 188ms | Tot: 25s181ms | Loss: 0.501 | Acc: 82.812% (19292/23 182/ [===>...]  Step: 191ms | Tot: 25s510ms | Loss: 0.502 | Acc: 82.740% (19487/23 184/ [===>...]  Step: 178ms | Tot: 25s843ms | Loss: 0.502 | Acc: 82.766% (19705/23 186/ [===>...]  Step: 145ms | Tot: 26s140ms | Loss: 0.502 | Acc: 82.779% (19920/24 188/ [===>...]  Step: 168ms | Tot: 26s455ms | Loss: 0.501 | Acc: 82.804% (20138/24 190/ [===>...]  Step: 143ms | Tot: 26s747ms | Loss: 0.501 | Acc: 82.764% (20340/24 192/ [===>...]  Step: 146ms | Tot: 27s42ms | Loss: 0.501 | Acc: 82.772% (20554/24 194/ [===>...]  Step: 154ms | Tot: 27s346ms | Loss: 0.501 | Acc: 82.789% (20770/25 196/ [===>...]  Step: 173ms | Tot: 27s671ms | Loss: 0.501 | Acc: 82.812% (20988/25 198/ [===>...]  Step: 140ms | Tot: 27s963ms | Loss: 0.501 | Acc: 82.793% (21195/25 200/ [===>...]  Step: 150ms | Tot: 28s266ms | Loss: 0.500 | Acc: 82.793% (21407/25 202/ [===>...]  Step: 165ms | Tot: 28s586ms | Loss: 0.500 | Acc: 82.805% (21622/26 204/ [===>...]  Step: 139ms | Tot: 28s873ms | Loss: 0.500 | Acc: 82.778% (21827/26 206/ [===>...]  Step: 131ms | Tot: 29s147ms | Loss: 0.499 | Acc: 82.801% (22045/26 208/ [===>...]  Step: 144ms | Tot: 29s423ms | Loss: 0.500 | Acc: 82.772% (22249/26 210/ [===>...]  Step: 147ms | Tot: 29s718ms | Loss: 0.501 | Acc: 82.783% (22464/27 212/ [===>...]  Step: 137ms | Tot: 29s993ms | Loss: 0.500 | Acc: 82.780% (22675/27 214/ [===>...]  Step: 146ms | Tot: 30s299ms | Loss: 0.500 | Acc: 82.802% (22893/27 216/ [===>...]  Step: 161ms | Tot: 30s599ms | Loss: 0.501 | Acc: 82.769% (23096/27 218/ [===>...]  Step: 135ms | Tot: 30s881ms | Loss: 0.502 | Acc: 82.763% (23306/28 220/ [===>...]  Step: 128ms | Tot: 31s146ms | Loss: 0.501 | Acc: 82.802% (23529/28 222/ [===>...]  Step: 129ms | Tot: 31s423ms | Loss: 0.501 | Acc: 82.781% (23735/28 224/ [====>..]  Step: 139ms | Tot: 31s821ms | Loss: 0.501 | Acc: 82.778% (23946/28 226/ [====>..]  Step: 144ms | Tot: 32s120ms | Loss: 0.501 | Acc: 82.789% (24161/29 228/ [====>..]  Step: 157ms | Tot: 32s413ms | Loss: 0.500 | Acc: 82.836% (24387/29 230/ [====>..]  Step: 134ms | Tot: 32s692ms | Loss: 0.500 | Acc: 82.856% (24605/29 232/ [====>..]  Step: 133ms | Tot: 32s961ms | Loss: 0.500 | Acc: 82.869% (24821/29 234/ [====>..]  Step: 131ms | Tot: 33s221ms | Loss: 0.499 | Acc: 82.892% (25040/30 236/ [====>..]  Step: 147ms | Tot: 33s508ms | Loss: 0.499 | Acc: 82.924% (25262/30 238/ [====>..]  Step: 190ms | Tot: 33s935ms | Loss: 0.498 | Acc: 82.930% (25476/30 240/ [====>..]  Step: 144ms | Tot: 34s222ms | Loss: 0.498 | Acc: 82.932% (25689/30 242/ [====>..]  Step: 139ms | Tot: 34s505ms | Loss: 0.498 | Acc: 82.953% (25908/31 244/ [====>..]  Step: 194ms | Tot: 34s839ms | Loss: 0.498 | Acc: 82.946% (26118/31 246/ [====>..]  Step: 133ms | Tot: 35s132ms | Loss: 0.498 | Acc: 82.951% (26332/31 248/ [====>..]  Step: 155ms | Tot: 35s455ms | Loss: 0.498 | Acc: 82.931% (26538/32 250/ [====>..]  Step: 157ms | Tot: 35s874ms | Loss: 0.499 | Acc: 82.924% (26748/32 252/ [====>..]  Step: 139ms | Tot: 36s165ms | Loss: 0.499 | Acc: 82.920% (26959/32 254/ [====>..]  Step: 148ms | Tot: 36s456ms | Loss: 0.499 | Acc: 82.913% (27169/32 256/ [====>..]  Step: 139ms | Tot: 36s737ms | Loss: 0.498 | Acc: 82.934% (27388/33 258/ [====>..]  Step: 149ms | Tot: 37s29ms | Loss: 0.498 | Acc: 82.933% (27600/33 260/ [====>..]  Step: 133ms | Tot: 37s303ms | Loss: 0.498 | Acc: 82.941% (27815/33 262/ [====>..]  Step: 136ms | Tot: 37s601ms | Loss: 0.497 | Acc: 82.955% (28032/33 264/ [====>..]  Step: 182ms | Tot: 37s921ms | Loss: 0.498 | Acc: 82.936% (28238/34 266/ [====>..]  Step: 136ms | Tot: 38s217ms | Loss: 0.498 | Acc: 82.944% (28453/34 268/ [====>..]  Step: 131ms | Tot: 38s515ms | Loss: 0.497 | Acc: 82.980% (28678/34 270/ [====>..]  Step: 139ms | Tot: 38s800ms | Loss: 0.496 | Acc: 83.005% (28899/34 272/ [====>..]  Step: 136ms | Tot: 39s75ms | Loss: 0.496 | Acc: 83.029% (29120/35 274/ [====>..]  Step: 136ms | Tot: 39s376ms | Loss: 0.496 | Acc: 83.016% (29328/35 276/ [====>..]  Step: 138ms | Tot: 39s653ms | Loss: 0.497 | Acc: 82.998% (29534/35 278/ [====>..]  Step: 136ms | Tot: 39s930ms | Loss: 0.497 | Acc: 82.974% (29738/35 280/ [=====>.]  Step: 212ms | Tot: 40s295ms | Loss: 0.497 | Acc: 83.012% (29964/36 282/ [=====>.]  Step: 156ms | Tot: 40s589ms | Loss: 0.498 | Acc: 83.002% (30173/36 284/ [=====>.]  Step: 205ms | Tot: 40s943ms | Loss: 0.498 | Acc: 83.001% (30385/36 286/ [=====>.]  Step: 134ms | Tot: 41s225ms | Loss: 0.498 | Acc: 83.016% (30603/36 288/ [=====>.]  Step: 178ms | Tot: 41s544ms | Loss: 0.497 | Acc: 83.039% (30824/37 290/ [=====>.]  Step: 141ms | Tot: 41s845ms | Loss: 0.497 | Acc: 83.061% (31045/37 292/ [=====>.]  Step: 143ms | Tot: 42s136ms | Loss: 0.498 | Acc: 83.044% (31251/37 294/ [=====>.]  Step: 138ms | Tot: 42s419ms | Loss: 0.498 | Acc: 83.032% (31459/37 296/ [=====>.]  Step: 138ms | Tot: 42s728ms | Loss: 0.498 | Acc: 83.038% (31674/38 298/ [=====>.]  Step: 148ms | Tot: 43s53ms | Loss: 0.498 | Acc: 83.039% (31887/38 300/ [=====>.]  Step: 136ms | Tot: 43s328ms | Loss: 0.497 | Acc: 83.058% (32107/38 302/ [=====>.]  Step: 134ms | Tot: 43s596ms | Loss: 0.497 | Acc: 83.052% (32317/38 304/ [=====>.]  Step: 138ms | Tot: 43s915ms | Loss: 0.497 | Acc: 83.050% (32529/39 306/ [=====>.]  Step: 132ms | Tot: 44s182ms | Loss: 0.498 | Acc: 83.048% (32741/39 308/ [=====>.]  Step: 144ms | Tot: 44s471ms | Loss: 0.498 | Acc: 83.049% (32954/39 310/ [=====>.]  Step: 134ms | Tot: 44s742ms | Loss: 0.497 | Acc: 83.040% (33163/39 312/ [=====>.]  Step: 179ms | Tot: 45s52ms | Loss: 0.498 | Acc: 83.026% (33370/40 314/ [=====>.]  Step: 134ms | Tot: 45s324ms | Loss: 0.497 | Acc: 83.045% (33590/40 316/ [=====>.]  Step: 135ms | Tot: 45s598ms | Loss: 0.498 | Acc: 83.019% (33792/40 318/ [=====>.]  Step: 131ms | Tot: 45s866ms | Loss: 0.498 | Acc: 83.018% (34004/40 320/ [=====>.]  Step: 135ms | Tot: 46s134ms | Loss: 0.497 | Acc: 83.028% (34221/41 322/ [=====>.]  Step: 133ms | Tot: 46s411ms | Loss: 0.498 | Acc: 83.025% (34432/41 324/ [=====>.]  Step: 130ms | Tot: 46s676ms | Loss: 0.498 | Acc: 83.033% (34648/41 326/ [=====>.]  Step: 145ms | Tot: 46s988ms | Loss: 0.498 | Acc: 83.017% (34854/41 328/ [=====>.]  Step: 172ms | Tot: 47s293ms | Loss: 0.498 | Acc: 83.011% (35064/42 330/ [=====>.]  Step: 135ms | Tot: 47s574ms | Loss: 0.498 | Acc: 83.015% (35278/42 332/ [=====>.]  Step: 131ms | Tot: 47s843ms | Loss: 0.497 | Acc: 83.042% (35502/42 334/ [=====>.]  Step: 140ms | Tot: 48s126ms | Loss: 0.497 | Acc: 83.040% (35714/43 336/ [======>]  Step: 132ms | Tot: 48s391ms | Loss: 0.498 | Acc: 83.025% (35920/43 338/ [======>]  Step: 135ms | Tot: 48s691ms | Loss: 0.498 | Acc: 83.022% (36131/43 340/ [======>]  Step: 145ms | Tot: 49s33ms | Loss: 0.498 | Acc: 83.034% (36349/43 342/ [======>]  Step: 131ms | Tot: 49s296ms | Loss: 0.498 | Acc: 83.046% (36567/44 344/ [======>]  Step: 143ms | Tot: 49s575ms | Loss: 0.497 | Acc: 83.081% (36795/44 346/ [======>]  Step: 132ms | Tot: 49s843ms | Loss: 0.496 | Acc: 83.107% (37019/44 348/ [======>]  Step: 139ms | Tot: 50s120ms | Loss: 0.496 | Acc: 83.134% (37244/44 350/ [======>]  Step: 135ms | Tot: 50s389ms | Loss: 0.496 | Acc: 83.105% (37444/45 352/ [======>]  Step: 145ms | Tot: 50s679ms | Loss: 0.496 | Acc: 83.084% (37647/45 354/ [======>]  Step: 141ms | Tot: 50s959ms | Loss: 0.496 | Acc: 83.080% (37858/45 356/ [======>]  Step: 142ms | Tot: 51s232ms | Loss: 0.496 | Acc: 83.103% (38081/45 358/ [======>]  Step: 134ms | Tot: 51s510ms | Loss: 0.496 | Acc: 83.108% (38296/46 360/ [======>]  Step: 141ms | Tot: 51s816ms | Loss: 0.496 | Acc: 83.117% (38513/46 362/ [======>]  Step: 138ms | Tot: 52s89ms | Loss: 0.496 | Acc: 83.104% (38720/46 364/ [======>]  Step: 137ms | Tot: 52s368ms | Loss: 0.496 | Acc: 83.099% (38930/46 366/ [======>]  Step: 145ms | Tot: 52s672ms | Loss: 0.496 | Acc: 83.095% (39141/47 368/ [======>]  Step: 138ms | Tot: 52s946ms | Loss: 0.496 | Acc: 83.091% (39352/47 370/ [======>]  Step: 131ms | Tot: 53s206ms | Loss: 0.497 | Acc: 83.069% (39554/47 372/ [======>]  Step: 133ms | Tot: 53s473ms | Loss: 0.497 | Acc: 83.076% (39770/47 374/ [======>]  Step: 138ms | Tot: 53s746ms | Loss: 0.497 | Acc: 83.070% (39980/48 376/ [======>]  Step: 135ms | Tot: 54s24ms | Loss: 0.497 | Acc: 83.067% (40191/48 378/ [======>]  Step: 130ms | Tot: 54s285ms | Loss: 0.497 | Acc: 83.065% (40403/48 380/ [======>]  Step: 145ms | Tot: 54s573ms | Loss: 0.498 | Acc: 83.058% (40612/48 382/ [======>]  Step: 129ms | Tot: 54s836ms | Loss: 0.498 | Acc: 83.061% (40826/49 384/ [======>]  Step: 128ms | Tot: 55s99ms | Loss: 0.498 | Acc: 83.053% (41035/49 386/ [======>]  Step: 132ms | Tot: 55s364ms | Loss: 0.498 | Acc: 83.058% (41250/49 388/ [======>]  Step: 131ms | Tot: 55s623ms | Loss: 0.498 | Acc: 83.077% (41472/49 390/\n",
      " [======>]  Step: 22ms | Tot: 2s462ms | Loss: 0.563 | Acc: 80.620% (8062/10 100/100 ..]  Step: 22ms | Tot: 1s123ms | Loss: 0.557 | Acc: 80.796% (3959/4 49/ [===>...]  Step: 22ms | Tot: 1s170ms | Loss: 0.559 | Acc: 80.686% (4115/5 51/ [===>...]  Step: 22ms | Tot: 1s215ms | Loss: 0.558 | Acc: 80.698% (4277/5 53/ [===>...]  Step: 22ms | Tot: 1s267ms | Loss: 0.555 | Acc: 80.800% (4444/5 55/ [===>...]  Step: 22ms | Tot: 1s313ms | Loss: 0.556 | Acc: 80.860% (4609/5 57/ [====>..]  Step: 22ms | Tot: 1s358ms | Loss: 0.559 | Acc: 80.797% (4767/5 59/ [====>..]  Step: 21ms | Tot: 1s402ms | Loss: 0.561 | Acc: 80.721% (4924/6 61/ [====>..]  Step: 22ms | Tot: 1s447ms | Loss: 0.562 | Acc: 80.778% (5089/6 63/ [====>..]  Step: 23ms | Tot: 1s493ms | Loss: 0.560 | Acc: 80.815% (5253/6 65/ [====>..]  Step: 22ms | Tot: 1s538ms | Loss: 0.560 | Acc: 80.836% (5416/6 67/ [====>..]  Step: 22ms | Tot: 1s583ms | Loss: 0.560 | Acc: 80.841% (5578/6 69/ [====>..]  Step: 22ms | Tot: 1s628ms | Loss: 0.563 | Acc: 80.789% (5736/7 71/ [=====>.]  Step: 21ms | Tot: 1s672ms | Loss: 0.563 | Acc: 80.740% (5894/7 73/ [=====>.]  Step: 204ms | Tot: 1s902ms | Loss: 0.562 | Acc: 80.760% (6057/7 75/ [=====>.]  Step: 23ms | Tot: 1s950ms | Loss: 0.562 | Acc: 80.688% (6213/7 77/ [=====>.]  Step: 21ms | Tot: 1s994ms | Loss: 0.563 | Acc: 80.620% (6369/7 79/ [=====>.]  Step: 22ms | Tot: 2s38ms | Loss: 0.560 | Acc: 80.654% (6533/8 81/ [=====>.]  Step: 23ms | Tot: 2s85ms | Loss: 0.561 | Acc: 80.651% (6694/8 83/ [=====>.]  Step: 22ms | Tot: 2s130ms | Loss: 0.563 | Acc: 80.541% (6846/8 85/ [======>]  Step: 22ms | Tot: 2s175ms | Loss: 0.565 | Acc: 80.471% (7001/8 87/ [======>]  Step: 21ms | Tot: 2s219ms | Loss: 0.567 | Acc: 80.427% (7158/8 89/ [======>]  Step: 21ms | Tot: 2s263ms | Loss: 0.566 | Acc: 80.484% (7324/9 91/ [======>]  Step: 22ms | Tot: 2s308ms | Loss: 0.565 | Acc: 80.538% (7490/9 93/ [======>]  Step: 22ms | Tot: 2s352ms | Loss: 0.565 | Acc: 80.568% (7654/9 95/ [======>]  Step: 21ms | Tot: 2s396ms | Loss: 0.562 | Acc: 80.660% (7824/9 97/ [======>]  Step: 22ms | Tot: 2s440ms | Loss: 0.563 | Acc: 80.586% (7978/9 99/\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      " [======>]  Step: 130ms | Tot: 57s183ms | Loss: 0.479 | Acc: 83.690% (41778/49 390/391 \r....]  Step: 200ms | Tot: 21s85ms | Loss: 0.477 | Acc: 83.688% (13390/16 125/ [==>....]  Step: 194ms | Tot: 21s441ms | Loss: 0.477 | Acc: 83.680% (13603/16 127/ [==>....]  Step: 178ms | Tot: 21s771ms | Loss: 0.477 | Acc: 83.709% (13822/16 129/ [==>....]  Step: 166ms | Tot: 22s85ms | Loss: 0.476 | Acc: 83.755% (14044/16 131/ [==>....]  Step: 152ms | Tot: 22s385ms | Loss: 0.476 | Acc: 83.758% (14259/17 133/ [==>....]  Step: 149ms | Tot: 22s675ms | Loss: 0.473 | Acc: 83.854% (14490/17 135/ [==>....]  Step: 163ms | Tot: 22s983ms | Loss: 0.474 | Acc: 83.822% (14699/17 137/ [==>....]  Step: 172ms | Tot: 23s299ms | Loss: 0.474 | Acc: 83.847% (14918/17 139/ [==>....]  Step: 131ms | Tot: 23s571ms | Loss: 0.473 | Acc: 83.887% (15140/18 141/ [==>....]  Step: 133ms | Tot: 23s874ms | Loss: 0.472 | Acc: 83.894% (15356/18 143/ [==>....]  Step: 140ms | Tot: 24s173ms | Loss: 0.472 | Acc: 83.890% (15570/18 145/ [==>....]  Step: 148ms | Tot: 24s459ms | Loss: 0.471 | Acc: 83.929% (15792/18 147/ [==>....]  Step: 131ms | Tot: 24s731ms | Loss: 0.473 | Acc: 83.856% (15993/19 149/ [==>....]  Step: 143ms | Tot: 25s9ms | Loss: 0.473 | Acc: 83.858% (16208/19 151/ [==>....]  Step: 134ms | Tot: 25s286ms | Loss: 0.475 | Acc: 83.793% (16410/19 153/ [==>....]  Step: 151ms | Tot: 25s566ms | Loss: 0.474 | Acc: 83.821% (16630/19 155/ [==>....]  Step: 135ms | Tot: 25s842ms | Loss: 0.475 | Acc: 83.798% (16840/20 157/ [==>....]  Step: 127ms | Tot: 26s97ms | Loss: 0.474 | Acc: 83.844% (17064/20 159/ [==>....]  Step: 131ms | Tot: 26s357ms | Loss: 0.475 | Acc: 83.798% (17269/20 161/ [==>....]  Step: 129ms | Tot: 26s616ms | Loss: 0.475 | Acc: 83.757% (17475/20 163/ [==>....]  Step: 140ms | Tot: 26s901ms | Loss: 0.474 | Acc: 83.736% (17685/21 165/ [==>....]  Step: 129ms | Tot: 27s161ms | Loss: 0.475 | Acc: 83.725% (17897/21 167/ [===>...]  Step: 130ms | Tot: 27s419ms | Loss: 0.473 | Acc: 83.802% (18128/21 169/ [===>...]  Step: 170ms | Tot: 27s720ms | Loss: 0.473 | Acc: 83.831% (18349/21 171/ [===>...]  Step: 134ms | Tot: 27s989ms | Loss: 0.473 | Acc: 83.838% (18565/22 173/ [===>...]  Step: 142ms | Tot: 28s287ms | Loss: 0.473 | Acc: 83.853% (18783/22 175/ [===>...]  Step: 146ms | Tot: 28s569ms | Loss: 0.473 | Acc: 83.854% (18998/22 177/ [===>...]  Step: 135ms | Tot: 28s834ms | Loss: 0.472 | Acc: 83.895% (19222/22 179/ [===>...]  Step: 133ms | Tot: 29s147ms | Loss: 0.472 | Acc: 83.900% (19438/23 181/ [===>...]  Step: 134ms | Tot: 29s414ms | Loss: 0.473 | Acc: 83.901% (19653/23 183/ [===>...]  Step: 129ms | Tot: 29s681ms | Loss: 0.472 | Acc: 83.906% (19869/23 185/ [===>...]  Step: 148ms | Tot: 29s964ms | Loss: 0.472 | Acc: 83.903% (20083/23 187/ [===>...]  Step: 131ms | Tot: 30s237ms | Loss: 0.472 | Acc: 83.900% (20297/24 189/ [===>...]  Step: 142ms | Tot: 30s521ms | Loss: 0.473 | Acc: 83.884% (20508/24 191/ [===>...]  Step: 137ms | Tot: 30s794ms | Loss: 0.472 | Acc: 83.914% (20730/24 193/ [===>...]  Step: 164ms | Tot: 31s91ms | Loss: 0.471 | Acc: 83.934% (20950/24 195/ [===>...]  Step: 129ms | Tot: 31s351ms | Loss: 0.472 | Acc: 83.907% (21158/25 197/ [===>...]  Step: 137ms | Tot: 31s621ms | Loss: 0.473 | Acc: 83.865% (21362/25 199/ [===>...]  Step: 142ms | Tot: 31s897ms | Loss: 0.471 | Acc: 83.920% (21591/25 201/ [===>...]  Step: 131ms | Tot: 32s157ms | Loss: 0.471 | Acc: 83.940% (21811/25 203/ [===>...]  Step: 130ms | Tot: 32s430ms | Loss: 0.470 | Acc: 83.948% (22028/26 205/ [===>...]  Step: 147ms | Tot: 32s705ms | Loss: 0.471 | Acc: 83.915% (22234/26 207/ [===>...]  Step: 129ms | Tot: 32s977ms | Loss: 0.470 | Acc: 83.945% (22457/26 209/ [===>...]  Step: 132ms | Tot: 33s240ms | Loss: 0.470 | Acc: 83.975% (22680/27 211/ [===>...]  Step: 132ms | Tot: 33s511ms | Loss: 0.470 | Acc: 83.972% (22894/27 213/ [===>...]  Step: 134ms | Tot: 33s779ms | Loss: 0.471 | Acc: 83.932% (23098/27 215/ [===>...]  Step: 129ms | Tot: 34s35ms | Loss: 0.472 | Acc: 83.896% (23303/27 217/ [===>...]  Step: 133ms | Tot: 34s294ms | Loss: 0.472 | Acc: 83.901% (23519/28 219/ [===>...]  Step: 169ms | Tot: 34s590ms | Loss: 0.472 | Acc: 83.894% (23732/28 221/ [===>...]  Step: 137ms | Tot: 34s861ms | Loss: 0.472 | Acc: 83.916% (23953/28 223/ [====>..]  Step: 139ms | Tot: 35s138ms | Loss: 0.472 | Acc: 83.910% (24166/28 225/ [====>..]  Step: 130ms | Tot: 35s401ms | Loss: 0.473 | Acc: 83.917% (24383/29 227/ [====>..]  Step: 152ms | Tot: 35s686ms | Loss: 0.473 | Acc: 83.880% (24587/29 229/ [====>..]  Step: 131ms | Tot: 35s964ms | Loss: 0.474 | Acc: 83.858% (24795/29 231/ [====>..]  Step: 132ms | Tot: 36s222ms | Loss: 0.474 | Acc: 83.832% (25002/29 233/ [====>..]  Step: 125ms | Tot: 36s477ms | Loss: 0.475 | Acc: 83.823% (25214/30 235/ [====>..]  Step: 130ms | Tot: 36s740ms | Loss: 0.474 | Acc: 83.841% (25434/30 237/ [====>..]  Step: 124ms | Tot: 36s992ms | Loss: 0.475 | Acc: 83.842% (25649/30 239/ [====>..]  Step: 131ms | Tot: 37s252ms | Loss: 0.475 | Acc: 83.834% (25861/30 241/ [====>..]  Step: 129ms | Tot: 37s508ms | Loss: 0.474 | Acc: 83.854% (26082/31 243/ [====>..]  Step: 129ms | Tot: 37s765ms | Loss: 0.474 | Acc: 83.874% (26303/31 245/ [====>..]  Step: 127ms | Tot: 38s27ms | Loss: 0.475 | Acc: 83.850% (26510/31 247/ [====>..]  Step: 167ms | Tot: 38s328ms | Loss: 0.474 | Acc: 83.870% (26731/31 249/ [====>..]  Step: 130ms | Tot: 38s590ms | Loss: 0.475 | Acc: 83.871% (26946/32 251/ [====>..]  Step: 142ms | Tot: 38s864ms | Loss: 0.475 | Acc: 83.856% (27156/32 253/ [====>..]  Step: 159ms | Tot: 39s154ms | Loss: 0.475 | Acc: 83.851% (27369/32 255/ [====>..]  Step: 128ms | Tot: 39s423ms | Loss: 0.475 | Acc: 83.876% (27592/32 257/ [====>..]  Step: 127ms | Tot: 39s682ms | Loss: 0.474 | Acc: 83.889% (27811/33 259/ [====>..]  Step: 144ms | Tot: 39s973ms | Loss: 0.474 | Acc: 83.905% (28031/33 261/ [====>..]  Step: 130ms | Tot: 40s232ms | Loss: 0.474 | Acc: 83.900% (28244/33 263/ [====>..]  Step: 130ms | Tot: 40s493ms | Loss: 0.474 | Acc: 83.889% (28455/33 265/ [====>..]  Step: 130ms | Tot: 40s752ms | Loss: 0.474 | Acc: 83.875% (28665/34 267/ [====>..]  Step: 133ms | Tot: 41s15ms | Loss: 0.475 | Acc: 83.832% (28865/34 269/ [====>..]  Step: 132ms | Tot: 41s274ms | Loss: 0.476 | Acc: 83.816% (29074/34 271/ [====>..]  Step: 136ms | Tot: 41s539ms | Loss: 0.476 | Acc: 83.797% (29282/34 273/ [====>..]  Step: 134ms | Tot: 41s816ms | Loss: 0.476 | Acc: 83.787% (29493/35 275/ [====>..]  Step: 131ms | Tot: 42s81ms | Loss: 0.476 | Acc: 83.777% (29704/35 277/ [====>..]  Step: 127ms | Tot: 42s332ms | Loss: 0.476 | Acc: 83.751% (29909/35 279/ [=====>.]  Step: 131ms | Tot: 42s596ms | Loss: 0.477 | Acc: 83.705% (30107/35 281/ [=====>.]  Step: 135ms | Tot: 42s863ms | Loss: 0.478 | Acc: 83.696% (30318/36 283/ [=====>.]  Step: 132ms | Tot: 43s129ms | Loss: 0.478 | Acc: 83.684% (30528/36 285/ [=====>.]  Step: 129ms | Tot: 43s386ms | Loss: 0.479 | Acc: 83.670% (30737/36 287/ [=====>.]  Step: 170ms | Tot: 43s686ms | Loss: 0.478 | Acc: 83.680% (30955/36 289/ [=====>.]  Step: 129ms | Tot: 43s950ms | Loss: 0.478 | Acc: 83.696% (31175/37 291/ [=====>.]  Step: 124ms | Tot: 44s200ms | Loss: 0.478 | Acc: 83.690% (31387/37 293/ [=====>.]  Step: 134ms | Tot: 44s466ms | Loss: 0.477 | Acc: 83.726% (31615/37 295/ [=====>.]  Step: 128ms | Tot: 44s725ms | Loss: 0.477 | Acc: 83.720% (31827/38 297/ [=====>.]  Step: 129ms | Tot: 44s981ms | Loss: 0.477 | Acc: 83.727% (32044/38 299/ [=====>.]  Step: 128ms | Tot: 45s236ms | Loss: 0.477 | Acc: 83.742% (32264/38 301/ [=====>.]  Step: 126ms | Tot: 45s490ms | Loss: 0.477 | Acc: 83.748% (32481/38 303/ [=====>.]  Step: 132ms | Tot: 45s759ms | Loss: 0.477 | Acc: 83.732% (32689/39 305/ [=====>.]  Step: 130ms | Tot: 46s22ms | Loss: 0.477 | Acc: 83.739% (32906/39 307/ [=====>.]  Step: 140ms | Tot: 46s298ms | Loss: 0.477 | Acc: 83.738% (33120/39 309/ [=====>.]  Step: 146ms | Tot: 46s578ms | Loss: 0.477 | Acc: 83.734% (33333/39 311/ [=====>.]  Step: 135ms | Tot: 46s850ms | Loss: 0.478 | Acc: 83.729% (33545/40 313/ [=====>.]  Step: 132ms | Tot: 47s114ms | Loss: 0.477 | Acc: 83.733% (33761/40 315/ [=====>.]  Step: 136ms | Tot: 47s395ms | Loss: 0.477 | Acc: 83.734% (33976/40 317/ [=====>.]  Step: 128ms | Tot: 47s657ms | Loss: 0.477 | Acc: 83.743% (34194/40 319/ [=====>.]  Step: 131ms | Tot: 47s957ms | Loss: 0.477 | Acc: 83.747% (34410/41 321/ [=====>.]  Step: 128ms | Tot: 48s219ms | Loss: 0.477 | Acc: 83.744% (34623/41 323/ [=====>.]  Step: 125ms | Tot: 48s477ms | Loss: 0.478 | Acc: 83.712% (34824/41 325/ [=====>.]  Step: 132ms | Tot: 48s776ms | Loss: 0.478 | Acc: 83.730% (35046/41 327/ [=====>.]  Step: 126ms | Tot: 49s37ms | Loss: 0.478 | Acc: 83.743% (35266/42 329/ [=====>.]  Step: 143ms | Tot: 49s335ms | Loss: 0.478 | Acc: 83.752% (35484/42 331/ [=====>.]  Step: 129ms | Tot: 49s596ms | Loss: 0.478 | Acc: 83.742% (35694/42 333/ [=====>.]  Step: 146ms | Tot: 49s869ms | Loss: 0.477 | Acc: 83.736% (35906/42 335/ [======>]  Step: 132ms | Tot: 50s135ms | Loss: 0.477 | Acc: 83.756% (36129/43 337/ [======>]  Step: 127ms | Tot: 50s399ms | Loss: 0.477 | Acc: 83.764% (36347/43 339/ [======>]  Step: 139ms | Tot: 50s688ms | Loss: 0.478 | Acc: 83.747% (36554/43 341/ [======>]  Step: 129ms | Tot: 50s950ms | Loss: 0.478 | Acc: 83.755% (36772/43 343/ [======>]  Step: 125ms | Tot: 51s202ms | Loss: 0.478 | Acc: 83.741% (36980/44 345/ [======>]  Step: 123ms | Tot: 51s453ms | Loss: 0.478 | Acc: 83.729% (37189/44 347/ [======>]  Step: 124ms | Tot: 51s707ms | Loss: 0.479 | Acc: 83.706% (37393/44 349/ [======>]  Step: 131ms | Tot: 51s970ms | Loss: 0.478 | Acc: 83.714% (37611/44 351/ [======>]  Step: 129ms | Tot: 52s229ms | Loss: 0.478 | Acc: 83.724% (37830/45 353/ [======>]  Step: 160ms | Tot: 52s545ms | Loss: 0.478 | Acc: 83.735% (38049/45 355/ [======>]  Step: 126ms | Tot: 52s804ms | Loss: 0.478 | Acc: 83.732% (38262/45 357/ [======>]  Step: 139ms | Tot: 53s85ms | Loss: 0.478 | Acc: 83.731% (38476/45 359/ [======>]  Step: 130ms | Tot: 53s347ms | Loss: 0.478 | Acc: 83.734% (38692/46 361/ [======>]  Step: 131ms | Tot: 53s606ms | Loss: 0.478 | Acc: 83.744% (38911/46 363/ [======>]  Step: 135ms | Tot: 53s884ms | Loss: 0.477 | Acc: 83.741% (39124/46 365/ [======>]  Step: 126ms | Tot: 54s140ms | Loss: 0.477 | Acc: 83.760% (39347/46 367/ [======>]  Step: 128ms | Tot: 54s397ms | Loss: 0.477 | Acc: 83.772% (39567/47 369/ [======>]  Step: 130ms | Tot: 54s698ms | Loss: 0.477 | Acc: 83.769% (39780/47 371/ [======>]  Step: 141ms | Tot: 54s972ms | Loss: 0.477 | Acc: 83.765% (39993/47 373/ [======>]  Step: 126ms | Tot: 55s234ms | Loss: 0.477 | Acc: 83.769% (40209/48 375/ [======>]  Step: 127ms | Tot: 55s488ms | Loss: 0.477 | Acc: 83.774% (40426/48 377/ [======>]  Step: 127ms | Tot: 55s747ms | Loss: 0.477 | Acc: 83.750% (40629/48 379/ [======>]  Step: 126ms | Tot: 56s2ms | Loss: 0.477 | Acc: 83.760% (40848/48 381/ [======>]  Step: 125ms | Tot: 56s257ms | Loss: 0.478 | Acc: 83.734% (41050/49 383/ [======>]  Step: 126ms | Tot: 56s513ms | Loss: 0.479 | Acc: 83.709% (41252/49 385/ [======>]  Step: 136ms | Tot: 56s779ms | Loss: 0.479 | Acc: 83.683% (41453/49 387/ [======>]  Step: 133ms | Tot: 57s53ms | Loss: 0.479 | Acc: 83.688% (41670/49 389/ [======>]  Step: 163ms | Tot: 57s347ms | Loss: 0.480 | Acc: 83.678% (41839/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s163ms | Loss: 0.725 | Acc: 76.869% (7610/9 99/100 \rtep: 21ms | Tot: 966ms | Loss: 0.714 | Acc: 77.750% (3421/4 44/ [===>...]  Step: 21ms | Tot: 1s9ms | Loss: 0.715 | Acc: 77.674% (3573/4 46/ [===>...]  Step: 21ms | Tot: 1s52ms | Loss: 0.713 | Acc: 77.708% (3730/4 48/ [===>...]  Step: 21ms | Tot: 1s96ms | Loss: 0.721 | Acc: 77.480% (3874/5 50/ [===>...]  Step: 21ms | Tot: 1s139ms | Loss: 0.720 | Acc: 77.519% (4031/5 52/ [===>...]  Step: 21ms | Tot: 1s182ms | Loss: 0.720 | Acc: 77.537% (4187/5 54/ [===>...]  Step: 21ms | Tot: 1s225ms | Loss: 0.723 | Acc: 77.393% (4334/5 56/ [===>...]  Step: 22ms | Tot: 1s269ms | Loss: 0.724 | Acc: 77.500% (4495/5 58/ [====>..]  Step: 22ms | Tot: 1s314ms | Loss: 0.730 | Acc: 77.217% (4633/6 60/ [====>..]  Step: 21ms | Tot: 1s357ms | Loss: 0.730 | Acc: 77.161% (4784/6 62/ [====>..]  Step: 21ms | Tot: 1s401ms | Loss: 0.728 | Acc: 77.250% (4944/6 64/ [====>..]  Step: 21ms | Tot: 1s444ms | Loss: 0.729 | Acc: 77.303% (5102/6 66/ [====>..]  Step: 21ms | Tot: 1s486ms | Loss: 0.727 | Acc: 77.221% (5251/6 68/ [====>..]  Step: 21ms | Tot: 1s530ms | Loss: 0.727 | Acc: 77.229% (5406/7 70/ [====>..]  Step: 21ms | Tot: 1s574ms | Loss: 0.730 | Acc: 77.153% (5555/7 72/ [=====>.]  Step: 21ms | Tot: 1s617ms | Loss: 0.728 | Acc: 77.122% (5707/7 74/ [=====>.]  Step: 22ms | Tot: 1s661ms | Loss: 0.725 | Acc: 77.132% (5862/7 76/ [=====>.]  Step: 21ms | Tot: 1s704ms | Loss: 0.725 | Acc: 77.090% (6013/7 78/ [=====>.]  Step: 21ms | Tot: 1s747ms | Loss: 0.724 | Acc: 77.050% (6164/8 80/ [=====>.]  Step: 22ms | Tot: 1s792ms | Loss: 0.725 | Acc: 76.988% (6313/8 82/ [=====>.]  Step: 22ms | Tot: 1s836ms | Loss: 0.724 | Acc: 77.024% (6470/8 84/ [=====>.]  Step: 22ms | Tot: 1s881ms | Loss: 0.726 | Acc: 76.988% (6621/8 86/ [======>]  Step: 21ms | Tot: 1s924ms | Loss: 0.727 | Acc: 76.841% (6762/8 88/ [======>]  Step: 21ms | Tot: 1s967ms | Loss: 0.727 | Acc: 76.889% (6920/9 90/ [======>]  Step: 21ms | Tot: 2s10ms | Loss: 0.725 | Acc: 76.967% (7081/9 92/ [======>]  Step: 21ms | Tot: 2s54ms | Loss: 0.727 | Acc: 76.872% (7226/9 94/ [======>]  Step: 21ms | Tot: 2s98ms | Loss: 0.725 | Acc: 76.948% (7387/9 96/ [======>]  Step: 21ms | Tot: 2s141ms | Loss: 0.723 | Acc: 76.939% (7540/9 98/ [======>]  Step: 21ms | Tot: 2s184ms | Loss: 0.726 | Acc: 76.830% (7683/10 100/100 \n",
      "\n",
      "Epoch: 13\n",
      " [======>]  Step: 239ms | Tot: 50s950ms | Loss: 0.465 | Acc: 84.000% (42000/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s197ms | Loss: 0.638 | Acc: 78.660% (7866/10 100/100 ep: 22ms | Tot: 201ms | Loss: 0.604 | Acc: 79.800% (798/1 10/ [>......]  Step: 22ms | Tot: 245ms | Loss: 0.635 | Acc: 79.167% (950/1 12/ [>......]  Step: 22ms | Tot: 289ms | Loss: 0.627 | Acc: 79.643% (1115/1 14/ [=>.....]  Step: 21ms | Tot: 333ms | Loss: 0.621 | Acc: 79.250% (1268/1 16/ [=>.....]  Step: 21ms | Tot: 376ms | Loss: 0.620 | Acc: 79.167% (1425/1 18/ [=>.....]  Step: 21ms | Tot: 420ms | Loss: 0.631 | Acc: 79.150% (1583/2 20/ [=>.....]  Step: 22ms | Tot: 464ms | Loss: 0.629 | Acc: 79.227% (1743/2 22/ [=>.....]  Step: 22ms | Tot: 508ms | Loss: 0.629 | Acc: 79.042% (1897/2 24/ [=>.....]  Step: 21ms | Tot: 552ms | Loss: 0.638 | Acc: 78.769% (2048/2 26/ [=>.....]  Step: 21ms | Tot: 595ms | Loss: 0.635 | Acc: 78.607% (2201/2 28/ [==>....]  Step: 21ms | Tot: 639ms | Loss: 0.630 | Acc: 78.833% (2365/3 30/ [==>....]  Step: 22ms | Tot: 682ms | Loss: 0.625 | Acc: 79.062% (2530/3 32/ [==>....]  Step: 21ms | Tot: 726ms | Loss: 0.632 | Acc: 78.735% (2677/3 34/ [==>....]  Step: 22ms | Tot: 770ms | Loss: 0.635 | Acc: 78.750% (2835/3 36/ [==>....]  Step: 22ms | Tot: 815ms | Loss: 0.641 | Acc: 78.553% (2985/3 38/ [==>....]  Step: 22ms | Tot: 860ms | Loss: 0.636 | Acc: 78.700% (3148/4 40/ [==>....]  Step: 22ms | Tot: 904ms | Loss: 0.640 | Acc: 78.667% (3304/4 42/ [===>...]  Step: 22ms | Tot: 948ms | Loss: 0.637 | Acc: 78.773% (3466/4 44/ [===>...]  Step: 22ms | Tot: 993ms | Loss: 0.636 | Acc: 78.696% (3620/4 46/ [===>...]  Step: 22ms | Tot: 1s38ms | Loss: 0.639 | Acc: 78.604% (3773/4 48/ [===>...]  Step: 21ms | Tot: 1s82ms | Loss: 0.640 | Acc: 78.540% (3927/5 50/ [===>...]  Step: 21ms | Tot: 1s126ms | Loss: 0.641 | Acc: 78.519% (4083/5 52/ [===>...]  Step: 21ms | Tot: 1s170ms | Loss: 0.640 | Acc: 78.648% (4247/5 54/ [===>...]  Step: 22ms | Tot: 1s215ms | Loss: 0.644 | Acc: 78.464% (4394/5 56/ [===>...]  Step: 22ms | Tot: 1s259ms | Loss: 0.641 | Acc: 78.655% (4562/5 58/ [====>..]  Step: 22ms | Tot: 1s305ms | Loss: 0.642 | Acc: 78.667% (4720/6 60/ [====>..]  Step: 21ms | Tot: 1s349ms | Loss: 0.640 | Acc: 78.806% (4886/6 62/ [====>..]  Step: 22ms | Tot: 1s393ms | Loss: 0.639 | Acc: 78.891% (5049/6 64/ [====>..]  Step: 21ms | Tot: 1s436ms | Loss: 0.642 | Acc: 78.818% (5202/6 66/ [====>..]  Step: 27ms | Tot: 1s493ms | Loss: 0.642 | Acc: 78.882% (5364/6 68/ [====>..]  Step: 21ms | Tot: 1s537ms | Loss: 0.644 | Acc: 78.771% (5514/7 70/ [====>..]  Step: 22ms | Tot: 1s581ms | Loss: 0.643 | Acc: 78.736% (5669/7 72/ [=====>.]  Step: 21ms | Tot: 1s625ms | Loss: 0.642 | Acc: 78.811% (5832/7 74/ [=====>.]  Step: 21ms | Tot: 1s668ms | Loss: 0.643 | Acc: 78.763% (5986/7 76/ [=====>.]  Step: 22ms | Tot: 1s713ms | Loss: 0.640 | Acc: 78.795% (6146/7 78/ [=====>.]  Step: 22ms | Tot: 1s758ms | Loss: 0.639 | Acc: 78.763% (6301/8 80/ [=====>.]  Step: 22ms | Tot: 1s803ms | Loss: 0.638 | Acc: 78.744% (6457/8 82/100 =====>.]  Step: 22ms | Tot: 1s825ms | Loss: 0.639 | Acc: 78.699% (6532/8 83/ [=====>.]  Step: 21ms | Tot: 1s869ms | Loss: 0.638 | Acc: 78.635% (6684/8 85/ [======>]  Step: 21ms | Tot: 1s913ms | Loss: 0.639 | Acc: 78.563% (6835/8 87/ [======>]  Step: 22ms | Tot: 1s956ms | Loss: 0.641 | Acc: 78.573% (6993/8 89/ [======>]  Step: 22ms | Tot: 2s | Loss: 0.641 | Acc: 78.615% (7154/9 91/ [======>]  Step: 22ms | Tot: 2s44ms | Loss: 0.640 | Acc: 78.677% (7317/9 93/ [======>]  Step: 21ms | Tot: 2s89ms | Loss: 0.638 | Acc: 78.674% (7474/9 95/ [======>]  Step: 21ms | Tot: 2s132ms | Loss: 0.637 | Acc: 78.742% (7638/9 97/ [======>]  Step: 21ms | Tot: 2s175ms | Loss: 0.637 | Acc: 78.717% (7793/9 99/\n",
      "\n",
      "Epoch: 14\n",
      " [==>....]  Step: 145ms | Tot: 20s197ms | Loss: 0.448 | Acc: 84.646% (13435/15 124/391 \r"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
