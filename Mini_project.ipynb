{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "import ssl\n",
    "def set_up_ssl():\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "set_up_ssl()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        # pruning model parameters in the first convolution layer\n",
    "        prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv1, 'weight')\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        # pruning model parameters in the second convolution layer\n",
    "        prune.random_unstructured(self.conv2, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv2, 'weight')\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv1, 'weight')\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        prune.random_unstructured(self.conv2, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv2, 'weight')\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        prune.random_unstructured(self.conv3, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv3, 'weight')\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        prune.random_unstructured(self.conv1, name='weight', amount=0.5)\n",
    "        prune.remove(self.conv1, 'weight')\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        prune.random_unstructured(self.linear, name='weight', amount=0.5)\n",
    "        prune.remove(self.linear, 'weight')\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n"
   ],
   "metadata": {
    "id": "wTLc4fVcQ857"
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "term_width = 5\n",
    "TOTAL_BAR_LENGTH = 7\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ],
   "metadata": {
    "id": "jQeGvfSCRM4i"
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "LOCAL_M1 = True\n",
    "\n",
    "if LOCAL_M1:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "else:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "net = ResNet18() # 11.2 params\n",
    "#net = ResNet50() # 23.5"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_K9-VkFRsiL",
    "outputId": "6f2e07bf-e937-4776-9a0e-4ee20f298bd5"
   },
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "import humanize\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of parameters\", humanize.intword(count_parameters(net)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgQPV3H4ZTxA",
    "outputId": "4d24d10a-e99b-4eba-a5f4-887dc2840e3c"
   },
   "execution_count": 104,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_59934/3903068146.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtorchsummary\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msummary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhumanize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcount_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequires_grad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torchsummary'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_total_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The total number of parameters is \", get_total_params(net))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_pruned_parameters_countget_pruned_parameters_count(pruned_model):\n",
    "    params = 0\n",
    "    for param in pruned_model.parameters():\n",
    "        if param is not None:\n",
    "            params += torch.nonzero(param).size(0)\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning some of the model parameters\n",
      "The total number of parameters before the pruning is  11173962\n",
      "The total number of parameters after the pruning is  5673002\n"
     ]
    }
   ],
   "source": [
    "print(\"Pruning some of the model parameters\")\n",
    "\n",
    "print(\"The total number of parameters before the pruning is \",\n",
    "      get_total_params(net))\n",
    "\n",
    "print(\"The total number of parameters after the pruning is \",\n",
    "      get_pruned_parameters_countget_pruned_parameters_count(net))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "lr = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "save_loss = {'train':[], 'test':[]}\n",
    "save_acc = {'train':[], 'test':[]}"
   ],
   "metadata": {
    "id": "a3kWtBzVWg3Y"
   },
   "execution_count": 106,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # print(f\"Epoch:{epoch} -- Phase:Train -- Loss:{save_loss[phase][-1]:.2f} -- Acc:{save_acc[phase][-1]*100:.2f}\")\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
   ],
   "metadata": {
    "id": "CIzJObnOWz2d"
   },
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n"
   ],
   "metadata": {
    "id": "t-33JkfpW1Cu"
   },
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device  mps\n"
     ]
    }
   ],
   "source": [
    "print(\"Using device \", device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5673002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_59934/2399023997.py:5: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  params += torch.nonzero(param).size(0)\n"
     ]
    }
   ],
   "source": [
    "total_params = get_pruned_parameters_countget_pruned_parameters_count(net)\n",
    "print(total_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJcMkrBzW7o7",
    "outputId": "c5a2ca2e-2506-4f05-b561-136556a44ebd",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [======>]  Step: 238ms | Tot: 55s225ms | Loss: inf | Acc: 26.544% (13272/50 391/391 \n",
      " [======>]  Step: 24ms | Tot: 2s190ms | Loss: 1.722 | Acc: 35.121% (3477/9 99/100 \r..]  Step: 22ms | Tot: 760ms | Loss: 1.725 | Acc: 35.429% (1240/3 35/ [==>....]  Step: 23ms | Tot: 807ms | Loss: 1.723 | Acc: 35.270% (1305/3 37/ [==>....]  Step: 21ms | Tot: 850ms | Loss: 1.723 | Acc: 35.256% (1375/3 39/ [==>....]  Step: 22ms | Tot: 895ms | Loss: 1.723 | Acc: 35.244% (1445/4 41/ [==>....]  Step: 22ms | Tot: 940ms | Loss: 1.722 | Acc: 35.442% (1524/4 43/ [===>...]  Step: 22ms | Tot: 985ms | Loss: 1.719 | Acc: 35.600% (1602/4 45/ [===>...]  Step: 22ms | Tot: 1s29ms | Loss: 1.718 | Acc: 35.468% (1667/4 47/ [===>...]  Step: 22ms | Tot: 1s75ms | Loss: 1.718 | Acc: 35.429% (1736/4 49/ [===>...]  Step: 23ms | Tot: 1s120ms | Loss: 1.716 | Acc: 35.549% (1813/5 51/ [===>...]  Step: 22ms | Tot: 1s165ms | Loss: 1.716 | Acc: 35.491% (1881/5 53/ [===>...]  Step: 21ms | Tot: 1s209ms | Loss: 1.723 | Acc: 35.255% (1939/5 55/ [===>...]  Step: 21ms | Tot: 1s254ms | Loss: 1.724 | Acc: 35.298% (2012/5 57/ [====>..]  Step: 22ms | Tot: 1s299ms | Loss: 1.720 | Acc: 35.305% (2083/5 59/ [====>..]  Step: 22ms | Tot: 1s343ms | Loss: 1.720 | Acc: 35.279% (2152/6 61/ 62/ [====>..]  Step: 22ms | Tot: 1s410ms | Loss: 1.720 | Acc: 35.328% (2261/6 64/ [====>..]  Step: 21ms | Tot: 1s453ms | Loss: 1.721 | Acc: 35.242% (2326/6 66/ [====>..]  Step: 22ms | Tot: 1s498ms | Loss: 1.720 | Acc: 35.147% (2390/6 68/ [====>..]  Step: 22ms | Tot: 1s543ms | Loss: 1.721 | Acc: 35.057% (2454/7 70/ [====>..]  Step: 22ms | Tot: 1s587ms | Loss: 1.720 | Acc: 35.125% (2529/7 72/ [=====>.]  Step: 21ms | Tot: 1s631ms | Loss: 1.719 | Acc: 35.230% (2607/7 74/ [=====>.]  Step: 23ms | Tot: 1s677ms | Loss: 1.719 | Acc: 35.184% (2674/7 76/ [=====>.]  Step: 21ms | Tot: 1s720ms | Loss: 1.717 | Acc: 35.244% (2749/7 78/ [=====>.]  Step: 21ms | Tot: 1s764ms | Loss: 1.720 | Acc: 35.237% (2819/8 80/ [=====>.]  Step: 22ms | Tot: 1s809ms | Loss: 1.719 | Acc: 35.171% (2884/8 82/ [=====>.]  Step: 21ms | Tot: 1s853ms | Loss: 1.720 | Acc: 35.250% (2961/8 84/ [=====>.]  Step: 21ms | Tot: 1s897ms | Loss: 1.721 | Acc: 35.151% (3023/8 86/ [======>]  Step: 21ms | Tot: 1s941ms | Loss: 1.722 | Acc: 35.091% (3088/8 88/ [======>]  Step: 21ms | Tot: 1s985ms | Loss: 1.722 | Acc: 35.089% (3158/9 90/ [======>]  Step: 22ms | Tot: 2s29ms | Loss: 1.721 | Acc: 35.087% (3228/9 92/ [======>]  Step: 22ms | Tot: 2s73ms | Loss: 1.720 | Acc: 35.202% (3309/9 94/ [======>]  Step: 24ms | Tot: 2s119ms | Loss: 1.720 | Acc: 35.167% (3376/9 96/ [======>]  Step: 23ms | Tot: 2s165ms | Loss: 1.722 | Acc: 35.122% (3442/9 98/ [======>]  Step: 22ms | Tot: 2s212ms | Loss: 1.721 | Acc: 35.140% (3514/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [======>]  Step: 153ms | Tot: 56s327ms | Loss: 1.602 | Acc: 40.200% (20100/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s158ms | Loss: 1.402 | Acc: 48.470% (4847/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [======>]  Step: 195ms | Tot: 55s903ms | Loss: 1.345 | Acc: 50.910% (25455/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s211ms | Loss: 1.237 | Acc: 55.030% (5503/10 100/100 ]  Step: 22ms | Tot: 1s13ms | Loss: 1.228 | Acc: 54.957% (2528/4 46/ [===>...]  Step: 21ms | Tot: 1s57ms | Loss: 1.223 | Acc: 55.167% (2648/4 [===>...]  Step: 22ms | Tot: 1s126ms | Loss: 1.215 | Acc: 55.490% (2830/5 51/ [===>...]  Step: 22ms | Tot: 1s171ms | Loss: 1.214 | Acc: 55.491% (2941/5 53/ [===>...]  Step: 22ms | Tot: 1s217ms | Loss: 1.216 | Acc: 55.382% (3046/5 55/ [===>...]  Step: 22ms | Tot: 1s263ms | Loss: 1.218 | Acc: 55.439% (3160/5 57/ [====>..]  Step: 22ms | Tot: 1s308ms | Loss: 1.218 | Acc: 55.458% (3272/5 59/ [====>..]  Step: 22ms | Tot: 1s352ms | Loss: 1.219 | Acc: 55.426% (3381/6 61/ [====>..]  Step: 22ms | Tot: 1s397ms | Loss: 1.219 | Acc: 55.349% (3487/6 63/ [====>..]  Step: 23ms | Tot: 1s442ms | Loss: 1.224 | Acc: 55.231% (3590/6 65/ [====>..]  Step: 22ms | Tot: 1s488ms | Loss: 1.227 | Acc: 55.104% (3692/6 67/ [====>..]  Step: 21ms | Tot: 1s532ms | Loss: 1.231 | Acc: 55.029% (3797/6 69/ [====>..]  Step: 22ms | Tot: 1s577ms | Loss: 1.232 | Acc: 55.028% (3907/7 71/ [=====>.]  Step: 21ms | Tot: 1s620ms | Loss: 1.228 | Acc: 55.205% (4030/7 73/ [=====>.]  Step: 21ms | Tot: 1s664ms | Loss: 1.226 | Acc: 55.160% (4137/7 75/ [=====>.]  Step: 21ms | Tot: 1s709ms | Loss: 1.228 | Acc: 55.143% (4246/7 77/ [=====>.]  Step: 21ms | Tot: 1s752ms | Loss: 1.228 | Acc: 55.114% (4354/7 79/ [=====>.]  Step: 22ms | Tot: 1s796ms | Loss: 1.228 | Acc: 55.099% (4463/8 81/ [=====>.]  Step: 22ms | Tot: 1s840ms | Loss: 1.231 | Acc: 54.916% (4558/8 83/ [=====>.]  Step: 22ms | Tot: 1s885ms | Loss: 1.233 | Acc: 54.929% (4669/8 85/ [======>]  Step: 21ms | Tot: 1s928ms | Loss: 1.235 | Acc: 54.885% (4775/8 87/ [======>]  Step: 21ms | Tot: 1s971ms | Loss: 1.235 | Acc: 54.933% (4889/8 89/ [======>]  Step: 21ms | Tot: 2s14ms | Loss: 1.236 | Acc: 54.890% (4995/9 91/ [======>]  Step: 21ms | Tot: 2s58ms | Loss: 1.234 | Acc: 55.043% (5119/9 93/ [======>]  Step: 22ms | Tot: 2s102ms | Loss: 1.236 | Acc: 55.063% (5231/9 95/ [======>]  Step: 21ms | Tot: 2s146ms | Loss: 1.235 | Acc: 55.113% (5346/9 97/ [======>]  Step: 21ms | Tot: 2s189ms | Loss: 1.236 | Acc: 55.030% (5448/9 99/\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [======>]  Step: 176ms | Tot: 54s542ms | Loss: 1.135 | Acc: 59.206% (29603/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s240ms | Loss: 1.064 | Acc: 62.380% (6238/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [======>]  Step: 261ms | Tot: 55s438ms | Loss: 0.936 | Acc: 66.854% (33427/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s253ms | Loss: 0.928 | Acc: 67.490% (6749/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [======>]  Step: 337ms | Tot: 1m2s | Loss: 0.794 | Acc: 72.200% (36100/50 391/391 \n",
      " [======>]  Step: 24ms | Tot: 2s518ms | Loss: 0.777 | Acc: 73.690% (7369/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [======>]  Step: 185ms | Tot: 59s925ms | Loss: 0.681 | Acc: 76.286% (38143/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s220ms | Loss: 0.804 | Acc: 72.333% (7161/9 99/100 \r..]  Step: 22ms | Tot: 1s15ms | Loss: 0.792 | Acc: 73.043% (3360/4 46/ [===>...]  Step: 23ms | Tot: 1s61ms | Loss: 0.790 | Acc: 73.104% (3509/4 48/ [===>...]  Step: 23ms | Tot: 1s107ms | Loss: 0.791 | Acc: 73.180% (3659/5 50/ [===>...]  Step: 21ms | Tot: 1s152ms | Loss: 0.790 | Acc: 73.250% (3809/5 52/ [===>...]  Step: 22ms | Tot: 1s198ms | Loss: 0.790 | Acc: 73.278% (3957/5 54/ [===>...]  Step: 22ms | Tot: 1s242ms | Loss: 0.789 | Acc: 73.179% (4098/5 56/ [===>...]  Step: 21ms | Tot: 1s285ms | Loss: 0.790 | Acc: 73.121% (4241/5 58/ [====>..]  Step: 23ms | Tot: 1s332ms | Loss: 0.793 | Acc: 73.083% (4385/6 60/ [====>..]  Step: 21ms | Tot: 1s377ms | Loss: 0.794 | Acc: 72.887% (4519/6 62/ [====>..]  Step: 22ms | Tot: 1s422ms | Loss: 0.791 | Acc: 72.984% (4671/6 64/ [====>..]  Step: 23ms | Tot: 1s468ms | Loss: 0.794 | Acc: 72.803% (4805/6 66/ [====>..]  Step: 22ms | Tot: 1s513ms | Loss: 0.797 | Acc: 72.691% (4943/6 68/ [====>..]  Step: 23ms | Tot: 1s559ms | Loss: 0.799 | Acc: 72.543% (5078/7 70/ [====>..]  Step: 22ms | Tot: 1s604ms | Loss: 0.800 | Acc: 72.458% (5217/7 72/ [=====>.]  Step: 22ms | Tot: 1s648ms | Loss: 0.797 | Acc: 72.608% (5373/7 74/ [=====>.]  Step: 22ms | Tot: 1s694ms | Loss: 0.797 | Acc: 72.553% (5514/7 76/ [=====>.]  Step: 22ms | Tot: 1s740ms | Loss: 0.801 | Acc: 72.372% (5645/7 78/ [=====>.]  Step: 21ms | Tot: 1s785ms | Loss: 0.802 | Acc: 72.338% (5787/8 80/ [=====>.]  Step: 24ms | Tot: 1s831ms | Loss: 0.801 | Acc: 72.366% (5934/8 82/ [=====>.]  Step: 25ms | Tot: 1s880ms | Loss: 0.801 | Acc: 72.393% (6081/8 84/ [=====>.]  Step: 23ms | Tot: 1s927ms | Loss: 0.803 | Acc: 72.349% (6222/8 86/ [======>]  Step: 22ms | Tot: 1s973ms | Loss: 0.804 | Acc: 72.295% (6362/8 88/ [======>]  Step: 22ms | Tot: 2s19ms | Loss: 0.804 | Acc: 72.289% (6506/9 90/ [======>]  Step: 22ms | Tot: 2s64ms | Loss: 0.801 | Acc: 72.380% (6659/9 92/ [======>]  Step: 22ms | Tot: 2s109ms | Loss: 0.803 | Acc: 72.415% (6807/9 94/ [======>]  Step: 22ms | Tot: 2s153ms | Loss: 0.802 | Acc: 72.448% (6955/9 96/ [======>]  Step: 22ms | Tot: 2s198ms | Loss: 0.803 | Acc: 72.439% (7099/9 98/ [======>]  Step: 22ms | Tot: 2s242ms | Loss: 0.804 | Acc: 72.330% (7233/10 100/100 \n",
      "\n",
      "Epoch: 7\n",
      " [======>]  Step: 218ms | Tot: 51s410ms | Loss: 0.621 | Acc: 78.610% (39305/50 391/391 \n",
      " [======>]  Step: 23ms | Tot: 2s359ms | Loss: 0.581 | Acc: 79.870% (7987/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [======>]  Step: 228ms | Tot: 53s471ms | Loss: 0.576 | Acc: 79.982% (39991/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s226ms | Loss: 0.607 | Acc: 79.172% (7838/9 99/100 \r=>.]  Step: 22ms | Tot: 1s705ms | Loss: 0.604 | Acc: 79.368% (6032/7 76/ [=====>.]  Step: 22ms | Tot: 1s749ms | Loss: 0.604 | Acc: 79.333% (6188/7 78/ [=====>.]  Step: 22ms | Tot: 1s794ms | Loss: 0.607 | Acc: 79.200% (6336/8 80/ [=====>.]  Step: 23ms | Tot: 1s840ms | Loss: 0.604 | Acc: 79.256% (6499/8 82/ [=====>.]  Step: 22ms | Tot: 1s885ms | Loss: 0.604 | Acc: 79.274% (6659/8 84/ [=====>.]  Step: 23ms | Tot: 1s931ms | Loss: 0.607 | Acc: 79.256% (6816/8 86/ [======>]  Step: 22ms | Tot: 1s976ms | Loss: 0.608 | Acc: 79.170% (6967/8 88/ [======>]  Step: 21ms | Tot: 2s20ms | Loss: 0.607 | Acc: 79.233% (7131/9 90/ [======>]  Step: 22ms | Tot: 2s65ms | Loss: 0.605 | Acc: 79.337% (7299/9 92/ [======>]  Step: 22ms | Tot: 2s110ms | Loss: 0.607 | Acc: 79.223% (7447/9 94/ [======>]  Step: 24ms | Tot: 2s158ms | Loss: 0.607 | Acc: 79.260% (7609/9 96/ [======>]  Step: 23ms | Tot: 2s204ms | Loss: 0.607 | Acc: 79.224% (7764/9 98/ [======>]  Step: 22ms | Tot: 2s248ms | Loss: 0.607 | Acc: 79.170% (7917/10 100/100 \n",
      "\n",
      "Epoch: 9\n",
      " [======>]  Step: 136ms | Tot: 55s635ms | Loss: 0.545 | Acc: 81.374% (40687/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s134ms | Loss: 0.741 | Acc: 75.242% (7449/9 99/100 \r===>..]  Step: 22ms | Tot: 1s550ms | Loss: 0.741 | Acc: 75.222% (5416/7 72/ [=====>.]  Step: 21ms | Tot: 1s594ms | Loss: 0.736 | Acc: 75.365% (5577/7 74/ [=====>.]  Step: 21ms | Tot: 1s637ms | Loss: 0.739 | Acc: 75.276% (5721/7 76/ [=====>.]  Step: 21ms | Tot: 1s680ms | Loss: 0.740 | Acc: 75.282% (5872/7 78/ [=====>.]  Step: 21ms | Tot: 1s723ms | Loss: 0.739 | Acc: 75.338% (6027/8 80/ [=====>.]  Step: 21ms | Tot: 1s767ms | Loss: 0.737 | Acc: 75.427% (6185/8 82/ [=====>.]  Step: 21ms | Tot: 1s810ms | Loss: 0.737 | Acc: 75.357% (6330/8 84/ [=====>.]  Step: 21ms | Tot: 1s853ms | Loss: 0.738 | Acc: 75.314% (6477/8 86/ [======>]  Step: 21ms | Tot: 1s896ms | Loss: 0.739 | Acc: 75.273% (6624/8 88/ [======>]  Step: 21ms | Tot: 1s939ms | Loss: 0.739 | Acc: 75.289% (6776/9 90/ [======>]  Step: 21ms | Tot: 1s983ms | Loss: 0.738 | Acc: 75.326% (6930/9 92/ [======>]  Step: 21ms | Tot: 2s26ms | Loss: 0.741 | Acc: 75.202% (7069/9 94/ [======>]  Step: 21ms | Tot: 2s70ms | Loss: 0.740 | Acc: 75.271% (7226/9 96/ [======>]  Step: 21ms | Tot: 2s113ms | Loss: 0.741 | Acc: 75.255% (7375/9 98/ [======>]  Step: 21ms | Tot: 2s156ms | Loss: 0.741 | Acc: 75.190% (7519/10 100/100 \n",
      "\n",
      "Epoch: 10\n",
      " [======>]  Step: 137ms | Tot: 53s626ms | Loss: 0.516 | Acc: 82.188% (41094/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s225ms | Loss: 0.684 | Acc: 77.590% (7759/10 100/100   Step: 22ms | Tot: 879ms | Loss: 0.682 | Acc: 77.585% (3181/4 41/ [==>....]  Step: 22ms | Tot: 924ms | Loss: 0.676 | Acc: 77.791% (3345/4 43/ [===>...]  Step: 22ms | Tot: 969ms | Loss: 0.681 | Acc: 77.667% (3495/4 45/ [===>...]  Step: 22ms | Tot: 1s15ms | Loss: 0.678 | Acc: 77.723% (3653/4 47/ [===>...]  Step: 22ms | Tot: 1s59ms | Loss: 0.678 | Acc: 77.714% (3808/4 49/ [===>...]  Step: 21ms | Tot: 1s103ms | Loss: 0.678 | Acc: 77.686% (3962/5 51/ [===>...]  Step: 25ms | Tot: 1s151ms | Loss: 0.678 | Acc: 77.623% (4114/5 53/ [===>...]  Step: 22ms | Tot: 1s195ms | Loss: 0.679 | Acc: 77.491% (4262/5 55/ [===>...]  Step: 21ms | Tot: 1s238ms | Loss: 0.680 | Acc: 77.474% (4416/5 57/ [====>..]  Step: 21ms | Tot: 1s282ms | Loss: 0.683 | Acc: 77.373% (4565/5 59/ [====>..]  Step: 22ms | Tot: 1s327ms | Loss: 0.680 | Acc: 77.443% (4724/6 61/ [====>..]  Step: 23ms | Tot: 1s373ms | Loss: 0.678 | Acc: 77.476% (4881/6 63/ [====>..]  Step: 23ms | Tot: 1s421ms | Loss: 0.679 | Acc: 77.446% (5034/6 65/ [====>..]  Step: 22ms | Tot: 1s469ms | Loss: 0.675 | Acc: 77.537% (5195/6 67/ [====>..]  Step: 22ms | Tot: 1s515ms | Loss: 0.679 | Acc: 77.420% (5342/6 69/ [====>..]  Step: 22ms | Tot: 1s559ms | Loss: 0.680 | Acc: 77.423% (5497/7 71/ [=====>.]  Step: 24ms | Tot: 1s606ms | Loss: 0.679 | Acc: 77.562% (5662/7 73/ [=====>.]  Step: 26ms | Tot: 1s655ms | Loss: 0.677 | Acc: 77.627% (5822/7 75/ [=====>.]  Step: 23ms | Tot: 1s701ms | Loss: 0.678 | Acc: 77.610% (5976/7 77/ [=====>.]  Step: 22ms | Tot: 1s746ms | Loss: 0.678 | Acc: 77.532% (6125/7 79/ [=====>.]  Step: 23ms | Tot: 1s792ms | Loss: 0.678 | Acc: 77.519% (6279/8 81/ [=====>.]  Step: 22ms | Tot: 1s838ms | Loss: 0.679 | Acc: 77.518% (6434/8 83/ [=====>.]  Step: 22ms | Tot: 1s884ms | Loss: 0.681 | Acc: 77.471% (6585/8 85/ [======>]  Step: 23ms | Tot: 1s931ms | Loss: 0.684 | Acc: 77.368% (6731/8 87/ [======>]  Step: 22ms | Tot: 1s976ms | Loss: 0.685 | Acc: 77.416% (6890/8 89/ [======>]  Step: 23ms | Tot: 2s22ms | Loss: 0.686 | Acc: 77.429% (7046/9 91/ [======>]  Step: 22ms | Tot: 2s71ms | Loss: 0.683 | Acc: 77.581% (7215/9 93/ [======>]  Step: 22ms | Tot: 2s116ms | Loss: 0.682 | Acc: 77.632% (7375/9 95/ [======>]  Step: 22ms | Tot: 2s159ms | Loss: 0.680 | Acc: 77.680% (7535/9 97/ [======>]  Step: 21ms | Tot: 2s203ms | Loss: 0.684 | Acc: 77.576% (7680/9 99/\n",
      "\n",
      "Epoch: 11\n",
      " [=====>.]  Step: 145ms | Tot: 42s281ms | Loss: 0.498 | Acc: 83.035% (31354/37 295/391 \r  Step: 200ms | Tot: 19s736ms | Loss: 0.504 | Acc: 82.679% (15451/18 146/ [==>....]  Step: 153ms | Tot: 20s47ms | Loss: 0.503 | Acc: 82.728% (15672/18 148/ [==>....]  Step: 211ms | Tot: 20s405ms | Loss: 0.503 | Acc: 82.745% (15887/19 150/ [==>....]  Step: 141ms | Tot: 20s704ms | Loss: 0.504 | Acc: 82.720% (16094/19 152/ [==>....]  Step: 145ms | Tot: 21s5ms | Loss: 0.505 | Acc: 82.706% (16303/19 154/ [==>....]  Step: 144ms | Tot: 21s290ms | Loss: 0.505 | Acc: 82.717% (16517/19 156/ [==>....]  Step: 138ms | Tot: 21s602ms | Loss: 0.505 | Acc: 82.719% (16729/20 158/ [==>....]  Step: 144ms | Tot: 21s897ms | Loss: 0.505 | Acc: 82.705% (16938/20 160/ [==>....]  Step: 140ms | Tot: 22s205ms | Loss: 0.505 | Acc: 82.731% (17155/20 162/ [==>....]  Step: 138ms | Tot: 22s502ms | Loss: 0.504 | Acc: 82.741% (17369/20 164/ [==>....]  Step: 138ms | Tot: 22s775ms | Loss: 0.503 | Acc: 82.780% (17589/21 166/ [==>....]  Step: 141ms | Tot: 23s73ms | Loss: 0.503 | Acc: 82.771% (17799/21 168/ [===>...]  Step: 136ms | Tot: 23s351ms | Loss: 0.502 | Acc: 82.790% (18015/21 170/ [===>...]  Step: 136ms | Tot: 23s624ms | Loss: 0.503 | Acc: 82.758% (18220/22 172/ [===>...]  Step: 145ms | Tot: 23s910ms | Loss: 0.503 | Acc: 82.732% (18426/22 174/ [===>...]  Step: 171ms | Tot: 24s235ms | Loss: 0.502 | Acc: 82.737% (18639/22 176/ [===>...]  Step: 171ms | Tot: 24s554ms | Loss: 0.502 | Acc: 82.764% (18857/22 178/ [===>...]  Step: 153ms | Tot: 24s847ms | Loss: 0.501 | Acc: 82.799% (19077/23 180/ [===>...]  Step: 188ms | Tot: 25s181ms | Loss: 0.501 | Acc: 82.812% (19292/23 182/ [===>...]  Step: 191ms | Tot: 25s510ms | Loss: 0.502 | Acc: 82.740% (19487/23 184/ [===>...]  Step: 178ms | Tot: 25s843ms | Loss: 0.502 | Acc: 82.766% (19705/23 186/ [===>...]  Step: 145ms | Tot: 26s140ms | Loss: 0.502 | Acc: 82.779% (19920/24 188/ [===>...]  Step: 168ms | Tot: 26s455ms | Loss: 0.501 | Acc: 82.804% (20138/24 190/ [===>...]  Step: 143ms | Tot: 26s747ms | Loss: 0.501 | Acc: 82.764% (20340/24 192/ [===>...]  Step: 146ms | Tot: 27s42ms | Loss: 0.501 | Acc: 82.772% (20554/24 194/ [===>...]  Step: 154ms | Tot: 27s346ms | Loss: 0.501 | Acc: 82.789% (20770/25 196/ [===>...]  Step: 173ms | Tot: 27s671ms | Loss: 0.501 | Acc: 82.812% (20988/25 198/ [===>...]  Step: 140ms | Tot: 27s963ms | Loss: 0.501 | Acc: 82.793% (21195/25 200/ [===>...]  Step: 150ms | Tot: 28s266ms | Loss: 0.500 | Acc: 82.793% (21407/25 202/ [===>...]  Step: 165ms | Tot: 28s586ms | Loss: 0.500 | Acc: 82.805% (21622/26 204/ [===>...]  Step: 139ms | Tot: 28s873ms | Loss: 0.500 | Acc: 82.778% (21827/26 206/ [===>...]  Step: 131ms | Tot: 29s147ms | Loss: 0.499 | Acc: 82.801% (22045/26 208/ [===>...]  Step: 144ms | Tot: 29s423ms | Loss: 0.500 | Acc: 82.772% (22249/26 210/ [===>...]  Step: 147ms | Tot: 29s718ms | Loss: 0.501 | Acc: 82.783% (22464/27 212/ [===>...]  Step: 137ms | Tot: 29s993ms | Loss: 0.500 | Acc: 82.780% (22675/27 214/ [===>...]  Step: 146ms | Tot: 30s299ms | Loss: 0.500 | Acc: 82.802% (22893/27 216/ [===>...]  Step: 161ms | Tot: 30s599ms | Loss: 0.501 | Acc: 82.769% (23096/27 218/ [===>...]  Step: 135ms | Tot: 30s881ms | Loss: 0.502 | Acc: 82.763% (23306/28 220/ [===>...]  Step: 128ms | Tot: 31s146ms | Loss: 0.501 | Acc: 82.802% (23529/28 222/ [===>...]  Step: 129ms | Tot: 31s423ms | Loss: 0.501 | Acc: 82.781% (23735/28 224/ [====>..]  Step: 139ms | Tot: 31s821ms | Loss: 0.501 | Acc: 82.778% (23946/28 226/ [====>..]  Step: 144ms | Tot: 32s120ms | Loss: 0.501 | Acc: 82.789% (24161/29 228/ [====>..]  Step: 157ms | Tot: 32s413ms | Loss: 0.500 | Acc: 82.836% (24387/29 230/ [====>..]  Step: 134ms | Tot: 32s692ms | Loss: 0.500 | Acc: 82.856% (24605/29 232/ [====>..]  Step: 133ms | Tot: 32s961ms | Loss: 0.500 | Acc: 82.869% (24821/29 234/ [====>..]  Step: 131ms | Tot: 33s221ms | Loss: 0.499 | Acc: 82.892% (25040/30 236/ [====>..]  Step: 147ms | Tot: 33s508ms | Loss: 0.499 | Acc: 82.924% (25262/30 238/ [====>..]  Step: 190ms | Tot: 33s935ms | Loss: 0.498 | Acc: 82.930% (25476/30 240/ [====>..]  Step: 144ms | Tot: 34s222ms | Loss: 0.498 | Acc: 82.932% (25689/30 242/ [====>..]  Step: 139ms | Tot: 34s505ms | Loss: 0.498 | Acc: 82.953% (25908/31 244/ [====>..]  Step: 194ms | Tot: 34s839ms | Loss: 0.498 | Acc: 82.946% (26118/31 246/ [====>..]  Step: 133ms | Tot: 35s132ms | Loss: 0.498 | Acc: 82.951% (26332/31 248/ [====>..]  Step: 155ms | Tot: 35s455ms | Loss: 0.498 | Acc: 82.931% (26538/32 250/ [====>..]  Step: 157ms | Tot: 35s874ms | Loss: 0.499 | Acc: 82.924% (26748/32 252/ [====>..]  Step: 139ms | Tot: 36s165ms | Loss: 0.499 | Acc: 82.920% (26959/32 254/ [====>..]  Step: 148ms | Tot: 36s456ms | Loss: 0.499 | Acc: 82.913% (27169/32 256/ [====>..]  Step: 139ms | Tot: 36s737ms | Loss: 0.498 | Acc: 82.934% (27388/33 258/ [====>..]  Step: 149ms | Tot: 37s29ms | Loss: 0.498 | Acc: 82.933% (27600/33 260/ [====>..]  Step: 133ms | Tot: 37s303ms | Loss: 0.498 | Acc: 82.941% (27815/33 262/ [====>..]  Step: 136ms | Tot: 37s601ms | Loss: 0.497 | Acc: 82.955% (28032/33 264/ [====>..]  Step: 182ms | Tot: 37s921ms | Loss: 0.498 | Acc: 82.936% (28238/34 266/ [====>..]  Step: 136ms | Tot: 38s217ms | Loss: 0.498 | Acc: 82.944% (28453/34 268/ [====>..]  Step: 131ms | Tot: 38s515ms | Loss: 0.497 | Acc: 82.980% (28678/34 270/ [====>..]  Step: 139ms | Tot: 38s800ms | Loss: 0.496 | Acc: 83.005% (28899/34 272/ [====>..]  Step: 136ms | Tot: 39s75ms | Loss: 0.496 | Acc: 83.029% (29120/35 274/ [====>..]  Step: 136ms | Tot: 39s376ms | Loss: 0.496 | Acc: 83.016% (29328/35 276/ [====>..]  Step: 138ms | Tot: 39s653ms | Loss: 0.497 | Acc: 82.998% (29534/35 278/ [====>..]  Step: 136ms | Tot: 39s930ms | Loss: 0.497 | Acc: 82.974% (29738/35 280/ [=====>.]  Step: 212ms | Tot: 40s295ms | Loss: 0.497 | Acc: 83.012% (29964/36 282/ [=====>.]  Step: 156ms | Tot: 40s589ms | Loss: 0.498 | Acc: 83.002% (30173/36 284/ [=====>.]  Step: 205ms | Tot: 40s943ms | Loss: 0.498 | Acc: 83.001% (30385/36 286/ [=====>.]  Step: 134ms | Tot: 41s225ms | Loss: 0.498 | Acc: 83.016% (30603/36 288/ [=====>.]  Step: 178ms | Tot: 41s544ms | Loss: 0.497 | Acc: 83.039% (30824/37 290/ [=====>.]  Step: 141ms | Tot: 41s845ms | Loss: 0.497 | Acc: 83.061% (31045/37 292/ [=====>.]  Step: 143ms | Tot: 42s136ms | Loss: 0.498 | Acc: 83.044% (31251/37 294/"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
