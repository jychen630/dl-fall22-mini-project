{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import ssl\n",
    "def set_up_ssl():\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "set_up_ssl()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        # pruning model parameters in the first convolution layer\n",
    "        prune.random_unstructured(self.conv1, name='weight', amount=0.3)\n",
    "        prune.remove(self.conv1, 'weight')\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        # pruning model parameters in the second convolution layer\n",
    "        prune.random_unstructured(self.conv2, name='weight', amount=0.3)\n",
    "        prune.remove(self.conv2, 'weight')\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n"
   ],
   "metadata": {
    "id": "wTLc4fVcQ857"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "term_width = 5\n",
    "TOTAL_BAR_LENGTH = 7\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ],
   "metadata": {
    "id": "jQeGvfSCRM4i"
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "LOCAL_M1 = True\n",
    "\n",
    "if LOCAL_M1:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "else:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "net = ResNet18() # 11.2 params\n",
    "#net = ResNet50() # 23.5"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_K9-VkFRsiL",
    "outputId": "6f2e07bf-e937-4776-9a0e-4ee20f298bd5"
   },
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "import humanize\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of parameters\", humanize.intword(count_parameters(net)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgQPV3H4ZTxA",
    "outputId": "4d24d10a-e99b-4eba-a5f4-887dc2840e3c"
   },
   "execution_count": 48,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_59934/3903068146.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtorchsummary\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msummary\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhumanize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcount_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequires_grad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torchsummary'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_total_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The total number of parameters is \", get_total_params(net))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def get_pruned_parameters_countget_pruned_parameters_count(pruned_model):\n",
    "    params = 0\n",
    "    for param in pruned_model.parameters():\n",
    "        if param is not None:\n",
    "            params += torch.nonzero(param).size(0)\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning some of the model parameters\n",
      "The total number of parameters before the pruning is  11173962\n",
      "The total number of parameters after the pruning is  6774972\n"
     ]
    }
   ],
   "source": [
    "print(\"Pruning some of the model parameters\")\n",
    "\n",
    "print(\"The total number of parameters before the pruning is \",\n",
    "      get_total_params(net))\n",
    "\n",
    "print(\"The total number of parameters after the pruning is \",\n",
    "      get_pruned_parameters_countget_pruned_parameters_count(net))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "lr = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "save_loss = {'train':[], 'test':[]}\n",
    "save_acc = {'train':[], 'test':[]}"
   ],
   "metadata": {
    "id": "a3kWtBzVWg3Y"
   },
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # print(f\"Epoch:{epoch} -- Phase:Train -- Loss:{save_loss[phase][-1]:.2f} -- Acc:{save_acc[phase][-1]*100:.2f}\")\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
   ],
   "metadata": {
    "id": "CIzJObnOWz2d"
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n"
   ],
   "metadata": {
    "id": "t-33JkfpW1Cu"
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device  mps\n"
     ]
    }
   ],
   "source": [
    "print(\"Using device \", device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11173962\n"
     ]
    }
   ],
   "source": [
    "total_params = get_pruned_parameters_countget_pruned_parameters_count(net)\n",
    "print(total_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJcMkrBzW7o7",
    "outputId": "c5a2ca2e-2506-4f05-b561-136556a44ebd"
   },
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [======>]  Step: 1s412ms | Tot: 55s661ms | Loss: 1.850 | Acc: 33.088% (16544/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s164ms | Loss: 1.545 | Acc: 42.370% (4237/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [======>]  Step: 137ms | Tot: 53s592ms | Loss: 1.357 | Acc: 50.528% (25264/50 391/391 \n",
      " [======>]  Step: 21ms | Tot: 2s162ms | Loss: 1.252 | Acc: 56.220% (5622/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [======>]  Step: 222ms | Tot: 1m15s | Loss: 1.074 | Acc: 61.730% (30865/50 391/391 \r>.]  Step: 201ms | Tot: 1m3s | Loss: 1.092 | Acc: 61.096% (25807/42 330/ [=====>.]  Step: 237ms | Tot: 1m4s | Loss: 1.091 | Acc: 61.123% (25975/42 332/ [=====>.]  Step: 201ms | Tot: 1m4s | Loss: 1.090 | Acc: 61.125% (26132/42 334/ [=====>.]  Step: 193ms | Tot: 1m5s | Loss: 1.091 | Acc: 61.110% (26282/43 336/ [======>]  Step: 216ms | Tot: 1m5s | Loss: 1.091 | Acc: 61.113% (26440/43 338/ [======>]  Step: 191ms | Tot: 1m5s | Loss: 1.090 | Acc: 61.135% (26606/43 340/ [======>]  Step: 195ms | Tot: 1m6s | Loss: 1.090 | Acc: 61.164% (26775/43 342/ [======>]  Step: 196ms | Tot: 1m6s | Loss: 1.089 | Acc: 61.178% (26938/44 344/ [======>]  Step: 209ms | Tot: 1m7s | Loss: 1.089 | Acc: 61.199% (27104/44 346/ [======>]  Step: 201ms | Tot: 1m7s | Loss: 1.088 | Acc: 61.232% (27275/44 348/ [======>]  Step: 246ms | Tot: 1m7s | Loss: 1.088 | Acc: 61.225% (27429/44 350/ [======>]  Step: 197ms | Tot: 1m8s | Loss: 1.088 | Acc: 61.228% (27587/45 352/ [======>]  Step: 181ms | Tot: 1m8s | Loss: 1.087 | Acc: 61.280% (27767/45 354/ [======>]  Step: 184ms | Tot: 1m9s | Loss: 1.086 | Acc: 61.306% (27936/45 356/ [======>]  Step: 194ms | Tot: 1m9s | Loss: 1.086 | Acc: 61.308% (28094/45 358/ [======>]  Step: 186ms | Tot: 1m9s | Loss: 1.085 | Acc: 61.341% (28266/46 360/ [======>]  Step: 181ms | Tot: 1m10s | Loss: 1.084 | Acc: 61.378% (28440/46 362/ [======>]  Step: 179ms | Tot: 1m10s | Loss: 1.083 | Acc: 61.403% (28609/46 364/ [======>]  Step: 217ms | Tot: 1m11s | Loss: 1.083 | Acc: 61.424% (28776/46 366/ [======>]  Step: 194ms | Tot: 1m11s | Loss: 1.082 | Acc: 61.447% (28944/47 368/ [======>]  Step: 187ms | Tot: 1m11s | Loss: 1.082 | Acc: 61.440% (29098/47 370/ [======>]  Step: 183ms | Tot: 1m12s | Loss: 1.081 | Acc: 61.467% (29268/47 372/ [======>]  Step: 191ms | Tot: 1m12s | Loss: 1.080 | Acc: 61.506% (29444/47 374/ [======>]  Step: 190ms | Tot: 1m12s | Loss: 1.079 | Acc: 61.540% (29618/48 376/ [======>]  Step: 208ms | Tot: 1m13s | Loss: 1.078 | Acc: 61.574% (29792/48 378/ [======>]  Step: 200ms | Tot: 1m13s | Loss: 1.077 | Acc: 61.600% (29962/48 380/ [======>]  Step: 189ms | Tot: 1m14s | Loss: 1.076 | Acc: 61.643% (30141/48 382/ [======>]  Step: 189ms | Tot: 1m14s | Loss: 1.076 | Acc: 61.666% (30310/49 384/ [======>]  Step: 185ms | Tot: 1m14s | Loss: 1.076 | Acc: 61.672% (30471/49 386/ [======>]  Step: 190ms | Tot: 1m15s | Loss: 1.075 | Acc: 61.693% (30639/49 388/ [======>]  Step: 195ms | Tot: 1m15s | Loss: 1.075 | Acc: 61.713% (30807/49 390/\n",
      " [======>]  Step: 22ms | Tot: 2s250ms | Loss: 1.243 | Acc: 58.080% (5808/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [======>]  Step: 160ms | Tot: 1m10s | Loss: 0.912 | Acc: 67.468% (33734/50 391/391 \n",
      " [======>]  Step: 31ms | Tot: 2s266ms | Loss: 1.013 | Acc: 65.071% (6442/9 99/100 \r....]  Step: 22ms | Tot: 681ms | Loss: 1.018 | Acc: 64.290% (1993/3 31/100 ==>....]  Step: 22ms | Tot: 704ms | Loss: 1.010 | Acc: 64.406% (2061/3 32/ [==>....]  Step: 22ms | Tot: 749ms | Loss: 1.014 | Acc: 64.471% (2192/3 34/ [==>....]  Step: 21ms | Tot: 793ms | Loss: 1.022 | Acc: 64.361% (2317/3 36/ [==>....]  Step: 22ms | Tot: 837ms | Loss: 1.030 | Acc: 64.237% (2441/3 38/ [==>....]  Step: 22ms | Tot: 882ms | Loss: 1.025 | Acc: 64.325% (2573/4 40/ [==>....]  Step: 22ms | Tot: 926ms | Loss: 1.021 | Acc: 64.405% (2705/4 42/ [===>...]  Step: 23ms | Tot: 972ms | Loss: 1.020 | Acc: 64.523% (2839/4 44/ [===>...]  Step: 22ms | Tot: 1s18ms | Loss: 1.017 | Acc: 64.630% (2973/4 46/ [===>...]  Step: 22ms | Tot: 1s63ms | Loss: 1.015 | Acc: 64.625% (3102/4 48/ [===>...]  Step: 23ms | Tot: 1s110ms | Loss: 1.010 | Acc: 64.780% (3239/5 50/ [===>...]  Step: 22ms | Tot: 1s155ms | Loss: 1.013 | Acc: 64.750% (3367/5 52/ [===>...]  Step: 22ms | Tot: 1s200ms | Loss: 1.016 | Acc: 64.593% (3488/5 54/ [===>...]  Step: 22ms | Tot: 1s245ms | Loss: 1.021 | Acc: 64.286% (3600/5 56/ [===>...]  Step: 22ms | Tot: 1s290ms | Loss: 1.017 | Acc: 64.345% (3732/5 58/ [====>..]  Step: 22ms | Tot: 1s335ms | Loss: 1.020 | Acc: 64.250% (3855/6 60/ [====>..]  Step: 22ms | Tot: 1s381ms | Loss: 1.018 | Acc: 64.242% (3983/6 62/ [====>..]  Step: 22ms | Tot: 1s425ms | Loss: 1.014 | Acc: 64.438% (4124/6 64/ [====>..]  Step: 22ms | Tot: 1s471ms | Loss: 1.018 | Acc: 64.394% (4250/6 66/ [====>..]  Step: 22ms | Tot: 1s517ms | Loss: 1.018 | Acc: 64.456% (4383/6 68/ [====>..]  Step: 22ms | Tot: 1s561ms | Loss: 1.019 | Acc: 64.471% (4513/7 70/ [====>..]  Step: 23ms | Tot: 1s607ms | Loss: 1.018 | Acc: 64.569% (4649/7 72/ [=====>.]  Step: 22ms | Tot: 1s653ms | Loss: 1.017 | Acc: 64.608% (4781/7 74/ [=====>.]  Step: 22ms | Tot: 1s699ms | Loss: 1.018 | Acc: 64.618% (4911/7 76/ [=====>.]  Step: 22ms | Tot: 1s745ms | Loss: 1.018 | Acc: 64.679% (5045/7 78/ [=====>.]  Step: 23ms | Tot: 1s793ms | Loss: 1.019 | Acc: 64.625% (5170/8 80/ [=====>.]  Step: 23ms | Tot: 1s841ms | Loss: 1.018 | Acc: 64.634% (5300/8 82/ [=====>.]  Step: 24ms | Tot: 1s888ms | Loss: 1.018 | Acc: 64.643% (5430/8 84/ [=====>.]  Step: 23ms | Tot: 1s936ms | Loss: 1.020 | Acc: 64.651% (5560/8 86/ [======>]  Step: 24ms | Tot: 1s983ms | Loss: 1.019 | Acc: 64.625% (5687/8 88/ [======>]  Step: 24ms | Tot: 2s34ms | Loss: 1.021 | Acc: 64.656% (5819/9 90/ [======>]  Step: 24ms | Tot: 2s84ms | Loss: 1.015 | Acc: 64.848% (5966/9 92/ [======>]  Step: 24ms | Tot: 2s133ms | Loss: 1.013 | Acc: 64.957% (6106/9 94/ [======>]  Step: 25ms | Tot: 2s184ms | Loss: 1.012 | Acc: 65.094% (6249/9 96/ [======>]  Step: 25ms | Tot: 2s235ms | Loss: 1.012 | Acc: 65.092% (6379/9 98/ [======>]  Step: 30ms | Tot: 2s296ms | Loss: 1.013 | Acc: 65.010% (6501/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [======>]  Step: 149ms | Tot: 1m5s | Loss: 0.769 | Acc: 72.918% (36459/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s247ms | Loss: 0.954 | Acc: 68.460% (6846/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [======>]  Step: 247ms | Tot: 1m3s | Loss: 0.661 | Acc: 77.018% (38509/50 391/391 \n",
      " [======>]  Step: 23ms | Tot: 2s223ms | Loss: 0.708 | Acc: 75.384% (7463/9 99/100 \r..]  Step: 22ms | Tot: 1s55ms | Loss: 0.706 | Acc: 75.354% (3617/4 48/ [===>...]  Step: 23ms | Tot: 1s100ms | Loss: 0.705 | Acc: 75.320% (3766/5 50/ [===>...]  Step: 23ms | Tot: 1s146ms | Loss: 0.703 | Acc: 75.365% (3919/5 52/ [===>...]  Step: 24ms | Tot: 1s194ms | Loss: 0.702 | Acc: 75.333% (4068/5 54/ [===>...]  Step: 22ms | Tot: 1s239ms | Loss: 0.702 | Acc: 75.375% (4221/5 56/ [===>...]  Step: 23ms | Tot: 1s284ms | Loss: 0.701 | Acc: 75.483% (4378/5 58/ [====>..]  Step: 22ms | Tot: 1s330ms | Loss: 0.706 | Acc: 75.383% (4523/6 60/ [====>..]  Step: 22ms | Tot: 1s375ms | Loss: 0.706 | Acc: 75.339% (4671/6 62/ [====>..]  Step: 22ms | Tot: 1s420ms | Loss: 0.704 | Acc: 75.328% (4821/6 64/ [====>..]  Step: 22ms | Tot: 1s465ms | Loss: 0.707 | Acc: 75.333% (4972/6 66/ [====>..]  Step: 22ms | Tot: 1s509ms | Loss: 0.707 | Acc: 75.294% (5120/6 68/ [====>..]  Step: 22ms | Tot: 1s555ms | Loss: 0.710 | Acc: 75.329% (5273/7 70/ [====>..]  Step: 22ms | Tot: 1s600ms | Loss: 0.710 | Acc: 75.333% (5424/7 72/ [=====>.]  Step: 22ms | Tot: 1s644ms | Loss: 0.709 | Acc: 75.392% (5579/7 74/ [=====>.]  Step: 22ms | Tot: 1s690ms | Loss: 0.710 | Acc: 75.382% (5729/7 76/ [=====>.]  Step: 23ms | Tot: 1s736ms | Loss: 0.713 | Acc: 75.244% (5869/7 78/ [=====>.]  Step: 23ms | Tot: 1s783ms | Loss: 0.713 | Acc: 75.250% (6020/8 80/ [=====>.]  Step: 23ms | Tot: 1s829ms | Loss: 0.712 | Acc: 75.305% (6175/8 82/ [=====>.]  Step: 23ms | Tot: 1s875ms | Loss: 0.713 | Acc: 75.286% (6324/8 84/ [=====>.]  Step: 22ms | Tot: 1s920ms | Loss: 0.714 | Acc: 75.256% (6472/8 86/ [======>]  Step: 23ms | Tot: 1s966ms | Loss: 0.714 | Acc: 75.250% (6622/8 88/ [======>]  Step: 22ms | Tot: 2s11ms | Loss: 0.714 | Acc: 75.244% (6772/9 90/ [======>]  Step: 22ms | Tot: 2s57ms | Loss: 0.712 | Acc: 75.315% (6929/9 92/ [======>]  Step: 23ms | Tot: 2s105ms | Loss: 0.711 | Acc: 75.309% (7079/9 94/ [======>]  Step: 24ms | Tot: 2s152ms | Loss: 0.710 | Acc: 75.385% (7237/9 96/ [======>]  Step: 22ms | Tot: 2s199ms | Loss: 0.708 | Acc: 75.418% (7391/9 98/ [======>]  Step: 23ms | Tot: 2s246ms | Loss: 0.708 | Acc: 75.390% (7539/10 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [======>]  Step: 147ms | Tot: 1m2s | Loss: 0.604 | Acc: 79.044% (39522/50 391/391 \n",
      " [======>]  Step: 22ms | Tot: 2s369ms | Loss: 1.095 | Acc: 66.810% (6681/10 100/100 \n",
      "\n",
      "Epoch: 7\n",
      " [======>]  Step: 229ms | Tot: 1m4s | Loss: 0.562 | Acc: 80.550% (40275/50 391/391 \r1 \r\r]  Step: 200ms | Tot: 13s709ms | Loss: 0.549 | Acc: 81.123% (8307/10 80/ [=>.....]  Step: 177ms | Tot: 14s49ms | Loss: 0.549 | Acc: 81.088% (8511/10 82/ [=>.....]  Step: 175ms | Tot: 14s406ms | Loss: 0.551 | Acc: 80.980% (8707/10 84/ [=>.....]  Step: 178ms | Tot: 14s756ms | Loss: 0.549 | Acc: 81.014% (8918/11 86/ [=>.....]  Step: 182ms | Tot: 15s115ms | Loss: 0.548 | Acc: 81.064% (9131/11 88/ [=>.....]  Step: 173ms | Tot: 15s468ms | Loss: 0.550 | Acc: 80.998% (9331/11 90/ [=>.....]  Step: 172ms | Tot: 15s901ms | Loss: 0.548 | Acc: 81.072% (9547/11 92/ [=>.....]  Step: 179ms | Tot: 16s266ms | Loss: 0.549 | Acc: 81.092% (9757/12 94/ [=>.....]  Step: 191ms | Tot: 16s621ms | Loss: 0.548 | Acc: 81.112% (9967/12 96/ [=>.....]  Step: 178ms | Tot: 16s960ms | Loss: 0.547 | Acc: 81.122% (10176/12 98/ [=>.....]  Step: 173ms | Tot: 17s320ms | Loss: 0.548 | Acc: 81.117% (10383/12 100/ [=>.....]  Step: 156ms | Tot: 17s679ms | Loss: 0.550 | Acc: 81.051% (10582/13 102/ [=>.....]  Step: 155ms | Tot: 18s16ms | Loss: 0.550 | Acc: 81.055% (10790/13 104/ [=>.....]  Step: 156ms | Tot: 18s341ms | Loss: 0.551 | Acc: 80.940% (10982/13 106/ [=>.....]  Step: 153ms | Tot: 18s646ms | Loss: 0.550 | Acc: 80.982% (11195/13 108/ [=>.....]  Step: 157ms | Tot: 18s958ms | Loss: 0.550 | Acc: 80.987% (11403/14 110/ [=>.....]  Step: 166ms | Tot: 19s297ms | Loss: 0.552 | Acc: 80.964% (11607/14 112/ [==>....]  Step: 157ms | Tot: 19s661ms | Loss: 0.553 | Acc: 80.914% (11807/14 114/ [==>....]  Step: 154ms | Tot: 19s972ms | Loss: 0.554 | Acc: 80.940% (12018/14 116/ [==>....]  Step: 158ms | Tot: 20s284ms | Loss: 0.556 | Acc: 80.833% (12209/15 118/ [==>....]  Step: 158ms | Tot: 20s655ms | Loss: 0.556 | Acc: 80.814% (12413/15 120/ [==>....]  Step: 154ms | Tot: 20s988ms | Loss: 0.558 | Acc: 80.757% (12611/15 122/ [==>....]  Step: 197ms | Tot: 21s336ms | Loss: 0.558 | Acc: 80.777% (12821/15 124/ [==>....]  Step: 187ms | Tot: 21s675ms | Loss: 0.558 | Acc: 80.785% (13029/16 126/ [==>....]  Step: 151ms | Tot: 21s977ms | Loss: 0.558 | Acc: 80.786% (13236/16 128/ [==>....]  Step: 161ms | Tot: 22s303ms | Loss: 0.559 | Acc: 80.727% (13433/16 130/ [==>....]  Step: 193ms | Tot: 22s657ms | Loss: 0.560 | Acc: 80.700% (13635/16 132/ [==>....]  Step: 181ms | Tot: 22s988ms | Loss: 0.561 | Acc: 80.731% (13847/17 134/ [==>....]  Step: 207ms | Tot: 23s347ms | Loss: 0.560 | Acc: 80.722% (14052/17 136/ [==>....]  Step: 154ms | Tot: 23s685ms | Loss: 0.560 | Acc: 80.707% (14256/17 138/ [==>....]  Step: 156ms | Tot: 23s997ms | Loss: 0.561 | Acc: 80.703% (14462/17 140/ [==>....]  Step: 169ms | Tot: 24s316ms | Loss: 0.561 | Acc: 80.689% (14666/18 142/ [==>....]  Step: 151ms | Tot: 24s629ms | Loss: 0.561 | Acc: 80.669% (14869/18 144/ [==>....]  Step: 154ms | Tot: 24s975ms | Loss: 0.561 | Acc: 80.667% (15075/18 146/ [==>....]  Step: 151ms | Tot: 25s320ms | Loss: 0.561 | Acc: 80.675% (15283/18 148/ [==>....]  Step: 151ms | Tot: 25s659ms | Loss: 0.562 | Acc: 80.661% (15487/19 150/ [==>....]  Step: 186ms | Tot: 26s13ms | Loss: 0.561 | Acc: 80.679% (15697/19 152/ [==>....]  Step: 196ms | Tot: 26s362ms | Loss: 0.562 | Acc: 80.646% (15897/19 154/ [==>....]  Step: 180ms | Tot: 26s696ms | Loss: 0.562 | Acc: 80.664% (16107/19 156/ [==>....]  Step: 172ms | Tot: 27s19ms | Loss: 0.563 | Acc: 80.627% (16306/20 158/ [==>....]  Step: 197ms | Tot: 27s367ms | Loss: 0.563 | Acc: 80.664% (16520/20 160/ [==>....]  Step: 152ms | Tot: 27s671ms | Loss: 0.564 | Acc: 80.628% (16719/20 162/ [==>....]  Step: 151ms | Tot: 28s17ms | Loss: 0.565 | Acc: 80.607% (16921/20 164/ [==>....]  Step: 158ms | Tot: 28s363ms | Loss: 0.565 | Acc: 80.586% (17123/21 166/ [==>....]  Step: 203ms | Tot: 28s724ms | Loss: 0.566 | Acc: 80.566% (17325/21 168/ [===>...]  Step: 196ms | Tot: 29s76ms | Loss: 0.565 | Acc: 80.597% (17538/21 170/ [===>...]  Step: 158ms | Tot: 29s383ms | Loss: 0.565 | Acc: 80.591% (17743/22 172/ [===>...]  Step: 158ms | Tot: 29s694ms | Loss: 0.565 | Acc: 80.590% (17949/22 174/ [===>...]  Step: 162ms | Tot: 30s9ms | Loss: 0.565 | Acc: 80.584% (18154/22 176/ [===>...]  Step: 149ms | Tot: 30s309ms | Loss: 0.565 | Acc: 80.557% (18354/22 178/ [===>...]  Step: 155ms | Tot: 30s666ms | Loss: 0.565 | Acc: 80.560% (18561/23 180/ [===>...]  Step: 160ms | Tot: 31s | Loss: 0.565 | Acc: 80.550% (18765/23 182/ [===>...]  Step: 153ms | Tot: 31s307ms | Loss: 0.566 | Acc: 80.549% (18971/23 184/ [===>...]  Step: 149ms | Tot: 31s659ms | Loss: 0.566 | Acc: 80.532% (19173/23 186/ [===>...]  Step: 164ms | Tot: 31s983ms | Loss: 0.566 | Acc: 80.502% (19372/24 188/ [===>...]  Step: 189ms | Tot: 32s327ms | Loss: 0.566 | Acc: 80.535% (19586/24 190/ [===>...]  Step: 149ms | Tot: 32s636ms | Loss: 0.567 | Acc: 80.473% (19777/24 192/ [===>...]  Step: 147ms | Tot: 32s933ms | Loss: 0.566 | Acc: 80.477% (19984/24 194/ [===>...]  Step: 163ms | Tot: 33s253ms | Loss: 0.565 | Acc: 80.473% (20189/25 196/ [===>...]  Step: 155ms | Tot: 33s564ms | Loss: 0.566 | Acc: 80.433% (20385/25 198/ [===>...]  Step: 151ms | Tot: 33s872ms | Loss: 0.567 | Acc: 80.418% (20587/25 200/ [===>...]  Step: 168ms | Tot: 34s192ms | Loss: 0.567 | Acc: 80.391% (20786/25 202/ [===>...]  Step: 148ms | Tot: 34s508ms | Loss: 0.567 | Acc: 80.388% (20991/26 204/ [===>...]  Step: 151ms | Tot: 34s809ms | Loss: 0.568 | Acc: 80.400% (21200/26 206/ [===>...]  Step: 174ms | Tot: 35s133ms | Loss: 0.568 | Acc: 80.371% (21398/26 208/ [===>...]  Step: 164ms | Tot: 35s454ms | Loss: 0.568 | Acc: 80.368% (21603/26 210/ [===>...]  Step: 157ms | Tot: 35s774ms | Loss: 0.567 | Acc: 80.391% (21815/27 212/ [===>...]  Step: 157ms | Tot: 36s84ms | Loss: 0.567 | Acc: 80.388% (22020/27 214/ [===>...]  Step: 151ms | Tot: 36s393ms | Loss: 0.566 | Acc: 80.396% (22228/27 216/ [===>...]  Step: 200ms | Tot: 36s747ms | Loss: 0.566 | Acc: 80.383% (22430/27 218/ [===>...]  Step: 151ms | Tot: 37s50ms | Loss: 0.566 | Acc: 80.359% (22629/28 220/ [===>...]  Step: 155ms | Tot: 37s355ms | Loss: 0.566 | Acc: 80.377% (22840/28 222/ [===>...]  Step: 150ms | Tot: 37s667ms | Loss: 0.567 | Acc: 80.375% (23045/28 224/ [====>..]  Step: 152ms | Tot: 37s975ms | Loss: 0.567 | Acc: 80.362% (23247/28 226/ [====>..]  Step: 155ms | Tot: 38s284ms | Loss: 0.568 | Acc: 80.311% (23438/29 228/ [====>..]  Step: 152ms | Tot: 38s599ms | Loss: 0.567 | Acc: 80.316% (23645/29 230/ [====>..]  Step: 164ms | Tot: 38s926ms | Loss: 0.567 | Acc: 80.304% (23847/29 232/ [====>..]  Step: 153ms | Tot: 39s238ms | Loss: 0.567 | Acc: 80.322% (24058/29 234/ [====>..]  Step: 161ms | Tot: 39s578ms | Loss: 0.567 | Acc: 80.323% (24264/30 236/ [====>..]  Step: 154ms | Tot: 39s884ms | Loss: 0.566 | Acc: 80.328% (24471/30 238/ [====>..]  Step: 174ms | Tot: 40s237ms | Loss: 0.566 | Acc: 80.355% (24685/30 240/ [====>..]  Step: 160ms | Tot: 40s560ms | Loss: 0.567 | Acc: 80.333% (24884/30 242/ [====>..]  Step: 150ms | Tot: 40s866ms | Loss: 0.568 | Acc: 80.289% (25076/31 244/ [====>..]  Step: 158ms | Tot: 41s184ms | Loss: 0.568 | Acc: 80.294% (25283/31 246/ [====>..]  Step: 160ms | Tot: 41s506ms | Loss: 0.568 | Acc: 80.302% (25491/31 248/ [====>..]  Step: 148ms | Tot: 41s812ms | Loss: 0.568 | Acc: 80.278% (25689/32 250/ [====>..]  Step: 155ms | Tot: 42s121ms | Loss: 0.568 | Acc: 80.292% (25899/32 252/ [====>..]  Step: 172ms | Tot: 42s442ms | Loss: 0.568 | Acc: 80.306% (26109/32 254/ [====>..]  Step: 150ms | Tot: 42s759ms | Loss: 0.568 | Acc: 80.322% (26320/32 256/ [====>..]  Step: 147ms | Tot: 43s56ms | Loss: 0.568 | Acc: 80.323% (26526/33 258/ [====>..]  Step: 163ms | Tot: 43s386ms | Loss: 0.567 | Acc: 80.349% (26740/33 260/ [====>..]  Step: 188ms | Tot: 43s731ms | Loss: 0.567 | Acc: 80.355% (26948/33 262/ [====>..]  Step: 152ms | Tot: 44s45ms | Loss: 0.566 | Acc: 80.362% (27156/33 264/ [====>..]  Step: 163ms | Tot: 44s380ms | Loss: 0.565 | Acc: 80.395% (27373/34 266/ [====>..]  Step: 153ms | Tot: 44s690ms | Loss: 0.566 | Acc: 80.381% (27574/34 268/ [====>..]  Step: 163ms | Tot: 45s13ms | Loss: 0.566 | Acc: 80.379% (27779/34 270/ [====>..]  Step: 162ms | Tot: 45s329ms | Loss: 0.566 | Acc: 80.397% (27991/34 272/ [====>..]  Step: 151ms | Tot: 45s647ms | Loss: 0.566 | Acc: 80.400% (28198/35 274/ [====>..]  Step: 161ms | Tot: 45s969ms | Loss: 0.565 | Acc: 80.409% (28407/35 276/ [====>..]  Step: 159ms | Tot: 46s282ms | Loss: 0.564 | Acc: 80.452% (28628/35 278/ [====>..]  Step: 151ms | Tot: 46s599ms | Loss: 0.564 | Acc: 80.458% (28836/35 280/ [=====>.]  Step: 149ms | Tot: 46s900ms | Loss: 0.565 | Acc: 80.436% (29034/36 282/ [=====>.]  Step: 154ms | Tot: 47s233ms | Loss: 0.565 | Acc: 80.427% (29237/36 284/ [=====>.]  Step: 151ms | Tot: 47s538ms | Loss: 0.565 | Acc: 80.433% (29445/36 286/ [=====>.]  Step: 151ms | Tot: 47s860ms | Loss: 0.564 | Acc: 80.439% (29653/36 288/ [=====>.]  Step: 161ms | Tot: 48s190ms | Loss: 0.564 | Acc: 80.420% (29852/37 290/ [=====>.]  Step: 158ms | Tot: 48s502ms | Loss: 0.564 | Acc: 80.450% (30069/37 292/ [=====>.]  Step: 149ms | Tot: 48s807ms | Loss: 0.565 | Acc: 80.432% (30268/37 294/ [=====>.]  Step: 149ms | Tot: 49s114ms | Loss: 0.564 | Acc: 80.434% (30475/37 296/ [=====>.]  Step: 155ms | Tot: 49s423ms | Loss: 0.564 | Acc: 80.411% (30672/38 298/ [=====>.]  Step: 160ms | Tot: 49s740ms | Loss: 0.565 | Acc: 80.398% (30873/38 300/ [=====>.]  Step: 155ms | Tot: 50s47ms | Loss: 0.565 | Acc: 80.407% (31082/38 302/ [=====>.]  Step: 192ms | Tot: 50s391ms | Loss: 0.566 | Acc: 80.402% (31286/38 304/ [=====>.]  Step: 158ms | Tot: 50s701ms | Loss: 0.566 | Acc: 80.400% (31491/39 306/ [=====>.]  Step: 152ms | Tot: 51s11ms | Loss: 0.565 | Acc: 80.421% (31705/39 308/ [=====>.]  Step: 159ms | Tot: 51s343ms | Loss: 0.565 | Acc: 80.433% (31916/39 310/ [=====>.]  Step: 147ms | Tot: 51s645ms | Loss: 0.565 | Acc: 80.431% (32121/39 312/ [=====>.]  Step: 204ms | Tot: 51s992ms | Loss: 0.565 | Acc: 80.412% (32319/40 314/ [=====>.]  Step: 152ms | Tot: 52s295ms | Loss: 0.566 | Acc: 80.412% (32525/40 316/ [=====>.]  Step: 147ms | Tot: 52s589ms | Loss: 0.565 | Acc: 80.454% (32748/40 318/ [=====>.]  Step: 157ms | Tot: 52s894ms | Loss: 0.565 | Acc: 80.442% (32949/40 320/ [=====>.]  Step: 149ms | Tot: 53s203ms | Loss: 0.564 | Acc: 80.464% (33164/41 322/ [=====>.]  Step: 151ms | Tot: 53s505ms | Loss: 0.564 | Acc: 80.442% (33361/41 324/ [=====>.]  Step: 158ms | Tot: 53s827ms | Loss: 0.564 | Acc: 80.462% (33575/41 326/ [=====>.]  Step: 151ms | Tot: 54s133ms | Loss: 0.565 | Acc: 80.431% (33768/41 328/ [=====>.]  Step: 166ms | Tot: 54s444ms | Loss: 0.564 | Acc: 80.443% (33979/42 330/ [=====>.]  Step: 151ms | Tot: 54s756ms | Loss: 0.564 | Acc: 80.450% (34188/42 332/ [=====>.]  Step: 154ms | Tot: 55s61ms | Loss: 0.564 | Acc: 80.445% (34392/42 334/ [=====>.]  Step: 153ms | Tot: 55s375ms | Loss: 0.564 | Acc: 80.478% (34612/43 336/ [======>]  Step: 144ms | Tot: 55s674ms | Loss: 0.563 | Acc: 80.483% (34820/43 338/ [======>]  Step: 155ms | Tot: 55s988ms | Loss: 0.563 | Acc: 80.483% (35026/43 340/ [======>]  Step: 151ms | Tot: 56s296ms | Loss: 0.563 | Acc: 80.478% (35230/43 342/ [======>]  Step: 150ms | Tot: 56s600ms | Loss: 0.563 | Acc: 80.505% (35448/44 344/ [======>]  Step: 155ms | Tot: 56s908ms | Loss: 0.562 | Acc: 80.541% (35670/44 346/ [======>]  Step: 153ms | Tot: 57s212ms | Loss: 0.562 | Acc: 80.568% (35888/44 348/ [======>]  Step: 158ms | Tot: 57s526ms | Loss: 0.562 | Acc: 80.558% (36090/44 350/ [======>]  Step: 159ms | Tot: 57s835ms | Loss: 0.562 | Acc: 80.564% (36299/45 352/ [======>]  Step: 164ms | Tot: 58s159ms | Loss: 0.562 | Acc: 80.550% (36499/45 354/ [======>]  Step: 156ms | Tot: 58s468ms | Loss: 0.562 | Acc: 80.546% (36703/45 356/ [======>]  Step: 157ms | Tot: 58s774ms | Loss: 0.562 | Acc: 80.558% (36915/45 358/ [======>]  Step: 154ms | Tot: 59s88ms | Loss: 0.562 | Acc: 80.564% (37124/46 360/ [======>]  Step: 165ms | Tot: 59s413ms | Loss: 0.562 | Acc: 80.579% (37337/46 362/ [======>]  Step: 153ms | Tot: 59s720ms | Loss: 0.562 | Acc: 80.574% (37541/46 364/ [======>]  Step: 152ms | Tot: 1m66ms | Loss: 0.561 | Acc: 80.590% (37755/46 366/ [======>]  Step: 164ms | Tot: 1m387ms | Loss: 0.561 | Acc: 80.579% (37956/47 368/ [======>]  Step: 170ms | Tot: 1m712ms | Loss: 0.561 | Acc: 80.589% (38167/47 370/ [======>]  Step: 151ms | Tot: 1m1s | Loss: 0.560 | Acc: 80.595% (38376/47 372/ [======>]  Step: 144ms | Tot: 1m1s | Loss: 0.560 | Acc: 80.602% (38586/47 374/ [======>]  Step: 155ms | Tot: 1m1s | Loss: 0.560 | Acc: 80.602% (38792/48 376/ [======>]  Step: 152ms | Tot: 1m1s | Loss: 0.560 | Acc: 80.591% (38993/48 378/ [======>]  Step: 161ms | Tot: 1m2s | Loss: 0.561 | Acc: 80.569% (39189/48 380/ [======>]  Step: 153ms | Tot: 1m2s | Loss: 0.561 | Acc: 80.567% (39394/48 382/ [======>]  Step: 158ms | Tot: 1m2s | Loss: 0.561 | Acc: 80.550% (39592/49 384/ [======>]  Step: 148ms | Tot: 1m3s | Loss: 0.561 | Acc: 80.566% (39806/49 386/ [======>]  Step: 159ms | Tot: 1m3s | Loss: 0.561 | Acc: 80.553% (40006/49 388/ [======>]  Step: 151ms | Tot: 1m3s | Loss: 0.561 | Acc: 80.557% (40214/49 390/\n",
      " [======>]  Step: 23ms | Tot: 2s272ms | Loss: 0.790 | Acc: 74.060% (7406/10 100/100 \n",
      "\n",
      "Epoch: 8\n",
      " [=>.....]  Step: 153ms | Tot: 13s540ms | Loss: 0.532 | Acc: 81.477% (8969/11 86/391 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10d8d4dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1474, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_59934/1089577939.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_epoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mscheduler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/4l/kfc4gh5d1jn6zkrmv51vvbyw0000gn/T/ipykernel_59934/860472993.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(epoch)\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredicted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mtotal\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mtargets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m         \u001B[0mcorrect\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mpredicted\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtargets\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m         \u001B[0;31m# print(f\"Epoch:{epoch} -- Phase:Train -- Loss:{save_loss[phase][-1]:.2f} -- Acc:{save_acc[phase][-1]*100:.2f}\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
